id,tag,timestamp,value
2,-Metrics/Training(Step): loss,1610339780220,2.1108100414276123
4,-Metrics/Training(Step): loss,1610339783919,1.909860610961914
6,-Metrics/Training(Step): loss,1610339787632,1.5486931800842285
8,-Metrics/Training(Step): loss,1610339791321,1.4515612125396729
10,-Metrics/Training(Step): loss,1610339795019,1.4961740970611572
12,-Metrics/Training(Step): loss,1610339798649,1.2600657939910889
14,-Metrics/Training(Step): loss,1610339802315,1.342171311378479
16,-Metrics/Training(Step): loss,1610339806119,1.2088333368301392
18,-Metrics/Training(Step): loss,1610339809641,1.2783955335617065
20,-Metrics/Training(Step): loss,1610339813258,1.2252092361450195
22,-Metrics/Training(Step): loss,1610339816931,1.1746424436569214
24,-Metrics/Training(Step): loss,1610339821021,1.1523715257644653
26,-Metrics/Training(Step): loss,1610339824720,1.2824771404266357
28,-Metrics/Training(Step): loss,1610339828243,1.2570843696594238
30,-Metrics/Training(Step): loss,1610339832120,1.2324674129486084
32,-Metrics/Training(Step): loss,1610339835657,1.0061321258544922
34,-Metrics/Training(Step): loss,1610339839445,1.0634055137634277
36,-Metrics/Training(Step): loss,1610339842921,0.8406095504760742
38,-Metrics/Training(Step): loss,1610339846634,1.2473033666610718
40,-Metrics/Training(Step): loss,1610339850519,0.7917338013648987
42,-Metrics/Training(Step): loss,1610339854241,0.965889573097229
44,-Metrics/Training(Step): loss,1610339857761,1.0606833696365356
46,-Metrics/Training(Step): loss,1610339860377,0.7979585528373718
48,-Metrics/Training(Step): loss,1610339863021,0.8904805183410645
50,-Metrics/Training(Step): loss,1610339865260,0.7846781611442566
52,-Metrics/Training(Step): loss,1610339867324,0.9051342606544495
54,-Metrics/Training(Step): loss,1610339869380,0.8920297622680664
56,-Metrics/Training(Step): loss,1610339871496,1.2097622156143188
58,-Metrics/Training(Step): loss,1610339873567,1.1389577388763428
60,-Metrics/Training(Step): loss,1610339875662,1.005092740058899
62,-Metrics/Training(Step): loss,1610339878024,1.0273606777191162
64,-Metrics/Training(Step): loss,1610339880091,0.8084088563919067
66,-Metrics/Training(Step): loss,1610339881972,0.9813117384910583
68,-Metrics/Training(Step): loss,1610339884068,1.2231786251068115
70,-Metrics/Training(Step): loss,1610339885934,1.0768725872039795
72,-Metrics/Training(Step): loss,1610339887877,1.104655146598816
74,-Metrics/Training(Step): loss,1610339889903,0.9337393045425415
76,-Metrics/Training(Step): loss,1610339891717,0.8505272269248962
78,-Metrics/Training(Step): loss,1610339893723,1.0298008918762207
80,-Metrics/Training(Step): loss,1610339895775,1.2072813510894775
82,-Metrics/Training(Step): loss,1610339897788,1.04415762424469
84,-Metrics/Training(Step): loss,1610339899750,0.8154706358909607
86,-Metrics/Training(Step): loss,1610339901765,0.9713699817657471
88,-Metrics/Training(Step): loss,1610339912821,0.8220980763435364
90,-Metrics/Training(Step): loss,1610339916729,1.2397260665893555
92,-Metrics/Training(Step): loss,1610339920520,1.1797471046447754
94,-Metrics/Training(Step): loss,1610339924419,0.9117338061332703
96,-Metrics/Training(Step): loss,1610339928220,0.9191354513168335
98,-Metrics/Training(Step): loss,1610339932240,0.9613458514213562
100,-Metrics/Training(Step): loss,1610339936158,1.0080664157867432
102,-Metrics/Training(Step): loss,1610339940031,0.7462430000305176
104,-Metrics/Training(Step): loss,1610339944340,0.6750813722610474
106,-Metrics/Training(Step): loss,1610339948257,0.8468528985977173
108,-Metrics/Training(Step): loss,1610339952339,1.1724269390106201
110,-Metrics/Training(Step): loss,1610339956119,0.9055574536323547
112,-Metrics/Training(Step): loss,1610339959434,0.7416666150093079
114,-Metrics/Training(Step): loss,1610339962920,0.8828340768814087
116,-Metrics/Training(Step): loss,1610339966822,1.0188082456588745
118,-Metrics/Training(Step): loss,1610339970357,0.7532101273536682
120,-Metrics/Training(Step): loss,1610339974223,1.0796152353286743
122,-Metrics/Training(Step): loss,1610339978044,0.8215329647064209
124,-Metrics/Training(Step): loss,1610339982119,0.9239758253097534
126,-Metrics/Training(Step): loss,1610339985180,0.8313087224960327
128,-Metrics/Training(Step): loss,1610339989352,1.0344997644424438
130,-Metrics/Training(Step): loss,1610339992786,1.0759475231170654
132,-Metrics/Training(Step): loss,1610339996768,0.8302078247070312
134,-Metrics/Training(Step): loss,1610339999562,0.8757913708686829
136,-Metrics/Training(Step): loss,1610340002408,0.9190927147865295
138,-Metrics/Training(Step): loss,1610340004417,1.1706383228302002
140,-Metrics/Training(Step): loss,1610340006535,0.7114010453224182
142,-Metrics/Training(Step): loss,1610340008619,0.8392355442047119
144,-Metrics/Training(Step): loss,1610340010688,0.6743487119674683
146,-Metrics/Training(Step): loss,1610340012749,0.7043072581291199
148,-Metrics/Training(Step): loss,1610340014777,0.7318102717399597
150,-Metrics/Training(Step): loss,1610340016802,0.6732525825500488
152,-Metrics/Training(Step): loss,1610340018830,0.8183172941207886
154,-Metrics/Training(Step): loss,1610340020861,0.9777277708053589
156,-Metrics/Training(Step): loss,1610340022850,0.6668663620948792
158,-Metrics/Training(Step): loss,1610340024761,0.7486362457275391
160,-Metrics/Training(Step): loss,1610340026674,0.8713942766189575
162,-Metrics/Training(Step): loss,1610340028652,0.893672525882721
164,-Metrics/Training(Step): loss,1610340030656,0.8981017470359802
166,-Metrics/Training(Step): loss,1610340032472,0.8730626702308655
168,-Metrics/Training(Step): loss,1610340034470,0.6796451210975647
170,-Metrics/Training(Step): loss,1610340036433,0.921859085559845
172,-Metrics/Training(Step): loss,1610340038448,0.8616587519645691
174,-Metrics/Training(Step): loss,1610340050720,0.9684665203094482
176,-Metrics/Training(Step): loss,1610340054719,0.7618001103401184
178,-Metrics/Training(Step): loss,1610340058552,0.8664506673812866
180,-Metrics/Training(Step): loss,1610340062517,0.9881502389907837
182,-Metrics/Training(Step): loss,1610340066532,0.7586612701416016
184,-Metrics/Training(Step): loss,1610340070221,0.8502339720726013
186,-Metrics/Training(Step): loss,1610340074119,0.8501742482185364
188,-Metrics/Training(Step): loss,1610340078219,0.7090501189231873
190,-Metrics/Training(Step): loss,1610340082020,0.6653189063072205
192,-Metrics/Training(Step): loss,1610340084719,0.7595223784446716
194,-Metrics/Training(Step): loss,1610340087922,0.9923303723335266
196,-Metrics/Training(Step): loss,1610340091667,0.8376708030700684
198,-Metrics/Training(Step): loss,1610340094985,0.9021578431129456
200,-Metrics/Training(Step): loss,1610340098328,0.6783621311187744
202,-Metrics/Training(Step): loss,1610340101338,0.7494972944259644
204,-Metrics/Training(Step): loss,1610340105257,0.8972420692443848
206,-Metrics/Training(Step): loss,1610340108871,0.9679020047187805
208,-Metrics/Training(Step): loss,1610340112418,0.794728696346283
210,-Metrics/Training(Step): loss,1610340116321,0.8609736561775208
212,-Metrics/Training(Step): loss,1610340119919,0.8260023593902588
214,-Metrics/Training(Step): loss,1610340123917,0.9086925983428955
216,-Metrics/Training(Step): loss,1610340127188,1.0574833154678345
218,-Metrics/Training(Step): loss,1610340130169,0.7990245223045349
220,-Metrics/Training(Step): loss,1610340132694,0.7834687232971191
222,-Metrics/Training(Step): loss,1610340134904,0.8195855021476746
224,-Metrics/Training(Step): loss,1610340136849,0.9354790449142456
226,-Metrics/Training(Step): loss,1610340138907,0.9028967618942261
228,-Metrics/Training(Step): loss,1610340140998,0.82083660364151
230,-Metrics/Training(Step): loss,1610340143040,1.5237799882888794
232,-Metrics/Training(Step): loss,1610340145121,1.089775800704956
234,-Metrics/Training(Step): loss,1610340147134,0.7290325164794922
236,-Metrics/Training(Step): loss,1610340149039,0.8247483372688293
238,-Metrics/Training(Step): loss,1610340151123,0.7647866010665894
240,-Metrics/Training(Step): loss,1610340153165,0.845794141292572
242,-Metrics/Training(Step): loss,1610340155225,1.0504696369171143
244,-Metrics/Training(Step): loss,1610340157215,0.6625422835350037
246,-Metrics/Training(Step): loss,1610340159219,0.6847180128097534
248,-Metrics/Training(Step): loss,1610340161307,0.5924509763717651
250,-Metrics/Training(Step): loss,1610340163350,0.8679282665252686
252,-Metrics/Training(Step): loss,1610340165369,0.7690429091453552
254,-Metrics/Training(Step): loss,1610340167369,0.7156736254692078
256,-Metrics/Training(Step): loss,1610340169409,0.9107198715209961
258,-Metrics/Training(Step): loss,1610340171426,0.7109704613685608
260,-Metrics/Training(Step): loss,1610340184119,0.7771478891372681
262,-Metrics/Training(Step): loss,1610340187829,0.8386892080307007
264,-Metrics/Training(Step): loss,1610340191628,0.7376103401184082
266,-Metrics/Training(Step): loss,1610340195248,0.7874120473861694
268,-Metrics/Training(Step): loss,1610340199019,0.665291428565979
270,-Metrics/Training(Step): loss,1610340203123,0.7180495858192444
272,-Metrics/Training(Step): loss,1610340206820,0.9097789525985718
274,-Metrics/Training(Step): loss,1610340210618,0.6315346360206604
276,-Metrics/Training(Step): loss,1610340214361,0.7409416437149048
278,-Metrics/Training(Step): loss,1610340217881,0.828364372253418
280,-Metrics/Training(Step): loss,1610340221819,0.7807286977767944
282,-Metrics/Training(Step): loss,1610340225528,0.7955307960510254
284,-Metrics/Training(Step): loss,1610340229521,0.8203933835029602
286,-Metrics/Training(Step): loss,1610340232732,0.9632447361946106
288,-Metrics/Training(Step): loss,1610340236358,0.6475192904472351
290,-Metrics/Training(Step): loss,1610340240274,0.7368935942649841
292,-Metrics/Training(Step): loss,1610340243833,1.0166958570480347
294,-Metrics/Training(Step): loss,1610340247219,0.6738726496696472
296,-Metrics/Training(Step): loss,1610340250132,0.7480690479278564
298,-Metrics/Training(Step): loss,1610340253626,0.8302323222160339
300,-Metrics/Training(Step): loss,1610340257222,0.7830109596252441
302,-Metrics/Training(Step): loss,1610340261045,0.6320835947990417
304,-Metrics/Training(Step): loss,1610340264642,0.8824769258499146
306,-Metrics/Training(Step): loss,1610340267018,0.7386810183525085
308,-Metrics/Training(Step): loss,1610340269136,0.6456576585769653
310,-Metrics/Training(Step): loss,1610340271293,0.9638742804527283
312,-Metrics/Training(Step): loss,1610340273357,0.8921526074409485
314,-Metrics/Training(Step): loss,1610340275381,0.6606483459472656
316,-Metrics/Training(Step): loss,1610340277462,0.7028896808624268
318,-Metrics/Training(Step): loss,1610340279530,0.9493191838264465
320,-Metrics/Training(Step): loss,1610340281530,0.7238470315933228
322,-Metrics/Training(Step): loss,1610340283557,0.800790011882782
324,-Metrics/Training(Step): loss,1610340285450,0.7453427314758301
326,-Metrics/Training(Step): loss,1610340287264,0.8053340315818787
328,-Metrics/Training(Step): loss,1610340289333,0.5581750273704529
330,-Metrics/Training(Step): loss,1610340291046,0.6258551478385925
332,-Metrics/Training(Step): loss,1610340293093,0.7674946784973145
334,-Metrics/Training(Step): loss,1610340295138,0.7360153198242188
336,-Metrics/Training(Step): loss,1610340297156,0.6932184100151062
338,-Metrics/Training(Step): loss,1610340299175,0.7585037350654602
340,-Metrics/Training(Step): loss,1610340301232,0.6468608975410461
342,-Metrics/Training(Step): loss,1610340303247,0.7045950889587402
344,-Metrics/Training(Step): loss,1610340305262,0.613827109336853
346,-Metrics/Training(Step): loss,1610340317535,0.6807966828346252
348,-Metrics/Training(Step): loss,1610340321319,0.5918387770652771
350,-Metrics/Training(Step): loss,1610340325221,0.6951149702072144
352,-Metrics/Training(Step): loss,1610340329219,0.5044512152671814
354,-Metrics/Training(Step): loss,1610340333122,0.5160800218582153
356,-Metrics/Training(Step): loss,1610340336858,0.6491423845291138
358,-Metrics/Training(Step): loss,1610340340620,0.6262814998626709
360,-Metrics/Training(Step): loss,1610340344820,0.5620762705802917
362,-Metrics/Training(Step): loss,1610340348534,0.6095190048217773
364,-Metrics/Training(Step): loss,1610340351733,0.5674261450767517
366,-Metrics/Training(Step): loss,1610340355333,0.5412563681602478
368,-Metrics/Training(Step): loss,1610340358676,0.4886840879917145
370,-Metrics/Training(Step): loss,1610340362056,0.9435437917709351
372,-Metrics/Training(Step): loss,1610340365820,0.6596445441246033
374,-Metrics/Training(Step): loss,1610340369125,0.7367177605628967
376,-Metrics/Training(Step): loss,1610340372523,0.7291855812072754
378,-Metrics/Training(Step): loss,1610340376119,0.6013211011886597
380,-Metrics/Training(Step): loss,1610340379056,0.6829763054847717
382,-Metrics/Training(Step): loss,1610340382724,0.7183199524879456
384,-Metrics/Training(Step): loss,1610340386083,0.7858608365058899
386,-Metrics/Training(Step): loss,1610340389431,0.6187076568603516
388,-Metrics/Training(Step): loss,1610340393420,0.6870195269584656
390,-Metrics/Training(Step): loss,1610340396619,0.5345058441162109
392,-Metrics/Training(Step): loss,1610340399768,0.932237446308136
394,-Metrics/Training(Step): loss,1610340402140,0.49111610651016235
396,-Metrics/Training(Step): loss,1610340404393,0.5211557745933533
398,-Metrics/Training(Step): loss,1610340406449,0.7320966124534607
400,-Metrics/Training(Step): loss,1610340408396,0.5542764067649841
402,-Metrics/Training(Step): loss,1610340410447,0.840948224067688
404,-Metrics/Training(Step): loss,1610340412561,0.6092572808265686
406,-Metrics/Training(Step): loss,1610340414636,0.6295647621154785
408,-Metrics/Training(Step): loss,1610340416702,0.6626700758934021
410,-Metrics/Training(Step): loss,1610340418745,0.7412965893745422
412,-Metrics/Training(Step): loss,1610340420763,0.7528666257858276
414,-Metrics/Training(Step): loss,1610340422847,0.5505775809288025
416,-Metrics/Training(Step): loss,1610340424917,0.7603054642677307
418,-Metrics/Training(Step): loss,1610340427002,0.8103097081184387
420,-Metrics/Training(Step): loss,1610340429046,0.6585787534713745
422,-Metrics/Training(Step): loss,1610340431071,0.69532710313797
424,-Metrics/Training(Step): loss,1610340433023,0.5083020329475403
426,-Metrics/Training(Step): loss,1610340435051,0.8187223076820374
428,-Metrics/Training(Step): loss,1610340437099,0.5967451930046082
430,-Metrics/Training(Step): loss,1610340438996,0.5657883882522583
432,-Metrics/Training(Step): loss,1610340450229,0.7503244876861572
434,-Metrics/Training(Step): loss,1610340454319,0.7789632081985474
436,-Metrics/Training(Step): loss,1610340458122,0.6743716597557068
438,-Metrics/Training(Step): loss,1610340461919,0.6365671753883362
440,-Metrics/Training(Step): loss,1610340465947,0.8143773078918457
442,-Metrics/Training(Step): loss,1610340469736,0.8720570802688599
444,-Metrics/Training(Step): loss,1610340474021,0.7378997206687927
446,-Metrics/Training(Step): loss,1610340478220,0.6757817268371582
448,-Metrics/Training(Step): loss,1610340482199,0.8244928121566772
450,-Metrics/Training(Step): loss,1610340485719,0.6518018245697021
452,-Metrics/Training(Step): loss,1610340488919,0.7795565128326416
454,-Metrics/Training(Step): loss,1610340492525,0.6584511399269104
456,-Metrics/Training(Step): loss,1610340496048,0.49775034189224243
458,-Metrics/Training(Step): loss,1610340499377,0.9156304001808167
460,-Metrics/Training(Step): loss,1610340503219,0.6107645034790039
462,-Metrics/Training(Step): loss,1610340506962,0.5918055176734924
464,-Metrics/Training(Step): loss,1610340510234,0.7292183637619019
466,-Metrics/Training(Step): loss,1610340513534,0.6158849596977234
468,-Metrics/Training(Step): loss,1610340516808,0.6212418675422668
470,-Metrics/Training(Step): loss,1610340520365,0.6766431927680969
472,-Metrics/Training(Step): loss,1610340524720,0.5646399259567261
474,-Metrics/Training(Step): loss,1610340528297,0.6238119602203369
476,-Metrics/Training(Step): loss,1610340531387,0.5147135853767395
478,-Metrics/Training(Step): loss,1610340534163,0.6967334747314453
480,-Metrics/Training(Step): loss,1610340536344,0.6407901048660278
482,-Metrics/Training(Step): loss,1610340538378,0.4578227400779724
484,-Metrics/Training(Step): loss,1610340540381,0.6978155970573425
486,-Metrics/Training(Step): loss,1610340542428,0.7466633319854736
488,-Metrics/Training(Step): loss,1610340544507,0.7264893054962158
490,-Metrics/Training(Step): loss,1610340546576,0.5566969513893127
492,-Metrics/Training(Step): loss,1610340548563,0.542754590511322
494,-Metrics/Training(Step): loss,1610340550501,0.7184689044952393
496,-Metrics/Training(Step): loss,1610340552519,0.4983729422092438
498,-Metrics/Training(Step): loss,1610340554522,0.5821858048439026
500,-Metrics/Training(Step): loss,1610340556552,0.7919022440910339
502,-Metrics/Training(Step): loss,1610340558622,0.5628880858421326
504,-Metrics/Training(Step): loss,1610340560707,0.5814508199691772
506,-Metrics/Training(Step): loss,1610340562703,0.4699874520301819
508,-Metrics/Training(Step): loss,1610340564747,0.8835108876228333
510,-Metrics/Training(Step): loss,1610340566762,0.5328764319419861
512,-Metrics/Training(Step): loss,1610340568756,0.6968626379966736
514,-Metrics/Training(Step): loss,1610340570800,0.7515678405761719
516,-Metrics/Training(Step): loss,1610340572820,0.6999412178993225
518,-Metrics/Training(Step): loss,1610340584734,0.6658608317375183
520,-Metrics/Training(Step): loss,1610340588721,0.7272871136665344
522,-Metrics/Training(Step): loss,1610340593024,0.5625669360160828
524,-Metrics/Training(Step): loss,1610340597138,0.7624395489692688
526,-Metrics/Training(Step): loss,1610340601053,0.702263593673706
528,-Metrics/Training(Step): loss,1610340604919,0.54618239402771
530,-Metrics/Training(Step): loss,1610340608720,0.5667731761932373
532,-Metrics/Training(Step): loss,1610340612425,0.6416358947753906
534,-Metrics/Training(Step): loss,1610340616119,0.7710176110267639
536,-Metrics/Training(Step): loss,1610340619678,0.5783065557479858
538,-Metrics/Training(Step): loss,1610340623235,0.5922630429267883
540,-Metrics/Training(Step): loss,1610340626919,0.8138917088508606
542,-Metrics/Training(Step): loss,1610340630955,0.972602128982544
544,-Metrics/Training(Step): loss,1610340634825,0.6039077639579773
546,-Metrics/Training(Step): loss,1610340638546,0.6207563877105713
548,-Metrics/Training(Step): loss,1610340641958,0.652615487575531
550,-Metrics/Training(Step): loss,1610340645540,0.8639571070671082
552,-Metrics/Training(Step): loss,1610340648692,0.6396797895431519
554,-Metrics/Training(Step): loss,1610340651898,0.6098480224609375
556,-Metrics/Training(Step): loss,1610340655883,0.6449655890464783
558,-Metrics/Training(Step): loss,1610340659536,0.7181729674339294
560,-Metrics/Training(Step): loss,1610340663089,0.42162638902664185
562,-Metrics/Training(Step): loss,1610340666463,0.7149915099143982
564,-Metrics/Training(Step): loss,1610340669087,0.59959876537323
566,-Metrics/Training(Step): loss,1610340671266,0.616975724697113
568,-Metrics/Training(Step): loss,1610340673325,0.5518785715103149
570,-Metrics/Training(Step): loss,1610340675340,0.7683929204940796
572,-Metrics/Training(Step): loss,1610340677426,0.7903408408164978
574,-Metrics/Training(Step): loss,1610340679523,0.6240713000297546
576,-Metrics/Training(Step): loss,1610340681552,0.6005419492721558
578,-Metrics/Training(Step): loss,1610340683566,0.7234600186347961
580,-Metrics/Training(Step): loss,1610340685655,0.5839000344276428
582,-Metrics/Training(Step): loss,1610340687733,0.6539713740348816
584,-Metrics/Training(Step): loss,1610340689791,0.6789904832839966
586,-Metrics/Training(Step): loss,1610340691719,0.5271288156509399
588,-Metrics/Training(Step): loss,1610340693526,0.6468002200126648
590,-Metrics/Training(Step): loss,1610340695743,0.42847079038619995
592,-Metrics/Training(Step): loss,1610340697924,0.7299806475639343
594,-Metrics/Training(Step): loss,1610340699961,0.5506476163864136
596,-Metrics/Training(Step): loss,1610340701927,0.4054175317287445
598,-Metrics/Training(Step): loss,1610340704106,0.611783504486084
600,-Metrics/Training(Step): loss,1610340706161,0.5302208662033081
602,-Metrics/Training(Step): loss,1610340708227,0.5990692377090454
604,-Metrics/Training(Step): loss,1610340720629,0.5489796996116638
606,-Metrics/Training(Step): loss,1610340724731,0.49314799904823303
608,-Metrics/Training(Step): loss,1610340728858,0.6029484868049622
610,-Metrics/Training(Step): loss,1610340732728,0.48678603768348694
612,-Metrics/Training(Step): loss,1610340737022,0.500555694103241
614,-Metrics/Training(Step): loss,1610340741419,0.6438595652580261
616,-Metrics/Training(Step): loss,1610340745327,0.7666544318199158
618,-Metrics/Training(Step): loss,1610340749145,0.5868012309074402
620,-Metrics/Training(Step): loss,1610340753019,0.7734538316726685
622,-Metrics/Training(Step): loss,1610340756906,0.623599112033844
624,-Metrics/Training(Step): loss,1610340760531,0.6561844348907471
626,-Metrics/Training(Step): loss,1610340763822,0.5338807702064514
628,-Metrics/Training(Step): loss,1610340767841,0.508039653301239
630,-Metrics/Training(Step): loss,1610340771383,0.4848727583885193
632,-Metrics/Training(Step): loss,1610340775320,0.5917859077453613
634,-Metrics/Training(Step): loss,1610340779320,0.758354663848877
636,-Metrics/Training(Step): loss,1610340782996,0.6175914406776428
638,-Metrics/Training(Step): loss,1610340787056,0.5222547650337219
640,-Metrics/Training(Step): loss,1610340790590,0.6358823180198669
642,-Metrics/Training(Step): loss,1610340794244,0.8800027370452881
644,-Metrics/Training(Step): loss,1610340798020,0.7572826147079468
646,-Metrics/Training(Step): loss,1610340801041,0.4891793131828308
648,-Metrics/Training(Step): loss,1610340804081,0.6813021898269653
650,-Metrics/Training(Step): loss,1610340806636,0.7916569709777832
652,-Metrics/Training(Step): loss,1610340808775,0.6669250726699829
654,-Metrics/Training(Step): loss,1610340810852,0.6939846277236938
656,-Metrics/Training(Step): loss,1610340812732,0.5592688918113708
658,-Metrics/Training(Step): loss,1610340814742,0.7207208871841431
660,-Metrics/Training(Step): loss,1610340816861,0.7989811897277832
662,-Metrics/Training(Step): loss,1610340818995,0.7883031964302063
664,-Metrics/Training(Step): loss,1610340821055,0.7043797969818115
666,-Metrics/Training(Step): loss,1610340823068,0.4615379869937897
668,-Metrics/Training(Step): loss,1610340825132,0.5585506558418274
670,-Metrics/Training(Step): loss,1610340827149,0.5942710041999817
672,-Metrics/Training(Step): loss,1610340829150,0.6345222592353821
674,-Metrics/Training(Step): loss,1610340831250,0.5103293061256409
676,-Metrics/Training(Step): loss,1610340833366,0.5893647074699402
678,-Metrics/Training(Step): loss,1610340835181,0.5780147314071655
680,-Metrics/Training(Step): loss,1610340837196,0.44478362798690796
682,-Metrics/Training(Step): loss,1610340839114,0.4264630973339081
684,-Metrics/Training(Step): loss,1610340841147,0.5625501275062561
686,-Metrics/Training(Step): loss,1610340843178,0.6907086968421936
688,-Metrics/Training(Step): loss,1610340845228,0.4842336177825928
690,-Metrics/Training(Step): loss,1610340857146,0.5312582850456238
692,-Metrics/Training(Step): loss,1610340861521,0.478666752576828
694,-Metrics/Training(Step): loss,1610340865641,0.574680507183075
696,-Metrics/Training(Step): loss,1610340869657,0.6180942058563232
698,-Metrics/Training(Step): loss,1610340873739,0.5115547776222229
700,-Metrics/Training(Step): loss,1610340877724,0.61365807056427
702,-Metrics/Training(Step): loss,1610340881727,0.7126675248146057
704,-Metrics/Training(Step): loss,1610340885419,0.4525870382785797
706,-Metrics/Training(Step): loss,1610340888748,0.5717864036560059
708,-Metrics/Training(Step): loss,1610340892331,0.4100077450275421
710,-Metrics/Training(Step): loss,1610340896144,0.6247537732124329
712,-Metrics/Training(Step): loss,1610340899715,0.4874381721019745
714,-Metrics/Training(Step): loss,1610340903519,0.4701167643070221
716,-Metrics/Training(Step): loss,1610340907119,0.548065185546875
718,-Metrics/Training(Step): loss,1610340910558,0.46962445974349976
720,-Metrics/Training(Step): loss,1610340913622,0.6173343658447266
722,-Metrics/Training(Step): loss,1610340917613,0.6198098659515381
724,-Metrics/Training(Step): loss,1610340921015,0.5632373690605164
726,-Metrics/Training(Step): loss,1610340924409,0.5645453333854675
728,-Metrics/Training(Step): loss,1610340927915,0.4801904559135437
730,-Metrics/Training(Step): loss,1610340931097,0.4629993736743927
732,-Metrics/Training(Step): loss,1610340934416,0.47069790959358215
734,-Metrics/Training(Step): loss,1610340937920,0.5713366270065308
736,-Metrics/Training(Step): loss,1610340940739,0.5284950137138367
738,-Metrics/Training(Step): loss,1610340942938,0.5820268988609314
740,-Metrics/Training(Step): loss,1610340944961,0.37451401352882385
742,-Metrics/Training(Step): loss,1610340947036,0.45963335037231445
744,-Metrics/Training(Step): loss,1610340949097,0.5169581174850464
746,-Metrics/Training(Step): loss,1610340951192,0.5918205976486206
748,-Metrics/Training(Step): loss,1610340953274,0.4246407747268677
750,-Metrics/Training(Step): loss,1610340955366,0.5678976774215698
752,-Metrics/Training(Step): loss,1610340957331,0.5735012888908386
754,-Metrics/Training(Step): loss,1610340959444,0.5195544958114624
756,-Metrics/Training(Step): loss,1610340961484,0.403927206993103
758,-Metrics/Training(Step): loss,1610340963474,0.6502272486686707
760,-Metrics/Training(Step): loss,1610340965353,0.4127481281757355
762,-Metrics/Training(Step): loss,1610340967313,0.4252608120441437
764,-Metrics/Training(Step): loss,1610340969383,0.5176274180412292
766,-Metrics/Training(Step): loss,1610340971358,0.7081550359725952
768,-Metrics/Training(Step): loss,1610340973420,0.6214276552200317
770,-Metrics/Training(Step): loss,1610340975422,0.5561816692352295
772,-Metrics/Training(Step): loss,1610340977476,0.5417335629463196
774,-Metrics/Training(Step): loss,1610340979587,0.5947312116622925
776,-Metrics/Training(Step): loss,1610340991245,0.47142258286476135
778,-Metrics/Training(Step): loss,1610340995423,0.5382239818572998
780,-Metrics/Training(Step): loss,1610340999419,0.5002092719078064
782,-Metrics/Training(Step): loss,1610341003531,0.46490076184272766
784,-Metrics/Training(Step): loss,1610341007533,0.4935358166694641
786,-Metrics/Training(Step): loss,1610341011515,0.5510983467102051
788,-Metrics/Training(Step): loss,1610341015323,0.5830925107002258
790,-Metrics/Training(Step): loss,1610341019021,0.5567703247070312
792,-Metrics/Training(Step): loss,1610341022018,0.6802676320075989
794,-Metrics/Training(Step): loss,1610341025461,0.5718948245048523
796,-Metrics/Training(Step): loss,1610341028850,0.4500632584095001
798,-Metrics/Training(Step): loss,1610341032966,0.46234792470932007
800,-Metrics/Training(Step): loss,1610341036817,0.5853994488716125
802,-Metrics/Training(Step): loss,1610341040419,0.4794425368309021
804,-Metrics/Training(Step): loss,1610341043819,0.5311912894248962
806,-Metrics/Training(Step): loss,1610341047250,0.5775679349899292
808,-Metrics/Training(Step): loss,1610341050939,0.5613847970962524
810,-Metrics/Training(Step): loss,1610341054443,0.5037408471107483
812,-Metrics/Training(Step): loss,1610341058120,0.49208882451057434
814,-Metrics/Training(Step): loss,1610341062020,0.49594756960868835
816,-Metrics/Training(Step): loss,1610341065732,0.5256385803222656
818,-Metrics/Training(Step): loss,1610341068744,0.4117235243320465
820,-Metrics/Training(Step): loss,1610341071819,0.7152291536331177
822,-Metrics/Training(Step): loss,1610341074974,0.8521508574485779
824,-Metrics/Training(Step): loss,1610341077490,0.5969901084899902
826,-Metrics/Training(Step): loss,1610341079571,0.5937696695327759
828,-Metrics/Training(Step): loss,1610341081665,0.6837158203125
830,-Metrics/Training(Step): loss,1610341083802,0.5770647525787354
832,-Metrics/Training(Step): loss,1610341085843,0.5241517424583435
834,-Metrics/Training(Step): loss,1610341087954,0.5674044489860535
836,-Metrics/Training(Step): loss,1610341089894,0.5186011791229248
838,-Metrics/Training(Step): loss,1610341091901,0.6310238838195801
840,-Metrics/Training(Step): loss,1610341094040,0.580880343914032
842,-Metrics/Training(Step): loss,1610341096129,0.5497156977653503
844,-Metrics/Training(Step): loss,1610341098138,0.6755576133728027
846,-Metrics/Training(Step): loss,1610341100251,0.4927205443382263
848,-Metrics/Training(Step): loss,1610341102243,0.35749465227127075
850,-Metrics/Training(Step): loss,1610341104287,0.4001038372516632
852,-Metrics/Training(Step): loss,1610341106408,0.4973677396774292
854,-Metrics/Training(Step): loss,1610341108492,0.5583716034889221
856,-Metrics/Training(Step): loss,1610341110569,0.6083810329437256
858,-Metrics/Training(Step): loss,1610341112681,0.6694493889808655
860,-Metrics/Training(Step): loss,1610341114724,0.4982893466949463
862,-Metrics/Training(Step): loss,1610341126830,0.4528752565383911
864,-Metrics/Training(Step): loss,1610341131219,0.604053258895874
866,-Metrics/Training(Step): loss,1610341135438,0.551910400390625
868,-Metrics/Training(Step): loss,1610341139437,0.3548978567123413
870,-Metrics/Training(Step): loss,1610341143219,0.41657954454421997
872,-Metrics/Training(Step): loss,1610341147219,0.5704763531684875
874,-Metrics/Training(Step): loss,1610341151822,0.5257406830787659
876,-Metrics/Training(Step): loss,1610341155791,0.4763891398906708
878,-Metrics/Training(Step): loss,1610341159224,0.5226877331733704
880,-Metrics/Training(Step): loss,1610341163021,0.5353790521621704
882,-Metrics/Training(Step): loss,1610341166520,0.5319452285766602
884,-Metrics/Training(Step): loss,1610341170120,0.6270961165428162
886,-Metrics/Training(Step): loss,1610341173810,0.5190216302871704
888,-Metrics/Training(Step): loss,1610341177543,0.49008506536483765
890,-Metrics/Training(Step): loss,1610341181020,0.41438794136047363
892,-Metrics/Training(Step): loss,1610341184226,0.5572587251663208
894,-Metrics/Training(Step): loss,1610341187490,0.3886798024177551
896,-Metrics/Training(Step): loss,1610341190643,0.3984906077384949
898,-Metrics/Training(Step): loss,1610341194430,0.511816680431366
900,-Metrics/Training(Step): loss,1610341198319,0.3924902379512787
902,-Metrics/Training(Step): loss,1610341201619,0.46572229266166687
904,-Metrics/Training(Step): loss,1610341204819,0.5894110202789307
906,-Metrics/Training(Step): loss,1610341208332,0.4068102538585663
908,-Metrics/Training(Step): loss,1610341211368,0.5328012704849243
910,-Metrics/Training(Step): loss,1610341213744,0.39262574911117554
912,-Metrics/Training(Step): loss,1610341215905,0.4829878509044647
914,-Metrics/Training(Step): loss,1610341218047,0.5550404787063599
916,-Metrics/Training(Step): loss,1610341220136,0.6178001165390015
918,-Metrics/Training(Step): loss,1610341222207,0.46969905495643616
920,-Metrics/Training(Step): loss,1610341224313,0.6034916639328003
922,-Metrics/Training(Step): loss,1610341226337,0.6173659563064575
924,-Metrics/Training(Step): loss,1610341228429,0.5082387328147888
926,-Metrics/Training(Step): loss,1610341230530,0.5060921311378479
928,-Metrics/Training(Step): loss,1610341232573,0.48563459515571594
930,-Metrics/Training(Step): loss,1610341234473,0.5093675851821899
932,-Metrics/Training(Step): loss,1610341236536,0.5210217833518982
934,-Metrics/Training(Step): loss,1610341238522,0.7365552186965942
936,-Metrics/Training(Step): loss,1610341240558,0.5444195866584778
938,-Metrics/Training(Step): loss,1610341242470,0.7260805368423462
940,-Metrics/Training(Step): loss,1610341244460,0.502885639667511
942,-Metrics/Training(Step): loss,1610341246371,0.5439026951789856
944,-Metrics/Training(Step): loss,1610341248414,0.7901285290718079
946,-Metrics/Training(Step): loss,1610341250439,0.4562561511993408
948,-Metrics/Training(Step): loss,1610341264629,0.6096619367599487
950,-Metrics/Training(Step): loss,1610341268421,0.5175126194953918
952,-Metrics/Training(Step): loss,1610341272020,0.7747827172279358
954,-Metrics/Training(Step): loss,1610341275858,0.4477429986000061
956,-Metrics/Training(Step): loss,1610341279826,0.48716166615486145
958,-Metrics/Training(Step): loss,1610341284220,0.4961240291595459
960,-Metrics/Training(Step): loss,1610341288321,0.6003856658935547
962,-Metrics/Training(Step): loss,1610341292417,0.47523292899131775
964,-Metrics/Training(Step): loss,1610341296010,0.37203049659729004
966,-Metrics/Training(Step): loss,1610341299158,0.5473852753639221
968,-Metrics/Training(Step): loss,1610341302991,0.5821194052696228
970,-Metrics/Training(Step): loss,1610341305949,0.5383133292198181
972,-Metrics/Training(Step): loss,1610341309948,0.5382316708564758
974,-Metrics/Training(Step): loss,1610341313819,0.4148879945278168
976,-Metrics/Training(Step): loss,1610341317321,0.6119267344474792
978,-Metrics/Training(Step): loss,1610341320623,0.5622570514678955
980,-Metrics/Training(Step): loss,1610341324323,0.41619595885276794
982,-Metrics/Training(Step): loss,1610341327929,0.6237145066261292
984,-Metrics/Training(Step): loss,1610341331529,0.4262745976448059
986,-Metrics/Training(Step): loss,1610341334619,0.5489017963409424
988,-Metrics/Training(Step): loss,1610341338220,0.48496633768081665
990,-Metrics/Training(Step): loss,1610341341420,0.4106014668941498
992,-Metrics/Training(Step): loss,1610341344919,0.47805413603782654
994,-Metrics/Training(Step): loss,1610341348062,0.4043837785720825
996,-Metrics/Training(Step): loss,1610341350245,0.6252349019050598
998,-Metrics/Training(Step): loss,1610341352462,0.4727039933204651
1000,-Metrics/Training(Step): loss,1610341354651,0.46635347604751587
1002,-Metrics/Training(Step): loss,1610341356751,0.4360509216785431
1004,-Metrics/Training(Step): loss,1610341358880,0.4581064283847809
1006,-Metrics/Training(Step): loss,1610341360993,0.46527284383773804
1008,-Metrics/Training(Step): loss,1610341363104,0.580434262752533
1010,-Metrics/Training(Step): loss,1610341365198,0.6464055180549622
1012,-Metrics/Training(Step): loss,1610341367245,0.47786152362823486
1014,-Metrics/Training(Step): loss,1610341369351,0.547159731388092
1016,-Metrics/Training(Step): loss,1610341371333,0.639136552810669
1018,-Metrics/Training(Step): loss,1610341373270,0.44676995277404785
1020,-Metrics/Training(Step): loss,1610341375369,0.38679757714271545
1022,-Metrics/Training(Step): loss,1610341377394,0.4703395664691925
1024,-Metrics/Training(Step): loss,1610341379439,0.5483890175819397
1026,-Metrics/Training(Step): loss,1610341381423,0.5477874875068665
1028,-Metrics/Training(Step): loss,1610341383408,0.551895797252655
1030,-Metrics/Training(Step): loss,1610341385443,0.47019943594932556
1032,-Metrics/Training(Step): loss,1610341387491,0.4338170289993286
1034,-Metrics/Training(Step): loss,1610341400720,0.3979587256908417
1036,-Metrics/Training(Step): loss,1610341404827,0.45552533864974976
1038,-Metrics/Training(Step): loss,1610341409022,0.5580743551254272
1040,-Metrics/Training(Step): loss,1610341412920,0.581348717212677
1042,-Metrics/Training(Step): loss,1610341417226,0.5509020090103149
1044,-Metrics/Training(Step): loss,1610341420722,0.5051867961883545
1046,-Metrics/Training(Step): loss,1610341424819,0.3826083540916443
1048,-Metrics/Training(Step): loss,1610341428764,0.6387074589729309
1050,-Metrics/Training(Step): loss,1610341432420,0.7549787163734436
1052,-Metrics/Training(Step): loss,1610341436020,0.46575069427490234
1054,-Metrics/Training(Step): loss,1610341439026,0.55958491563797
1056,-Metrics/Training(Step): loss,1610341443038,0.49420544505119324
1058,-Metrics/Training(Step): loss,1610341446657,0.485202819108963
1060,-Metrics/Training(Step): loss,1610341449966,0.49225619435310364
1062,-Metrics/Training(Step): loss,1610341453527,0.45064809918403625
1064,-Metrics/Training(Step): loss,1610341457224,0.42156675457954407
1066,-Metrics/Training(Step): loss,1610341460458,0.5932594537734985
1068,-Metrics/Training(Step): loss,1610341464119,0.3730017840862274
1070,-Metrics/Training(Step): loss,1610341468416,0.48861339688301086
1072,-Metrics/Training(Step): loss,1610341472043,0.4827203154563904
1074,-Metrics/Training(Step): loss,1610341475419,0.6922893524169922
1076,-Metrics/Training(Step): loss,1610341479308,0.44988271594047546
1078,-Metrics/Training(Step): loss,1610341482390,0.5496770739555359
1080,-Metrics/Training(Step): loss,1610341485121,0.6032028198242188
1082,-Metrics/Training(Step): loss,1610341487111,0.5159651637077332
1084,-Metrics/Training(Step): loss,1610341489281,0.5065124034881592
1086,-Metrics/Training(Step): loss,1610341491406,0.5638995170593262
1088,-Metrics/Training(Step): loss,1610341493435,0.47561365365982056
1090,-Metrics/Training(Step): loss,1610341495428,0.4455818831920624
1092,-Metrics/Training(Step): loss,1610341497532,0.4859142005443573
1094,-Metrics/Training(Step): loss,1610341499637,0.8408369421958923
1096,-Metrics/Training(Step): loss,1610341501769,0.5813112258911133
1098,-Metrics/Training(Step): loss,1610341503668,0.5004842281341553
1100,-Metrics/Training(Step): loss,1610341505736,0.42012330889701843
1102,-Metrics/Training(Step): loss,1610341507657,0.3938404321670532
1104,-Metrics/Training(Step): loss,1610341509634,0.5358347296714783
1106,-Metrics/Training(Step): loss,1610341511690,0.525158703327179
1108,-Metrics/Training(Step): loss,1610341513734,0.5469516515731812
1110,-Metrics/Training(Step): loss,1610341515833,0.43132540583610535
1112,-Metrics/Training(Step): loss,1610341517913,0.47681039571762085
1114,-Metrics/Training(Step): loss,1610341519819,0.40581080317497253
1116,-Metrics/Training(Step): loss,1610341521841,0.480996698141098
1118,-Metrics/Training(Step): loss,1610341523859,0.6305589079856873
1120,-Metrics/Training(Step): loss,1610341537145,0.466770201921463
1122,-Metrics/Training(Step): loss,1610341541220,0.48121246695518494
1124,-Metrics/Training(Step): loss,1610341544920,0.6639584302902222
1126,-Metrics/Training(Step): loss,1610341549024,0.3996274769306183
1128,-Metrics/Training(Step): loss,1610341553024,0.48480159044265747
1130,-Metrics/Training(Step): loss,1610341557319,0.41281986236572266
1132,-Metrics/Training(Step): loss,1610341560852,0.4312031865119934
1134,-Metrics/Training(Step): loss,1610341564820,0.4329374432563782
1136,-Metrics/Training(Step): loss,1610341568588,0.4275532066822052
1138,-Metrics/Training(Step): loss,1610341572182,0.52259761095047
1140,-Metrics/Training(Step): loss,1610341575820,0.42238470911979675
1142,-Metrics/Training(Step): loss,1610341579219,0.27166372537612915
1144,-Metrics/Training(Step): loss,1610341582916,0.4545491635799408
1146,-Metrics/Training(Step): loss,1610341586436,0.4889676868915558
1148,-Metrics/Training(Step): loss,1610341590329,0.38441577553749084
1150,-Metrics/Training(Step): loss,1610341593440,0.377705842256546
1152,-Metrics/Training(Step): loss,1610341596915,0.5937674045562744
1154,-Metrics/Training(Step): loss,1610341600537,0.6294097900390625
1156,-Metrics/Training(Step): loss,1610341604420,0.6017013788223267
1158,-Metrics/Training(Step): loss,1610341607944,0.5368525981903076
1160,-Metrics/Training(Step): loss,1610341611717,0.2925426959991455
1162,-Metrics/Training(Step): loss,1610341615415,0.41868624091148376
1164,-Metrics/Training(Step): loss,1610341618858,0.3757270276546478
1166,-Metrics/Training(Step): loss,1610341621050,0.5149708390235901
1168,-Metrics/Training(Step): loss,1610341623217,0.45409056544303894
1170,-Metrics/Training(Step): loss,1610341625286,0.4208797812461853
1172,-Metrics/Training(Step): loss,1610341627385,0.6523236632347107
1174,-Metrics/Training(Step): loss,1610341629208,0.5501985549926758
1176,-Metrics/Training(Step): loss,1610341631336,0.47026845812797546
1178,-Metrics/Training(Step): loss,1610341633345,0.39147597551345825
1180,-Metrics/Training(Step): loss,1610341635377,0.48494815826416016
1182,-Metrics/Training(Step): loss,1610341637486,0.3982798755168915
1184,-Metrics/Training(Step): loss,1610341639573,0.4762435257434845
1186,-Metrics/Training(Step): loss,1610341641533,0.36571866273880005
1188,-Metrics/Training(Step): loss,1610341643600,0.38837260007858276
1190,-Metrics/Training(Step): loss,1610341645564,0.45837047696113586
1192,-Metrics/Training(Step): loss,1610341647585,0.3726489245891571
1194,-Metrics/Training(Step): loss,1610341649537,0.35166135430336
1196,-Metrics/Training(Step): loss,1610341651485,0.5203425288200378
1198,-Metrics/Training(Step): loss,1610341653537,0.5093877911567688
1200,-Metrics/Training(Step): loss,1610341655580,0.3826369047164917
1202,-Metrics/Training(Step): loss,1610341657646,0.5793265104293823
1204,-Metrics/Training(Step): loss,1610341659690,0.3731188178062439
1206,-Metrics/Training(Step): loss,1610341672622,0.4964342415332794
1208,-Metrics/Training(Step): loss,1610341676919,0.4255236089229584
1210,-Metrics/Training(Step): loss,1610341680924,0.4456695318222046
1212,-Metrics/Training(Step): loss,1610341684658,0.40899354219436646
1214,-Metrics/Training(Step): loss,1610341688448,0.42373180389404297
1216,-Metrics/Training(Step): loss,1610341692520,0.4623452126979828
1218,-Metrics/Training(Step): loss,1610341696319,0.4042953550815582
1220,-Metrics/Training(Step): loss,1610341699724,0.4252835512161255
1222,-Metrics/Training(Step): loss,1610341703333,0.3169240653514862
1224,-Metrics/Training(Step): loss,1610341706898,0.4132763743400574
1226,-Metrics/Training(Step): loss,1610341710218,0.4776048958301544
1228,-Metrics/Training(Step): loss,1610341713664,0.4614613354206085
1230,-Metrics/Training(Step): loss,1610341717420,0.4552576541900635
1232,-Metrics/Training(Step): loss,1610341720920,0.37864887714385986
1234,-Metrics/Training(Step): loss,1610341724119,0.46976786851882935
1236,-Metrics/Training(Step): loss,1610341727319,0.6120445728302002
1238,-Metrics/Training(Step): loss,1610341731370,0.46034830808639526
1240,-Metrics/Training(Step): loss,1610341734720,0.400472491979599
1242,-Metrics/Training(Step): loss,1610341738036,0.42629605531692505
1244,-Metrics/Training(Step): loss,1610341741420,0.5020016431808472
1246,-Metrics/Training(Step): loss,1610341745181,0.3933010995388031
1248,-Metrics/Training(Step): loss,1610341749102,0.445290744304657
1250,-Metrics/Training(Step): loss,1610341752235,0.5239116549491882
1252,-Metrics/Training(Step): loss,1610341755704,0.3775857985019684
1254,-Metrics/Training(Step): loss,1610341758104,0.3748772442340851
1256,-Metrics/Training(Step): loss,1610341760222,0.5198439359664917
1258,-Metrics/Training(Step): loss,1610341762282,0.4003005623817444
1260,-Metrics/Training(Step): loss,1610341764344,0.4731314182281494
1262,-Metrics/Training(Step): loss,1610341766372,0.5014073848724365
1264,-Metrics/Training(Step): loss,1610341768479,0.36260759830474854
1266,-Metrics/Training(Step): loss,1610341770590,0.4614332914352417
1268,-Metrics/Training(Step): loss,1610341772647,0.5653492212295532
1270,-Metrics/Training(Step): loss,1610341774499,0.4763275980949402
1272,-Metrics/Training(Step): loss,1610341776560,0.4836560785770416
1274,-Metrics/Training(Step): loss,1610341778621,0.5351598262786865
1276,-Metrics/Training(Step): loss,1610341780541,0.37319982051849365
1278,-Metrics/Training(Step): loss,1610341782626,0.504382848739624
1280,-Metrics/Training(Step): loss,1610341784601,0.42716068029403687
1282,-Metrics/Training(Step): loss,1610341786617,0.465178519487381
1284,-Metrics/Training(Step): loss,1610341788582,0.42287173867225647
1286,-Metrics/Training(Step): loss,1610341790681,0.5463417172431946
1288,-Metrics/Training(Step): loss,1610341792728,0.3653108775615692
1290,-Metrics/Training(Step): loss,1610341794794,0.38419628143310547
1292,-Metrics/Training(Step): loss,1610341808336,0.47392457723617554
1294,-Metrics/Training(Step): loss,1610341812458,0.46372681856155396
1296,-Metrics/Training(Step): loss,1610341816219,0.41318753361701965
1298,-Metrics/Training(Step): loss,1610341819925,0.5023490190505981
1300,-Metrics/Training(Step): loss,1610341823820,0.4203578233718872
1302,-Metrics/Training(Step): loss,1610341827737,0.35589051246643066
1304,-Metrics/Training(Step): loss,1610341831521,0.48954322934150696
1306,-Metrics/Training(Step): loss,1610341835020,0.41998106241226196
1308,-Metrics/Training(Step): loss,1610341838078,0.48652416467666626
1310,-Metrics/Training(Step): loss,1610341841721,0.45140886306762695
1312,-Metrics/Training(Step): loss,1610341845016,0.444585382938385
1314,-Metrics/Training(Step): loss,1610341848316,0.3979335427284241
1316,-Metrics/Training(Step): loss,1610341851319,0.5046472549438477
1318,-Metrics/Training(Step): loss,1610341855058,0.5035696625709534
1320,-Metrics/Training(Step): loss,1610341858421,0.49928054213523865
1322,-Metrics/Training(Step): loss,1610341862281,0.4063270390033722
1324,-Metrics/Training(Step): loss,1610341865920,0.36743202805519104
1326,-Metrics/Training(Step): loss,1610341869442,0.30663424730300903
1328,-Metrics/Training(Step): loss,1610341872654,0.44702330231666565
1330,-Metrics/Training(Step): loss,1610341876222,0.4306345582008362
1332,-Metrics/Training(Step): loss,1610341879320,0.44207343459129333
1334,-Metrics/Training(Step): loss,1610341883241,0.4406780004501343
1336,-Metrics/Training(Step): loss,1610341886606,0.38715460896492004
1338,-Metrics/Training(Step): loss,1610341890148,0.5243896245956421
1340,-Metrics/Training(Step): loss,1610341893136,0.46733227372169495
1342,-Metrics/Training(Step): loss,1610341895358,0.41735365986824036
1344,-Metrics/Training(Step): loss,1610341897551,0.6017428636550903
1346,-Metrics/Training(Step): loss,1610341899673,0.44821202754974365
1348,-Metrics/Training(Step): loss,1610341901756,0.45006632804870605
1350,-Metrics/Training(Step): loss,1610341903844,0.380269318819046
1352,-Metrics/Training(Step): loss,1610341905946,0.4495239853858948
1354,-Metrics/Training(Step): loss,1610341907905,0.49592888355255127
1356,-Metrics/Training(Step): loss,1610341909939,0.47908085584640503
1358,-Metrics/Training(Step): loss,1610341912029,0.4712822437286377
1360,-Metrics/Training(Step): loss,1610341914084,0.4115365147590637
1362,-Metrics/Training(Step): loss,1610341916062,0.3912203013896942
1364,-Metrics/Training(Step): loss,1610341918069,0.3672548830509186
1366,-Metrics/Training(Step): loss,1610341920127,0.47730565071105957
1368,-Metrics/Training(Step): loss,1610341922168,0.4449694752693176
1370,-Metrics/Training(Step): loss,1610341924218,0.4836854338645935
1372,-Metrics/Training(Step): loss,1610341926131,0.343204528093338
1374,-Metrics/Training(Step): loss,1610341928128,0.39458608627319336
1376,-Metrics/Training(Step): loss,1610341930149,0.39994823932647705
1378,-Metrics/Training(Step): loss,1610341942322,0.4007565677165985
1380,-Metrics/Training(Step): loss,1610341946320,0.47588056325912476
1382,-Metrics/Training(Step): loss,1610341950422,0.31410524249076843
1384,-Metrics/Training(Step): loss,1610341954029,0.45307034254074097
1386,-Metrics/Training(Step): loss,1610341957852,0.30647769570350647
1388,-Metrics/Training(Step): loss,1610341961526,0.4140201210975647
1390,-Metrics/Training(Step): loss,1610341965520,0.4269130229949951
1392,-Metrics/Training(Step): loss,1610341969670,0.4015447795391083
1394,-Metrics/Training(Step): loss,1610341973239,0.5837252140045166
1396,-Metrics/Training(Step): loss,1610341976955,0.5193976759910583
1398,-Metrics/Training(Step): loss,1610341980658,0.43159979581832886
1400,-Metrics/Training(Step): loss,1610341984375,0.41480281949043274
1402,-Metrics/Training(Step): loss,1610341988097,0.4167404770851135
1404,-Metrics/Training(Step): loss,1610341991819,0.34384840726852417
1406,-Metrics/Training(Step): loss,1610341994819,0.33931976556777954
1408,-Metrics/Training(Step): loss,1610341998319,0.3886224031448364
1410,-Metrics/Training(Step): loss,1610342001519,0.5184163451194763
1412,-Metrics/Training(Step): loss,1610342005373,0.4565737247467041
1414,-Metrics/Training(Step): loss,1610342008984,0.45233768224716187
1416,-Metrics/Training(Step): loss,1610342012119,0.47237297892570496
1418,-Metrics/Training(Step): loss,1610342015810,0.3291725814342499
1420,-Metrics/Training(Step): loss,1610342019889,0.37152430415153503
1422,-Metrics/Training(Step): loss,1610342022838,0.3600248694419861
1424,-Metrics/Training(Step): loss,1610342025256,0.5714603066444397
1426,-Metrics/Training(Step): loss,1610342027713,0.3905872404575348
1428,-Metrics/Training(Step): loss,1610342030194,0.5165773630142212
1430,-Metrics/Training(Step): loss,1610342032277,0.6190921664237976
1432,-Metrics/Training(Step): loss,1610342034365,0.2799866497516632
1434,-Metrics/Training(Step): loss,1610342036503,0.5274038910865784
1436,-Metrics/Training(Step): loss,1610342038553,0.43525317311286926
1438,-Metrics/Training(Step): loss,1610342040600,0.4930475056171417
1440,-Metrics/Training(Step): loss,1610342042558,0.445482075214386
1442,-Metrics/Training(Step): loss,1610342044585,0.35495617985725403
1444,-Metrics/Training(Step): loss,1610342046555,0.3886212706565857
1446,-Metrics/Training(Step): loss,1610342048592,0.5354354381561279
1448,-Metrics/Training(Step): loss,1610342050481,0.4022830128669739
1450,-Metrics/Training(Step): loss,1610342052530,0.3882802128791809
1452,-Metrics/Training(Step): loss,1610342054644,0.33357319235801697
1454,-Metrics/Training(Step): loss,1610342056670,0.405427485704422
1456,-Metrics/Training(Step): loss,1610342058735,0.4405267536640167
1458,-Metrics/Training(Step): loss,1610342060817,0.5125424265861511
1460,-Metrics/Training(Step): loss,1610342062818,0.37728938460350037
1462,-Metrics/Training(Step): loss,1610342064849,0.3948085308074951
1464,-Metrics/Training(Step): loss,1610342077918,0.374157190322876
1466,-Metrics/Training(Step): loss,1610342082021,0.4489606022834778
1468,-Metrics/Training(Step): loss,1610342085933,0.36419856548309326
1470,-Metrics/Training(Step): loss,1610342089719,0.3498317301273346
1472,-Metrics/Training(Step): loss,1610342093453,0.3668508231639862
1474,-Metrics/Training(Step): loss,1610342097320,0.49542999267578125
1476,-Metrics/Training(Step): loss,1610342101220,0.4557207226753235
1478,-Metrics/Training(Step): loss,1610342105221,0.4760821461677551
1480,-Metrics/Training(Step): loss,1610342109121,0.40717121958732605
1482,-Metrics/Training(Step): loss,1610342112520,0.4099612534046173
1484,-Metrics/Training(Step): loss,1610342116209,0.5123140811920166
1486,-Metrics/Training(Step): loss,1610342119611,0.5061212182044983
1488,-Metrics/Training(Step): loss,1610342123519,0.3957618176937103
1490,-Metrics/Training(Step): loss,1610342126120,0.35189512372016907
1492,-Metrics/Training(Step): loss,1610342129519,0.48227202892303467
1494,-Metrics/Training(Step): loss,1610342132438,0.37963390350341797
1496,-Metrics/Training(Step): loss,1610342136290,0.46110692620277405
1498,-Metrics/Training(Step): loss,1610342140020,0.42015278339385986
1500,-Metrics/Training(Step): loss,1610342143419,0.42017728090286255
1502,-Metrics/Training(Step): loss,1610342146919,0.38899359107017517
1504,-Metrics/Training(Step): loss,1610342151320,0.38870787620544434
1506,-Metrics/Training(Step): loss,1610342154339,0.44229522347450256
1508,-Metrics/Training(Step): loss,1610342157336,0.42511242628097534
1510,-Metrics/Training(Step): loss,1610342160115,0.4558230936527252
1512,-Metrics/Training(Step): loss,1610342162269,0.32437124848365784
1514,-Metrics/Training(Step): loss,1610342164446,0.3763412535190582
1516,-Metrics/Training(Step): loss,1610342166487,0.5437408685684204
1518,-Metrics/Training(Step): loss,1610342168501,0.5141233205795288
1520,-Metrics/Training(Step): loss,1610342170614,0.418229341506958
1522,-Metrics/Training(Step): loss,1610342172707,0.4681073725223541
1524,-Metrics/Training(Step): loss,1610342174803,0.3849934935569763
1526,-Metrics/Training(Step): loss,1610342176790,0.5912131071090698
1528,-Metrics/Training(Step): loss,1610342178816,0.5725868344306946
1530,-Metrics/Training(Step): loss,1610342180728,0.3421459496021271
1532,-Metrics/Training(Step): loss,1610342182810,0.3740939199924469
1534,-Metrics/Training(Step): loss,1610342184856,0.39737629890441895
1536,-Metrics/Training(Step): loss,1610342186944,0.37706106901168823
1538,-Metrics/Training(Step): loss,1610342188923,0.36503738164901733
1540,-Metrics/Training(Step): loss,1610342190806,0.33984851837158203
1542,-Metrics/Training(Step): loss,1610342192820,0.4631025493144989
1544,-Metrics/Training(Step): loss,1610342194864,0.38674062490463257
1546,-Metrics/Training(Step): loss,1610342196920,0.3910870850086212
1548,-Metrics/Training(Step): loss,1610342198962,0.33024242520332336
1550,-Metrics/Training(Step): loss,1610342211735,0.4153342843055725
1552,-Metrics/Training(Step): loss,1610342216320,0.477923184633255
1554,-Metrics/Training(Step): loss,1610342220231,0.34689000248908997
1556,-Metrics/Training(Step): loss,1610342224520,0.4037379026412964
1558,-Metrics/Training(Step): loss,1610342228520,0.4449178874492645
1560,-Metrics/Training(Step): loss,1610342232422,0.45129165053367615
1562,-Metrics/Training(Step): loss,1610342236234,0.34565743803977966
1564,-Metrics/Training(Step): loss,1610342239481,0.4517625868320465
1566,-Metrics/Training(Step): loss,1610342243003,0.32194948196411133
1568,-Metrics/Training(Step): loss,1610342246524,0.47637036442756653
1570,-Metrics/Training(Step): loss,1610342250120,0.3299902677536011
1572,-Metrics/Training(Step): loss,1610342253920,0.4337719976902008
1574,-Metrics/Training(Step): loss,1610342257621,0.47282353043556213
1576,-Metrics/Training(Step): loss,1610342261425,0.3786296844482422
1578,-Metrics/Training(Step): loss,1610342264440,0.44118696451187134
1580,-Metrics/Training(Step): loss,1610342267820,0.3900257647037506
1582,-Metrics/Training(Step): loss,1610342271059,0.31846854090690613
1584,-Metrics/Training(Step): loss,1610342274619,0.49009859561920166
1586,-Metrics/Training(Step): loss,1610342277843,0.37076887488365173
1588,-Metrics/Training(Step): loss,1610342281640,0.6042930483818054
1590,-Metrics/Training(Step): loss,1610342285135,0.2894982099533081
1592,-Metrics/Training(Step): loss,1610342288623,0.37975314259529114
1594,-Metrics/Training(Step): loss,1610342291920,0.3960338830947876
1596,-Metrics/Training(Step): loss,1610342295121,0.3565414845943451
1598,-Metrics/Training(Step): loss,1610342297604,0.4155961871147156
1600,-Metrics/Training(Step): loss,1610342299912,0.39917996525764465
1602,-Metrics/Training(Step): loss,1610342301975,0.4294331669807434
1604,-Metrics/Training(Step): loss,1610342303938,0.41138017177581787
1606,-Metrics/Training(Step): loss,1610342306032,0.41931718587875366
1608,-Metrics/Training(Step): loss,1610342308049,0.3881855607032776
1610,-Metrics/Training(Step): loss,1610342310135,0.40759140253067017
1612,-Metrics/Training(Step): loss,1610342312235,0.48122265934944153
1614,-Metrics/Training(Step): loss,1610342314316,0.4067094922065735
1616,-Metrics/Training(Step): loss,1610342316379,0.32829585671424866
1618,-Metrics/Training(Step): loss,1610342318443,0.4512888491153717
1620,-Metrics/Training(Step): loss,1610342320133,0.3996964395046234
1622,-Metrics/Training(Step): loss,1610342322237,0.37396296858787537
1624,-Metrics/Training(Step): loss,1610342324288,0.40458232164382935
1626,-Metrics/Training(Step): loss,1610342326394,0.40038543939590454
1628,-Metrics/Training(Step): loss,1610342328416,0.4495287835597992
1630,-Metrics/Training(Step): loss,1610342330537,0.3738082945346832
1632,-Metrics/Training(Step): loss,1610342332519,0.3816134035587311
1634,-Metrics/Training(Step): loss,1610342334575,0.40523460507392883
1636,-Metrics/Training(Step): loss,1610342346637,0.4278525412082672
1638,-Metrics/Training(Step): loss,1610342350630,0.37373095750808716
1640,-Metrics/Training(Step): loss,1610342354820,0.48113691806793213
1642,-Metrics/Training(Step): loss,1610342358818,0.35383763909339905
1644,-Metrics/Training(Step): loss,1610342362917,0.4106851816177368
1646,-Metrics/Training(Step): loss,1610342367321,0.3873404264450073
1648,-Metrics/Training(Step): loss,1610342371254,0.32357895374298096
1650,-Metrics/Training(Step): loss,1610342375420,0.5125131011009216
1652,-Metrics/Training(Step): loss,1610342379569,0.4093196392059326
1654,-Metrics/Training(Step): loss,1610342383221,0.3971002399921417
1656,-Metrics/Training(Step): loss,1610342386321,0.34981951117515564
1658,-Metrics/Training(Step): loss,1610342390420,0.4391061067581177
1660,-Metrics/Training(Step): loss,1610342393720,0.4702506959438324
1662,-Metrics/Training(Step): loss,1610342397717,0.3373613953590393
1664,-Metrics/Training(Step): loss,1610342401316,0.4466556906700134
1666,-Metrics/Training(Step): loss,1610342405520,0.3695806860923767
1668,-Metrics/Training(Step): loss,1610342409043,0.4061267673969269
1670,-Metrics/Training(Step): loss,1610342412789,0.4178658723831177
1672,-Metrics/Training(Step): loss,1610342416475,0.5070498585700989
1674,-Metrics/Training(Step): loss,1610342420318,0.40229764580726624
1676,-Metrics/Training(Step): loss,1610342423419,0.7181663513183594
1678,-Metrics/Training(Step): loss,1610342427038,0.4750058352947235
1680,-Metrics/Training(Step): loss,1610342430082,0.43706488609313965
1682,-Metrics/Training(Step): loss,1610342432637,0.6123296022415161
1684,-Metrics/Training(Step): loss,1610342434900,0.5589807629585266
1686,-Metrics/Training(Step): loss,1610342436964,0.3805370330810547
1688,-Metrics/Training(Step): loss,1610342439022,0.38376373052597046
1690,-Metrics/Training(Step): loss,1610342441083,0.4561764895915985
1692,-Metrics/Training(Step): loss,1610342443143,0.7223691344261169
1694,-Metrics/Training(Step): loss,1610342445210,0.4247889518737793
1696,-Metrics/Training(Step): loss,1610342447324,0.40727153420448303
1698,-Metrics/Training(Step): loss,1610342449415,0.5073142051696777
1700,-Metrics/Training(Step): loss,1610342451445,0.5011394023895264
1702,-Metrics/Training(Step): loss,1610342453333,0.4093098044395447
1704,-Metrics/Training(Step): loss,1610342455405,0.7194151878356934
1706,-Metrics/Training(Step): loss,1610342457477,0.5464361906051636
1708,-Metrics/Training(Step): loss,1610342459534,0.618281364440918
1710,-Metrics/Training(Step): loss,1610342461664,0.4477144479751587
1712,-Metrics/Training(Step): loss,1610342463641,0.4823327660560608
1714,-Metrics/Training(Step): loss,1610342465388,0.454173743724823
1716,-Metrics/Training(Step): loss,1610342467411,0.39980554580688477
1718,-Metrics/Training(Step): loss,1610342469477,0.4920661449432373
1720,-Metrics/Training(Step): loss,1610342471500,0.38093453645706177
1722,-Metrics/Training(Step): loss,1610342483535,0.49221664667129517
1724,-Metrics/Training(Step): loss,1610342487737,0.4594779312610626
1726,-Metrics/Training(Step): loss,1610342491720,0.45328134298324585
1728,-Metrics/Training(Step): loss,1610342495919,0.5181499123573303
1730,-Metrics/Training(Step): loss,1610342499821,0.5414109826087952
1732,-Metrics/Training(Step): loss,1610342503826,0.35048311948776245
1734,-Metrics/Training(Step): loss,1610342507927,0.47492799162864685
1736,-Metrics/Training(Step): loss,1610342512422,0.5392476320266724
1738,-Metrics/Training(Step): loss,1610342516231,0.5416252613067627
1740,-Metrics/Training(Step): loss,1610342520328,0.4144131541252136
1742,-Metrics/Training(Step): loss,1610342523819,0.3967345654964447
1744,-Metrics/Training(Step): loss,1610342527233,0.4308350384235382
1746,-Metrics/Training(Step): loss,1610342531021,0.6565369367599487
1748,-Metrics/Training(Step): loss,1610342534519,0.383087694644928
1750,-Metrics/Training(Step): loss,1610342537727,0.6059170365333557
1752,-Metrics/Training(Step): loss,1610342541947,0.3555464744567871
1754,-Metrics/Training(Step): loss,1610342545519,0.5405662655830383
1756,-Metrics/Training(Step): loss,1610342548921,0.40306517481803894
1758,-Metrics/Training(Step): loss,1610342552684,0.37709856033325195
1760,-Metrics/Training(Step): loss,1610342556620,0.428390771150589
1762,-Metrics/Training(Step): loss,1610342560259,0.2902011573314667
1764,-Metrics/Training(Step): loss,1610342563521,0.37263986468315125
1766,-Metrics/Training(Step): loss,1610342566696,0.3518781363964081
1768,-Metrics/Training(Step): loss,1610342569534,0.46999266743659973
1770,-Metrics/Training(Step): loss,1610342571860,0.32170331478118896
1772,-Metrics/Training(Step): loss,1610342573890,0.31819573044776917
1774,-Metrics/Training(Step): loss,1610342575922,0.42453691363334656
1776,-Metrics/Training(Step): loss,1610342578002,0.4307498037815094
1778,-Metrics/Training(Step): loss,1610342580103,0.45519211888313293
1780,-Metrics/Training(Step): loss,1610342582223,0.31918784976005554
1782,-Metrics/Training(Step): loss,1610342584264,0.4613948464393616
1784,-Metrics/Training(Step): loss,1610342586321,0.341396301984787
1786,-Metrics/Training(Step): loss,1610342588403,0.42369428277015686
1788,-Metrics/Training(Step): loss,1610342590389,0.36845797300338745
1790,-Metrics/Training(Step): loss,1610342592450,0.44496211409568787
1792,-Metrics/Training(Step): loss,1610342594533,0.44550809264183044
1794,-Metrics/Training(Step): loss,1610342596472,0.44337108731269836
1796,-Metrics/Training(Step): loss,1610342598582,0.3002312481403351
1798,-Metrics/Training(Step): loss,1610342600641,0.4112447500228882
1800,-Metrics/Training(Step): loss,1610342602744,0.4454445540904999
1802,-Metrics/Training(Step): loss,1610342604823,0.43147966265678406
1804,-Metrics/Training(Step): loss,1610342606920,0.39220094680786133
1806,-Metrics/Training(Step): loss,1610342608986,0.3802124559879303
1808,-Metrics/Training(Step): loss,1610342621923,0.404796838760376
1810,-Metrics/Training(Step): loss,1610342626031,0.3973599374294281
1812,-Metrics/Training(Step): loss,1610342630020,0.40964269638061523
1814,-Metrics/Training(Step): loss,1610342633740,0.39973026514053345
1816,-Metrics/Training(Step): loss,1610342637816,0.42328348755836487
1818,-Metrics/Training(Step): loss,1610342641543,0.3185569643974304
1820,-Metrics/Training(Step): loss,1610342646034,0.44525399804115295
1822,-Metrics/Training(Step): loss,1610342650262,0.272474080324173
1824,-Metrics/Training(Step): loss,1610342654522,0.2937096953392029
1826,-Metrics/Training(Step): loss,1610342658289,0.4586169421672821
1828,-Metrics/Training(Step): loss,1610342662320,0.30447572469711304
1830,-Metrics/Training(Step): loss,1610342666120,0.3384580612182617
1832,-Metrics/Training(Step): loss,1610342669961,0.5762877464294434
1834,-Metrics/Training(Step): loss,1610342673417,0.4665485918521881
1836,-Metrics/Training(Step): loss,1610342677020,0.3309493064880371
1838,-Metrics/Training(Step): loss,1610342680221,0.4199111759662628
1840,-Metrics/Training(Step): loss,1610342683844,0.4545571804046631
1842,-Metrics/Training(Step): loss,1610342687345,0.36811596155166626
1844,-Metrics/Training(Step): loss,1610342690395,0.3832824230194092
1846,-Metrics/Training(Step): loss,1610342694552,0.4239572286605835
1848,-Metrics/Training(Step): loss,1610342697967,0.35102760791778564
1850,-Metrics/Training(Step): loss,1610342701640,0.42456090450286865
1852,-Metrics/Training(Step): loss,1610342704752,0.33701542019844055
1854,-Metrics/Training(Step): loss,1610342707293,0.4295223653316498
1856,-Metrics/Training(Step): loss,1610342709398,0.33298537135124207
1858,-Metrics/Training(Step): loss,1610342711510,0.4705815613269806
1860,-Metrics/Training(Step): loss,1610342713586,0.3471137285232544
1862,-Metrics/Training(Step): loss,1610342715729,0.5120035409927368
1864,-Metrics/Training(Step): loss,1610342717824,0.4200618267059326
1866,-Metrics/Training(Step): loss,1610342719965,0.4172842800617218
1868,-Metrics/Training(Step): loss,1610342722090,0.37841442227363586
1870,-Metrics/Training(Step): loss,1610342724201,0.4200120270252228
1872,-Metrics/Training(Step): loss,1610342726307,0.322243869304657
1874,-Metrics/Training(Step): loss,1610342728191,0.3255905210971832
1876,-Metrics/Training(Step): loss,1610342730136,0.403668075799942
1878,-Metrics/Training(Step): loss,1610342732204,0.5178439617156982
1880,-Metrics/Training(Step): loss,1610342734226,0.4521317481994629
1882,-Metrics/Training(Step): loss,1610342736226,0.4400186836719513
1884,-Metrics/Training(Step): loss,1610342738292,0.42093196511268616
1886,-Metrics/Training(Step): loss,1610342740363,0.36662688851356506
1888,-Metrics/Training(Step): loss,1610342742423,0.40013861656188965
1890,-Metrics/Training(Step): loss,1610342744430,0.4916588366031647
1892,-Metrics/Training(Step): loss,1610342746469,0.3190346360206604
1894,-Metrics/Training(Step): loss,1610342758822,0.4855295419692993
1896,-Metrics/Training(Step): loss,1610342762920,0.383029043674469
1898,-Metrics/Training(Step): loss,1610342766919,0.37538257241249084
1900,-Metrics/Training(Step): loss,1610342771017,0.23948246240615845
1902,-Metrics/Training(Step): loss,1610342774919,0.4325789213180542
1904,-Metrics/Training(Step): loss,1610342778820,0.3520822525024414
1906,-Metrics/Training(Step): loss,1610342782920,0.46693554520606995
1908,-Metrics/Training(Step): loss,1610342787024,0.27175188064575195
1910,-Metrics/Training(Step): loss,1610342791019,0.3637009561061859
1912,-Metrics/Training(Step): loss,1610342795120,0.345089316368103
1914,-Metrics/Training(Step): loss,1610342798920,0.3620156943798065
1916,-Metrics/Training(Step): loss,1610342802220,0.3832930624485016
1918,-Metrics/Training(Step): loss,1610342806121,0.24192945659160614
1920,-Metrics/Training(Step): loss,1610342810015,0.34262844920158386
1922,-Metrics/Training(Step): loss,1610342814230,0.41092580556869507
1924,-Metrics/Training(Step): loss,1610342818132,0.34349656105041504
1926,-Metrics/Training(Step): loss,1610342821915,0.3953399360179901
1928,-Metrics/Training(Step): loss,1610342825141,0.34380850195884705
1930,-Metrics/Training(Step): loss,1610342828538,0.28467851877212524
1932,-Metrics/Training(Step): loss,1610342831958,0.30058422684669495
1934,-Metrics/Training(Step): loss,1610342835425,0.35635823011398315
1936,-Metrics/Training(Step): loss,1610342839119,0.35871538519859314
1938,-Metrics/Training(Step): loss,1610342842963,0.37426164746284485
1940,-Metrics/Training(Step): loss,1610342846453,0.3779374361038208
1942,-Metrics/Training(Step): loss,1610342849513,0.39469170570373535
1944,-Metrics/Training(Step): loss,1610342852143,0.2953673005104065
1946,-Metrics/Training(Step): loss,1610342854700,0.36338672041893005
1948,-Metrics/Training(Step): loss,1610342857093,0.4767325222492218
1950,-Metrics/Training(Step): loss,1610342859194,0.36352846026420593
1952,-Metrics/Training(Step): loss,1610342861318,0.358856201171875
1954,-Metrics/Training(Step): loss,1610342863433,0.39619576930999756
1956,-Metrics/Training(Step): loss,1610342865510,0.2652408480644226
1958,-Metrics/Training(Step): loss,1610342867435,0.28856605291366577
1960,-Metrics/Training(Step): loss,1610342869550,0.35471248626708984
1962,-Metrics/Training(Step): loss,1610342871628,0.3168208599090576
1964,-Metrics/Training(Step): loss,1610342873725,0.31428948044776917
1966,-Metrics/Training(Step): loss,1610342875715,0.29618018865585327
1968,-Metrics/Training(Step): loss,1610342877759,0.41206374764442444
1970,-Metrics/Training(Step): loss,1610342879740,0.36135828495025635
1972,-Metrics/Training(Step): loss,1610342881826,0.4773874282836914
1974,-Metrics/Training(Step): loss,1610342883815,0.3900373876094818
1976,-Metrics/Training(Step): loss,1610342885944,0.3976149559020996
1978,-Metrics/Training(Step): loss,1610342887940,0.3537098467350006
1980,-Metrics/Training(Step): loss,1610342900647,0.3280884921550751
1982,-Metrics/Training(Step): loss,1610342904823,0.39766234159469604
1984,-Metrics/Training(Step): loss,1610342908721,0.38897204399108887
1986,-Metrics/Training(Step): loss,1610342912220,0.41356778144836426
1988,-Metrics/Training(Step): loss,1610342916219,0.3484334349632263
1990,-Metrics/Training(Step): loss,1610342920421,0.3004947900772095
1992,-Metrics/Training(Step): loss,1610342924220,0.4794365167617798
1994,-Metrics/Training(Step): loss,1610342927922,0.3281213343143463
1996,-Metrics/Training(Step): loss,1610342931399,0.42119938135147095
1998,-Metrics/Training(Step): loss,1610342935154,0.3595038652420044
2000,-Metrics/Training(Step): loss,1610342939155,0.4166989028453827
2002,-Metrics/Training(Step): loss,1610342943059,0.4482513964176178
2004,-Metrics/Training(Step): loss,1610342946937,0.3992001712322235
2006,-Metrics/Training(Step): loss,1610342950920,0.38148391246795654
2008,-Metrics/Training(Step): loss,1610342954445,0.42668643593788147
2010,-Metrics/Training(Step): loss,1610342957418,0.32247620820999146
2012,-Metrics/Training(Step): loss,1610342960619,0.35482460260391235
2014,-Metrics/Training(Step): loss,1610342964365,0.42524150013923645
2016,-Metrics/Training(Step): loss,1610342967743,0.341253399848938
2018,-Metrics/Training(Step): loss,1610342971020,0.40967628359794617
2020,-Metrics/Training(Step): loss,1610342974579,0.38964027166366577
2022,-Metrics/Training(Step): loss,1610342978119,0.31930750608444214
2024,-Metrics/Training(Step): loss,1610342981990,0.3471358120441437
2026,-Metrics/Training(Step): loss,1610342984746,0.35217031836509705
2028,-Metrics/Training(Step): loss,1610342987245,0.4057445526123047
2030,-Metrics/Training(Step): loss,1610342989522,0.42731422185897827
2032,-Metrics/Training(Step): loss,1610342991557,0.4536796510219574
2034,-Metrics/Training(Step): loss,1610342993647,0.45124882459640503
2036,-Metrics/Training(Step): loss,1610342995732,0.323771208524704
2038,-Metrics/Training(Step): loss,1610342997854,0.39854416251182556
2040,-Metrics/Training(Step): loss,1610342999802,0.37571191787719727
2042,-Metrics/Training(Step): loss,1610343001881,0.3249283730983734
2044,-Metrics/Training(Step): loss,1610343004001,0.3722207844257355
2046,-Metrics/Training(Step): loss,1610343005928,0.3049962818622589
2048,-Metrics/Training(Step): loss,1610343007938,0.347660630941391
2050,-Metrics/Training(Step): loss,1610343009935,0.4498380124568939
2052,-Metrics/Training(Step): loss,1610343012040,0.3493405878543854
2054,-Metrics/Training(Step): loss,1610343014131,0.38615864515304565
2056,-Metrics/Training(Step): loss,1610343016204,0.4591522514820099
2058,-Metrics/Training(Step): loss,1610343018337,0.3208652436733246
2060,-Metrics/Training(Step): loss,1610343020430,0.32873907685279846
2062,-Metrics/Training(Step): loss,1610343022526,0.3673277199268341
2064,-Metrics/Training(Step): loss,1610343024587,0.36994707584381104
2066,-Metrics/Training(Step): loss,1610343037730,0.402437686920166
2068,-Metrics/Training(Step): loss,1610343041520,0.4250766336917877
2070,-Metrics/Training(Step): loss,1610343045321,0.33529993891716003
2072,-Metrics/Training(Step): loss,1610343049619,0.32776394486427307
2074,-Metrics/Training(Step): loss,1610343053720,0.26835283637046814
2076,-Metrics/Training(Step): loss,1610343057520,0.3210645914077759
2078,-Metrics/Training(Step): loss,1610343061620,0.2929964065551758
2080,-Metrics/Training(Step): loss,1610343065223,0.33695438504219055
2082,-Metrics/Training(Step): loss,1610343068920,0.3645387291908264
2084,-Metrics/Training(Step): loss,1610343072919,0.4237172603607178
2086,-Metrics/Training(Step): loss,1610343076421,0.4274207055568695
2088,-Metrics/Training(Step): loss,1610343080425,0.27462905645370483
2090,-Metrics/Training(Step): loss,1610343083740,0.3218531608581543
2092,-Metrics/Training(Step): loss,1610343087221,0.38306859135627747
2094,-Metrics/Training(Step): loss,1610343090819,0.30611714720726013
2096,-Metrics/Training(Step): loss,1610343094143,0.3023335039615631
2098,-Metrics/Training(Step): loss,1610343097143,0.31029215455055237
2100,-Metrics/Training(Step): loss,1610343100943,0.44801226258277893
2102,-Metrics/Training(Step): loss,1610343104419,0.2780177593231201
2104,-Metrics/Training(Step): loss,1610343108045,0.4232318699359894
2106,-Metrics/Training(Step): loss,1610343111718,0.36976295709609985
2108,-Metrics/Training(Step): loss,1610343115297,0.3346206843852997
2110,-Metrics/Training(Step): loss,1610343118970,0.32464301586151123
2112,-Metrics/Training(Step): loss,1610343122334,0.25304606556892395
2114,-Metrics/Training(Step): loss,1610343124734,0.3011438846588135
2116,-Metrics/Training(Step): loss,1610343126698,0.3560177683830261
2118,-Metrics/Training(Step): loss,1610343128792,0.40131819248199463
2120,-Metrics/Training(Step): loss,1610343130796,0.2826327681541443
2122,-Metrics/Training(Step): loss,1610343132885,0.2752716541290283
2124,-Metrics/Training(Step): loss,1610343134924,0.3971601724624634
2126,-Metrics/Training(Step): loss,1610343137237,0.37162935733795166
2128,-Metrics/Training(Step): loss,1610343139503,0.3025382161140442
2130,-Metrics/Training(Step): loss,1610343141603,0.32703620195388794
2132,-Metrics/Training(Step): loss,1610343143640,0.3542128801345825
2134,-Metrics/Training(Step): loss,1610343145523,0.29792651534080505
2136,-Metrics/Training(Step): loss,1610343147367,0.38831791281700134
2138,-Metrics/Training(Step): loss,1610343149456,0.37401461601257324
2140,-Metrics/Training(Step): loss,1610343151337,0.26884159445762634
2142,-Metrics/Training(Step): loss,1610343153385,0.2503191828727722
2144,-Metrics/Training(Step): loss,1610343155431,0.391150563955307
2146,-Metrics/Training(Step): loss,1610343157472,0.35707026720046997
2148,-Metrics/Training(Step): loss,1610343159389,0.2578934133052826
2150,-Metrics/Training(Step): loss,1610343161423,0.27763548493385315
2152,-Metrics/Training(Step): loss,1610343174132,0.38531672954559326
2154,-Metrics/Training(Step): loss,1610343178055,0.2691529095172882
2156,-Metrics/Training(Step): loss,1610343182058,0.359819620847702
2158,-Metrics/Training(Step): loss,1610343186020,0.3662705719470978
2160,-Metrics/Training(Step): loss,1610343190032,0.42679253220558167
2162,-Metrics/Training(Step): loss,1610343193850,0.5230837464332581
2164,-Metrics/Training(Step): loss,1610343197623,0.37303289771080017
2166,-Metrics/Training(Step): loss,1610343201726,0.3774728775024414
2168,-Metrics/Training(Step): loss,1610343205886,0.3492032587528229
2170,-Metrics/Training(Step): loss,1610343208920,0.37174487113952637
2172,-Metrics/Training(Step): loss,1610343212422,0.34847763180732727
2174,-Metrics/Training(Step): loss,1610343216020,0.3473801016807556
2176,-Metrics/Training(Step): loss,1610343219356,0.27553895115852356
2178,-Metrics/Training(Step): loss,1610343222722,0.2942281663417816
2180,-Metrics/Training(Step): loss,1610343226221,0.3704872131347656
2182,-Metrics/Training(Step): loss,1610343229620,0.3461507260799408
2184,-Metrics/Training(Step): loss,1610343233420,0.31390145421028137
2186,-Metrics/Training(Step): loss,1610343237427,0.3552233874797821
2188,-Metrics/Training(Step): loss,1610343240842,0.32704275846481323
2190,-Metrics/Training(Step): loss,1610343244123,0.32504236698150635
2192,-Metrics/Training(Step): loss,1610343247238,0.34559816122055054
2194,-Metrics/Training(Step): loss,1610343250640,0.32635337114334106
2196,-Metrics/Training(Step): loss,1610343254320,0.3523953855037689
2198,-Metrics/Training(Step): loss,1610343257358,0.35316094756126404
2200,-Metrics/Training(Step): loss,1610343259835,0.3668866753578186
2202,-Metrics/Training(Step): loss,1610343261974,0.3746021091938019
2204,-Metrics/Training(Step): loss,1610343264076,0.3687840700149536
2206,-Metrics/Training(Step): loss,1610343266256,0.3056347966194153
2208,-Metrics/Training(Step): loss,1610343268394,0.35358595848083496
2210,-Metrics/Training(Step): loss,1610343270493,0.27204620838165283
2212,-Metrics/Training(Step): loss,1610343272640,0.3463510572910309
2214,-Metrics/Training(Step): loss,1610343274692,0.3607439398765564
2216,-Metrics/Training(Step): loss,1610343276581,0.2732400894165039
2218,-Metrics/Training(Step): loss,1610343278743,0.3824150860309601
2220,-Metrics/Training(Step): loss,1610343280728,0.3056395947933197
2222,-Metrics/Training(Step): loss,1610343282836,0.26738232374191284
2224,-Metrics/Training(Step): loss,1610343284919,0.34295201301574707
2226,-Metrics/Training(Step): loss,1610343287046,0.262101948261261
2228,-Metrics/Training(Step): loss,1610343289108,0.33805444836616516
2230,-Metrics/Training(Step): loss,1610343291247,0.2832464277744293
2232,-Metrics/Training(Step): loss,1610343293318,0.3261066675186157
2234,-Metrics/Training(Step): loss,1610343295341,0.26053982973098755
2236,-Metrics/Training(Step): loss,1610343297386,0.29655003547668457
2238,-Metrics/Training(Step): loss,1610343311032,0.40179264545440674
2240,-Metrics/Training(Step): loss,1610343315120,0.3174436688423157
2242,-Metrics/Training(Step): loss,1610343319216,0.3217621445655823
2244,-Metrics/Training(Step): loss,1610343323146,0.5418077707290649
2246,-Metrics/Training(Step): loss,1610343327722,0.4778532087802887
2248,-Metrics/Training(Step): loss,1610343331830,0.3358084261417389
2250,-Metrics/Training(Step): loss,1610343335925,0.4705905020236969
2252,-Metrics/Training(Step): loss,1610343339664,0.39870965480804443
2254,-Metrics/Training(Step): loss,1610343343422,0.42662012577056885
2256,-Metrics/Training(Step): loss,1610343346548,0.3121355473995209
2258,-Metrics/Training(Step): loss,1610343350094,0.3843400478363037
2260,-Metrics/Training(Step): loss,1610343353549,0.29259490966796875
2262,-Metrics/Training(Step): loss,1610343357120,0.3784676194190979
2264,-Metrics/Training(Step): loss,1610343360820,0.3491870164871216
2266,-Metrics/Training(Step): loss,1610343364321,0.3696663975715637
2268,-Metrics/Training(Step): loss,1610343367424,0.34976115822792053
2270,-Metrics/Training(Step): loss,1610343371154,0.47828710079193115
2272,-Metrics/Training(Step): loss,1610343374844,0.3588958978652954
2274,-Metrics/Training(Step): loss,1610343378543,0.2812778055667877
2276,-Metrics/Training(Step): loss,1610343381408,0.31297603249549866
2278,-Metrics/Training(Step): loss,1610343384693,0.32613444328308105
2280,-Metrics/Training(Step): loss,1610343388119,0.4657312333583832
2282,-Metrics/Training(Step): loss,1610343391649,0.33579158782958984
2284,-Metrics/Training(Step): loss,1610343395522,0.30643194913864136
2286,-Metrics/Training(Step): loss,1610343397807,0.4499457776546478
2288,-Metrics/Training(Step): loss,1610343399917,0.2887084484100342
2290,-Metrics/Training(Step): loss,1610343402055,0.4130650460720062
2292,-Metrics/Training(Step): loss,1610343404109,0.3496014177799225
2294,-Metrics/Training(Step): loss,1610343406084,0.3380684554576874
2296,-Metrics/Training(Step): loss,1610343408167,0.3072108030319214
2298,-Metrics/Training(Step): loss,1610343410257,0.2931666970252991
2300,-Metrics/Training(Step): loss,1610343412348,0.35451188683509827
2302,-Metrics/Training(Step): loss,1610343414282,0.34617725014686584
2304,-Metrics/Training(Step): loss,1610343416363,0.33965423703193665
2306,-Metrics/Training(Step): loss,1610343418135,0.2841450870037079
2308,-Metrics/Training(Step): loss,1610343420229,0.33039578795433044
2310,-Metrics/Training(Step): loss,1610343422342,0.3850328326225281
2312,-Metrics/Training(Step): loss,1610343424445,0.24697880446910858
2314,-Metrics/Training(Step): loss,1610343426560,0.3125055432319641
2316,-Metrics/Training(Step): loss,1610343428486,0.267335444688797
2318,-Metrics/Training(Step): loss,1610343430485,0.24977464973926544
2320,-Metrics/Training(Step): loss,1610343432534,0.3493705689907074
2322,-Metrics/Training(Step): loss,1610343434587,0.29881152510643005
2324,-Metrics/Training(Step): loss,1610343447731,0.2995564341545105
2326,-Metrics/Training(Step): loss,1610343451915,0.30506059527397156
2328,-Metrics/Training(Step): loss,1610343456219,0.40919703245162964
2330,-Metrics/Training(Step): loss,1610343460455,0.3800557255744934
2332,-Metrics/Training(Step): loss,1610343464820,0.29809367656707764
2334,-Metrics/Training(Step): loss,1610343468629,0.29112377762794495
2336,-Metrics/Training(Step): loss,1610343472820,0.3905695080757141
2338,-Metrics/Training(Step): loss,1610343476621,0.32419323921203613
2340,-Metrics/Training(Step): loss,1610343480720,0.3670235872268677
2342,-Metrics/Training(Step): loss,1610343484320,0.29912424087524414
2344,-Metrics/Training(Step): loss,1610343487958,0.28577837347984314
2346,-Metrics/Training(Step): loss,1610343491914,0.28696584701538086
2348,-Metrics/Training(Step): loss,1610343495161,0.29132306575775146
2350,-Metrics/Training(Step): loss,1610343498755,0.3683207333087921
2352,-Metrics/Training(Step): loss,1610343502420,0.30289891362190247
2354,-Metrics/Training(Step): loss,1610343505645,0.3319041430950165
2356,-Metrics/Training(Step): loss,1610343509519,0.3738088011741638
2358,-Metrics/Training(Step): loss,1610343512503,0.3402729332447052
2360,-Metrics/Training(Step): loss,1610343515734,0.28636351227760315
2362,-Metrics/Training(Step): loss,1610343519019,0.3059597313404083
2364,-Metrics/Training(Step): loss,1610343522619,0.2941935658454895
2366,-Metrics/Training(Step): loss,1610343526141,0.37307992577552795
2368,-Metrics/Training(Step): loss,1610343529660,0.3013993203639984
2370,-Metrics/Training(Step): loss,1610343532519,0.2829887866973877
2372,-Metrics/Training(Step): loss,1610343534859,0.3249998986721039
2374,-Metrics/Training(Step): loss,1610343537019,0.2777325510978699
2376,-Metrics/Training(Step): loss,1610343539253,0.30199846625328064
2378,-Metrics/Training(Step): loss,1610343541344,0.34763050079345703
2380,-Metrics/Training(Step): loss,1610343543410,0.3382887840270996
2382,-Metrics/Training(Step): loss,1610343545443,0.3190149664878845
2384,-Metrics/Training(Step): loss,1610343547551,0.35076385736465454
2386,-Metrics/Training(Step): loss,1610343549495,0.27652767300605774
2388,-Metrics/Training(Step): loss,1610343551555,0.30964186787605286
2390,-Metrics/Training(Step): loss,1610343553665,0.29014500975608826
2392,-Metrics/Training(Step): loss,1610343555668,0.3204953074455261
2394,-Metrics/Training(Step): loss,1610343557739,0.38751867413520813
2396,-Metrics/Training(Step): loss,1610343559795,0.2853469252586365
2398,-Metrics/Training(Step): loss,1610343561685,0.33141225576400757
2400,-Metrics/Training(Step): loss,1610343563726,0.32377007603645325
2402,-Metrics/Training(Step): loss,1610343565792,0.3057340383529663
2404,-Metrics/Training(Step): loss,1610343567781,0.2875237464904785
2406,-Metrics/Training(Step): loss,1610343569812,0.33566829562187195
2408,-Metrics/Training(Step): loss,1610343571897,0.3498716354370117
2410,-Metrics/Training(Step): loss,1610343584436,0.2957671880722046
2412,-Metrics/Training(Step): loss,1610343588422,0.3344007730484009
2414,-Metrics/Training(Step): loss,1610343592442,0.3161710202693939
2416,-Metrics/Training(Step): loss,1610343596621,0.2523694336414337
2418,-Metrics/Training(Step): loss,1610343600752,0.31065958738327026
2420,-Metrics/Training(Step): loss,1610343604719,0.32850155234336853
2422,-Metrics/Training(Step): loss,1610343608731,0.35335439443588257
2424,-Metrics/Training(Step): loss,1610343612320,0.3484415113925934
2426,-Metrics/Training(Step): loss,1610343615586,0.3100120723247528
2428,-Metrics/Training(Step): loss,1610343619138,0.3348900079727173
2430,-Metrics/Training(Step): loss,1610343622564,0.3345155417919159
2432,-Metrics/Training(Step): loss,1610343625884,0.31853267550468445
2434,-Metrics/Training(Step): loss,1610343629436,0.3390214443206787
2436,-Metrics/Training(Step): loss,1610343632520,0.2594306170940399
2438,-Metrics/Training(Step): loss,1610343635915,0.3552975058555603
2440,-Metrics/Training(Step): loss,1610343639748,0.2903826832771301
2442,-Metrics/Training(Step): loss,1610343642956,0.33293989300727844
2444,-Metrics/Training(Step): loss,1610343645919,0.3301408886909485
2446,-Metrics/Training(Step): loss,1610343649720,0.3163306415081024
2448,-Metrics/Training(Step): loss,1610343653325,0.335730642080307
2450,-Metrics/Training(Step): loss,1610343656717,0.312456876039505
2452,-Metrics/Training(Step): loss,1610343659541,0.34162643551826477
2454,-Metrics/Training(Step): loss,1610343663021,0.3233213722705841
2456,-Metrics/Training(Step): loss,1610343666724,0.3408873677253723
2458,-Metrics/Training(Step): loss,1610343669784,0.3323175609111786
2460,-Metrics/Training(Step): loss,1610343671980,0.208632230758667
2462,-Metrics/Training(Step): loss,1610343674221,0.2397780865430832
2464,-Metrics/Training(Step): loss,1610343676361,0.3434855341911316
2466,-Metrics/Training(Step): loss,1610343678478,0.4353015720844269
2468,-Metrics/Training(Step): loss,1610343680492,0.287095308303833
2470,-Metrics/Training(Step): loss,1610343682518,0.32422152161598206
2472,-Metrics/Training(Step): loss,1610343684593,0.32450148463249207
2474,-Metrics/Training(Step): loss,1610343686476,0.2547386586666107
2476,-Metrics/Training(Step): loss,1610343688486,0.4449447989463806
2478,-Metrics/Training(Step): loss,1610343690444,0.3263152241706848
2480,-Metrics/Training(Step): loss,1610343692489,0.2726370394229889
2482,-Metrics/Training(Step): loss,1610343694578,0.34505149722099304
2484,-Metrics/Training(Step): loss,1610343696606,0.29261913895606995
2486,-Metrics/Training(Step): loss,1610343698641,0.3740083575248718
2488,-Metrics/Training(Step): loss,1610343700550,0.2438802421092987
2490,-Metrics/Training(Step): loss,1610343702587,0.35735976696014404
2492,-Metrics/Training(Step): loss,1610343704650,0.3811260163784027
2494,-Metrics/Training(Step): loss,1610343706710,0.3347337245941162
2496,-Metrics/Training(Step): loss,1610343719624,0.28556033968925476
2498,-Metrics/Training(Step): loss,1610343723636,0.247846782207489
2500,-Metrics/Training(Step): loss,1610343727535,0.2940104305744171
2502,-Metrics/Training(Step): loss,1610343732220,0.23118340969085693
2504,-Metrics/Training(Step): loss,1610343736218,0.3365175127983093
2506,-Metrics/Training(Step): loss,1610343739820,0.31056031584739685
2508,-Metrics/Training(Step): loss,1610343743238,0.27388808131217957
2510,-Metrics/Training(Step): loss,1610343747021,0.44363340735435486
2512,-Metrics/Training(Step): loss,1610343750521,0.36916670203208923
2514,-Metrics/Training(Step): loss,1610343754020,0.26415401697158813
2516,-Metrics/Training(Step): loss,1610343757329,0.27994564175605774
2518,-Metrics/Training(Step): loss,1610343760823,0.305298388004303
2520,-Metrics/Training(Step): loss,1610343764258,0.30721110105514526
2522,-Metrics/Training(Step): loss,1610343768024,0.30951881408691406
2524,-Metrics/Training(Step): loss,1610343771442,0.28700166940689087
2526,-Metrics/Training(Step): loss,1610343775338,0.35785195231437683
2528,-Metrics/Training(Step): loss,1610343778789,0.33783671259880066
2530,-Metrics/Training(Step): loss,1610343782436,0.37014099955558777
2532,-Metrics/Training(Step): loss,1610343785643,0.2489989846944809
2534,-Metrics/Training(Step): loss,1610343789752,0.33476483821868896
2536,-Metrics/Training(Step): loss,1610343792844,0.310659795999527
2538,-Metrics/Training(Step): loss,1610343796733,0.3905237019062042
2540,-Metrics/Training(Step): loss,1610343800720,0.39210283756256104
2542,-Metrics/Training(Step): loss,1610343803832,0.34372279047966003
2544,-Metrics/Training(Step): loss,1610343806014,0.36333343386650085
2546,-Metrics/Training(Step): loss,1610343808289,0.32486751675605774
2548,-Metrics/Training(Step): loss,1610343810445,0.233627587556839
2550,-Metrics/Training(Step): loss,1610343812464,0.32384324073791504
2552,-Metrics/Training(Step): loss,1610343814447,0.261746883392334
2554,-Metrics/Training(Step): loss,1610343816460,0.31379568576812744
2556,-Metrics/Training(Step): loss,1610343818561,0.42444658279418945
2558,-Metrics/Training(Step): loss,1610343820651,0.39102911949157715
2560,-Metrics/Training(Step): loss,1610343822731,0.30847641825675964
2562,-Metrics/Training(Step): loss,1610343824839,0.32924318313598633
2564,-Metrics/Training(Step): loss,1610343826763,0.30438029766082764
2566,-Metrics/Training(Step): loss,1610343828576,0.25104743242263794
2568,-Metrics/Training(Step): loss,1610343830447,0.30344676971435547
2570,-Metrics/Training(Step): loss,1610343832518,0.3113791346549988
2572,-Metrics/Training(Step): loss,1610343834463,0.4530720114707947
2574,-Metrics/Training(Step): loss,1610343836549,0.3409031629562378
2576,-Metrics/Training(Step): loss,1610343838582,0.36596933007240295
2578,-Metrics/Training(Step): loss,1610343840533,0.3487870693206787
2580,-Metrics/Training(Step): loss,1610343842562,0.2967201769351959
2582,-Metrics/Training(Step): loss,1610343869222,0.30178987979888916
2584,-Metrics/Training(Step): loss,1610343873620,0.32430779933929443
2586,-Metrics/Training(Step): loss,1610343877720,0.2823578417301178
2588,-Metrics/Training(Step): loss,1610343881421,0.32478398084640503
2590,-Metrics/Training(Step): loss,1610343885719,0.24880963563919067
2592,-Metrics/Training(Step): loss,1610343889791,0.23605437576770782
2594,-Metrics/Training(Step): loss,1610343893894,0.34033456444740295
2596,-Metrics/Training(Step): loss,1610343896921,0.26448631286621094
2598,-Metrics/Training(Step): loss,1610343900844,0.32454371452331543
2600,-Metrics/Training(Step): loss,1610343904220,0.41431576013565063
2602,-Metrics/Training(Step): loss,1610343907627,0.3302719295024872
2604,-Metrics/Training(Step): loss,1610343911220,0.35080334544181824
2606,-Metrics/Training(Step): loss,1610343915001,0.30904558300971985
2608,-Metrics/Training(Step): loss,1610343918655,0.3207724392414093
2610,-Metrics/Training(Step): loss,1610343922525,0.3240291476249695
2612,-Metrics/Training(Step): loss,1610343926041,0.24792098999023438
2614,-Metrics/Training(Step): loss,1610343929541,0.22620506584644318
2616,-Metrics/Training(Step): loss,1610343932887,0.2670455276966095
2618,-Metrics/Training(Step): loss,1610343936520,0.3245237469673157
2620,-Metrics/Training(Step): loss,1610343940170,0.3301508128643036
2622,-Metrics/Training(Step): loss,1610343943635,0.26584282517433167
2624,-Metrics/Training(Step): loss,1610343947101,0.2982904016971588
2626,-Metrics/Training(Step): loss,1610343950961,0.27596601843833923
2628,-Metrics/Training(Step): loss,1610343954019,0.25293394923210144
2630,-Metrics/Training(Step): loss,1610343956311,0.28369322419166565
2632,-Metrics/Training(Step): loss,1610343958327,0.2535444498062134
2634,-Metrics/Training(Step): loss,1610343960390,0.2765257954597473
2636,-Metrics/Training(Step): loss,1610343962494,0.3807927072048187
2638,-Metrics/Training(Step): loss,1610343964622,0.3305149972438812
2640,-Metrics/Training(Step): loss,1610343966581,0.2879020571708679
2642,-Metrics/Training(Step): loss,1610343968638,0.23855827748775482
2644,-Metrics/Training(Step): loss,1610343970612,0.37086349725723267
2646,-Metrics/Training(Step): loss,1610343972660,0.25567424297332764
2648,-Metrics/Training(Step): loss,1610343974621,0.24886129796504974
2650,-Metrics/Training(Step): loss,1610343976735,0.3191072344779968
2652,-Metrics/Training(Step): loss,1610343978629,0.3030882477760315
2654,-Metrics/Training(Step): loss,1610343980738,0.2924986481666565
2656,-Metrics/Training(Step): loss,1610343982765,0.27754610776901245
2658,-Metrics/Training(Step): loss,1610343984783,0.30739980936050415
2660,-Metrics/Training(Step): loss,1610343986751,0.3044373691082001
2662,-Metrics/Training(Step): loss,1610343988690,0.26417917013168335
2664,-Metrics/Training(Step): loss,1610343990764,0.2539732754230499
2666,-Metrics/Training(Step): loss,1610343992817,0.29960495233535767
2668,-Metrics/Training(Step): loss,1610344006225,0.3588760495185852
2670,-Metrics/Training(Step): loss,1610344010223,0.32325994968414307
2672,-Metrics/Training(Step): loss,1610344013815,0.328214168548584
2674,-Metrics/Training(Step): loss,1610344017820,0.23094013333320618
2676,-Metrics/Training(Step): loss,1610344022120,0.3205024003982544
2678,-Metrics/Training(Step): loss,1610344025849,0.28561636805534363
2680,-Metrics/Training(Step): loss,1610344029921,0.2828119397163391
2682,-Metrics/Training(Step): loss,1610344033523,0.27977827191352844
2684,-Metrics/Training(Step): loss,1610344037220,0.24164415895938873
2686,-Metrics/Training(Step): loss,1610344040733,0.2677638828754425
2688,-Metrics/Training(Step): loss,1610344044660,0.24457485973834991
2690,-Metrics/Training(Step): loss,1610344048220,0.3000375032424927
2692,-Metrics/Training(Step): loss,1610344051820,0.271543949842453
2694,-Metrics/Training(Step): loss,1610344055124,0.2084495723247528
2696,-Metrics/Training(Step): loss,1610344059020,0.3451038599014282
2698,-Metrics/Training(Step): loss,1610344062334,0.28190675377845764
2700,-Metrics/Training(Step): loss,1610344065744,0.2857082486152649
2702,-Metrics/Training(Step): loss,1610344069220,0.25078943371772766
2704,-Metrics/Training(Step): loss,1610344073047,0.2974495589733124
2706,-Metrics/Training(Step): loss,1610344076817,0.35530099272727966
2708,-Metrics/Training(Step): loss,1610344080107,0.28456780314445496
2710,-Metrics/Training(Step): loss,1610344083619,0.27382948994636536
2712,-Metrics/Training(Step): loss,1610344086756,0.23734617233276367
2714,-Metrics/Training(Step): loss,1610344089801,0.29197627305984497
2716,-Metrics/Training(Step): loss,1610344091963,0.2447868287563324
2718,-Metrics/Training(Step): loss,1610344094164,0.3058148920536041
2720,-Metrics/Training(Step): loss,1610344096240,0.33350762724876404
2722,-Metrics/Training(Step): loss,1610344098257,0.3033702075481415
2724,-Metrics/Training(Step): loss,1610344100339,0.25919654965400696
2726,-Metrics/Training(Step): loss,1610344102437,0.25604525208473206
2728,-Metrics/Training(Step): loss,1610344104555,0.2674112021923065
2730,-Metrics/Training(Step): loss,1610344106672,0.26869216561317444
2732,-Metrics/Training(Step): loss,1610344108627,0.28106439113616943
2734,-Metrics/Training(Step): loss,1610344110705,0.2666822075843811
2736,-Metrics/Training(Step): loss,1610344112734,0.2807242274284363
2738,-Metrics/Training(Step): loss,1610344114610,0.322936087846756
2740,-Metrics/Training(Step): loss,1610344116535,0.23478765785694122
2742,-Metrics/Training(Step): loss,1610344118485,0.2847047746181488
2744,-Metrics/Training(Step): loss,1610344120549,0.20740383863449097
2746,-Metrics/Training(Step): loss,1610344122629,0.2727547585964203
2748,-Metrics/Training(Step): loss,1610344124715,0.2682674825191498
2750,-Metrics/Training(Step): loss,1610344126662,0.2665281295776367
2752,-Metrics/Training(Step): loss,1610344128663,0.2244075983762741
2754,-Metrics/Training(Step): loss,1610344141430,0.2505500912666321
2756,-Metrics/Training(Step): loss,1610344145520,0.35781535506248474
2758,-Metrics/Training(Step): loss,1610344149318,0.24839961528778076
2760,-Metrics/Training(Step): loss,1610344153158,0.29786765575408936
2762,-Metrics/Training(Step): loss,1610344156937,0.3065592646598816
2764,-Metrics/Training(Step): loss,1610344160926,0.3092443645000458
2766,-Metrics/Training(Step): loss,1610344165525,0.25363805890083313
2768,-Metrics/Training(Step): loss,1610344169021,0.25346314907073975
2770,-Metrics/Training(Step): loss,1610344172686,0.28516465425491333
2772,-Metrics/Training(Step): loss,1610344176130,0.31896260380744934
2774,-Metrics/Training(Step): loss,1610344180020,0.2100594937801361
2776,-Metrics/Training(Step): loss,1610344183720,0.31352123618125916
2778,-Metrics/Training(Step): loss,1610344187120,0.37672796845436096
2780,-Metrics/Training(Step): loss,1610344190627,0.405428946018219
2782,-Metrics/Training(Step): loss,1610344194345,0.2651950716972351
2784,-Metrics/Training(Step): loss,1610344197740,0.4332726001739502
2786,-Metrics/Training(Step): loss,1610344201600,0.31590166687965393
2788,-Metrics/Training(Step): loss,1610344204519,0.5470131039619446
2790,-Metrics/Training(Step): loss,1610344208376,0.4432884454727173
2792,-Metrics/Training(Step): loss,1610344211923,0.439583957195282
2794,-Metrics/Training(Step): loss,1610344215720,0.6298468112945557
2796,-Metrics/Training(Step): loss,1610344219496,0.672274649143219
2798,-Metrics/Training(Step): loss,1610344222725,0.6100910305976868
2800,-Metrics/Training(Step): loss,1610344225361,0.7433825135231018
2802,-Metrics/Training(Step): loss,1610344227709,0.6400486826896667
2804,-Metrics/Training(Step): loss,1610344230011,0.7482597827911377
2806,-Metrics/Training(Step): loss,1610344232051,0.6158318519592285
2808,-Metrics/Training(Step): loss,1610344234145,0.6734291315078735
2810,-Metrics/Training(Step): loss,1610344236259,0.6432973742485046
2812,-Metrics/Training(Step): loss,1610344238324,0.49045529961586
2814,-Metrics/Training(Step): loss,1610344240346,0.7000441551208496
2816,-Metrics/Training(Step): loss,1610344242436,0.5036447048187256
2818,-Metrics/Training(Step): loss,1610344244482,0.6732993721961975
2820,-Metrics/Training(Step): loss,1610344246432,0.6004552245140076
2822,-Metrics/Training(Step): loss,1610344248521,0.6940496563911438
2824,-Metrics/Training(Step): loss,1610344250457,0.5774558186531067
2826,-Metrics/Training(Step): loss,1610344252584,0.38028258085250854
2828,-Metrics/Training(Step): loss,1610344254581,0.3934868276119232
2830,-Metrics/Training(Step): loss,1610344256524,0.5372000336647034
2832,-Metrics/Training(Step): loss,1610344258483,0.5220604538917542
2834,-Metrics/Training(Step): loss,1610344260527,0.5557866096496582
2836,-Metrics/Training(Step): loss,1610344262552,0.5584587454795837
2838,-Metrics/Training(Step): loss,1610344264565,0.4982195198535919
2840,-Metrics/Training(Step): loss,1610344277928,0.5792717933654785
2842,-Metrics/Training(Step): loss,1610344282015,0.513119637966156
2844,-Metrics/Training(Step): loss,1610344285915,0.343328058719635
2846,-Metrics/Training(Step): loss,1610344289922,0.34360432624816895
2848,-Metrics/Training(Step): loss,1610344293816,0.5102419853210449
2850,-Metrics/Training(Step): loss,1610344298020,0.40671786665916443
2852,-Metrics/Training(Step): loss,1610344302094,0.34603384137153625
2854,-Metrics/Training(Step): loss,1610344305521,0.3790358603000641
2856,-Metrics/Training(Step): loss,1610344309021,0.4403811991214752
2858,-Metrics/Training(Step): loss,1610344312620,0.45854678750038147
2860,-Metrics/Training(Step): loss,1610344316586,0.38551509380340576
2862,-Metrics/Training(Step): loss,1610344320535,0.3783316910266876
2864,-Metrics/Training(Step): loss,1610344324520,0.3721640706062317
2866,-Metrics/Training(Step): loss,1610344328411,0.3173362612724304
2868,-Metrics/Training(Step): loss,1610344331720,0.48156866431236267
2870,-Metrics/Training(Step): loss,1610344335110,0.4471529424190521
2872,-Metrics/Training(Step): loss,1610344338620,0.3442841172218323
2874,-Metrics/Training(Step): loss,1610344342549,0.35952457785606384
2876,-Metrics/Training(Step): loss,1610344346020,0.38438206911087036
2878,-Metrics/Training(Step): loss,1610344349521,0.4117925465106964
2880,-Metrics/Training(Step): loss,1610344352619,0.517521321773529
2882,-Metrics/Training(Step): loss,1610344355823,0.6139769554138184
2884,-Metrics/Training(Step): loss,1610344359240,0.6054884195327759
2886,-Metrics/Training(Step): loss,1610344362185,0.46976637840270996
2888,-Metrics/Training(Step): loss,1610344364397,0.6124865412712097
2890,-Metrics/Training(Step): loss,1610344366586,1.0261114835739136
2892,-Metrics/Training(Step): loss,1610344368606,0.499746710062027
2894,-Metrics/Training(Step): loss,1610344370708,0.707855224609375
2896,-Metrics/Training(Step): loss,1610344372794,0.40716293454170227
2898,-Metrics/Training(Step): loss,1610344374911,0.6073250770568848
2900,-Metrics/Training(Step): loss,1610344376994,0.7075626254081726
2902,-Metrics/Training(Step): loss,1610344379093,0.49358946084976196
2904,-Metrics/Training(Step): loss,1610344381147,0.5516207814216614
2906,-Metrics/Training(Step): loss,1610344383231,0.5954540371894836
2908,-Metrics/Training(Step): loss,1610344385184,0.6846683025360107
2910,-Metrics/Training(Step): loss,1610344387317,0.8876211643218994
2912,-Metrics/Training(Step): loss,1610344389355,0.5524218678474426
2914,-Metrics/Training(Step): loss,1610344391497,0.4571031928062439
2916,-Metrics/Training(Step): loss,1610344393511,0.4232258200645447
2918,-Metrics/Training(Step): loss,1610344395496,0.5605270862579346
2920,-Metrics/Training(Step): loss,1610344397565,0.4113534390926361
2922,-Metrics/Training(Step): loss,1610344399623,0.44867363572120667
2924,-Metrics/Training(Step): loss,1610344401627,0.6139951944351196
2926,-Metrics/Training(Step): loss,1610344413946,0.5152703523635864
2928,-Metrics/Training(Step): loss,1610344417922,0.4970279037952423
2930,-Metrics/Training(Step): loss,1610344421817,0.5768746733665466
2932,-Metrics/Training(Step): loss,1610344425732,0.5045658946037292
2934,-Metrics/Training(Step): loss,1610344430023,0.3343711197376251
2936,-Metrics/Training(Step): loss,1610344434023,0.30884501338005066
2938,-Metrics/Training(Step): loss,1610344437820,0.39602530002593994
2940,-Metrics/Training(Step): loss,1610344441520,0.4711296260356903
2942,-Metrics/Training(Step): loss,1610344445420,0.4730391502380371
2944,-Metrics/Training(Step): loss,1610344449321,0.4581783413887024
2946,-Metrics/Training(Step): loss,1610344452503,0.4225805103778839
2948,-Metrics/Training(Step): loss,1610344456645,0.4409841001033783
2950,-Metrics/Training(Step): loss,1610344460073,0.360031396150589
2952,-Metrics/Training(Step): loss,1610344463920,0.3653596043586731
2954,-Metrics/Training(Step): loss,1610344467614,0.40785640478134155
2956,-Metrics/Training(Step): loss,1610344470925,0.410778671503067
2958,-Metrics/Training(Step): loss,1610344474158,0.401886522769928
2960,-Metrics/Training(Step): loss,1610344477221,0.3679279386997223
2962,-Metrics/Training(Step): loss,1610344480542,0.3685334622859955
2964,-Metrics/Training(Step): loss,1610344484402,0.43622100353240967
2966,-Metrics/Training(Step): loss,1610344487468,0.33137524127960205
2968,-Metrics/Training(Step): loss,1610344491121,0.334548681974411
2970,-Metrics/Training(Step): loss,1610344494340,0.26608797907829285
2972,-Metrics/Training(Step): loss,1610344497642,0.4537094533443451
2974,-Metrics/Training(Step): loss,1610344500001,0.3661017119884491
2976,-Metrics/Training(Step): loss,1610344502238,0.43226975202560425
2978,-Metrics/Training(Step): loss,1610344504439,0.35080966353416443
2980,-Metrics/Training(Step): loss,1610344506522,0.38590532541275024
2982,-Metrics/Training(Step): loss,1610344508650,0.4250141680240631
2984,-Metrics/Training(Step): loss,1610344510805,0.3743553161621094
2986,-Metrics/Training(Step): loss,1610344512897,0.33520105481147766
2988,-Metrics/Training(Step): loss,1610344515053,0.2821146547794342
2990,-Metrics/Training(Step): loss,1610344517096,0.2755148410797119
2992,-Metrics/Training(Step): loss,1610344519194,0.4299567937850952
2994,-Metrics/Training(Step): loss,1610344521185,0.3521794378757477
2996,-Metrics/Training(Step): loss,1610344523246,0.36872780323028564
2998,-Metrics/Training(Step): loss,1610344525197,0.35933297872543335
3000,-Metrics/Training(Step): loss,1610344527212,0.3709934949874878
3002,-Metrics/Training(Step): loss,1610344529273,0.35367727279663086
3004,-Metrics/Training(Step): loss,1610344531351,0.3763170540332794
3006,-Metrics/Training(Step): loss,1610344533437,0.32943928241729736
3008,-Metrics/Training(Step): loss,1610344535513,0.4067552387714386
3010,-Metrics/Training(Step): loss,1610344537577,0.3024008572101593
3012,-Metrics/Training(Step): loss,1610344550527,0.3323654532432556
3014,-Metrics/Training(Step): loss,1610344554658,0.38715893030166626
3016,-Metrics/Training(Step): loss,1610344558319,0.2651858925819397
3018,-Metrics/Training(Step): loss,1610344562320,0.4599243700504303
3020,-Metrics/Training(Step): loss,1610344566458,0.39563003182411194
3022,-Metrics/Training(Step): loss,1610344570719,0.3211188018321991
3024,-Metrics/Training(Step): loss,1610344575419,0.3995715379714966
3026,-Metrics/Training(Step): loss,1610344579223,0.2984103560447693
3028,-Metrics/Training(Step): loss,1610344582766,0.3339840769767761
3030,-Metrics/Training(Step): loss,1610344586219,0.34133201837539673
3032,-Metrics/Training(Step): loss,1610344589245,0.3776684105396271
3034,-Metrics/Training(Step): loss,1610344592820,0.3807970881462097
3036,-Metrics/Training(Step): loss,1610344596319,0.3572578728199005
3038,-Metrics/Training(Step): loss,1610344600019,0.3119036555290222
3040,-Metrics/Training(Step): loss,1610344603322,0.3294365704059601
3042,-Metrics/Training(Step): loss,1610344606721,0.36161690950393677
3044,-Metrics/Training(Step): loss,1610344610021,0.314912348985672
3046,-Metrics/Training(Step): loss,1610344613320,0.2570961117744446
3048,-Metrics/Training(Step): loss,1610344616822,0.3339208662509918
3050,-Metrics/Training(Step): loss,1610344619882,0.29869237542152405
3052,-Metrics/Training(Step): loss,1610344623316,0.3138202130794525
3054,-Metrics/Training(Step): loss,1610344626496,0.3562397062778473
3056,-Metrics/Training(Step): loss,1610344629339,0.29287078976631165
3058,-Metrics/Training(Step): loss,1610344632240,0.31942498683929443
3060,-Metrics/Training(Step): loss,1610344635130,0.36703595519065857
3062,-Metrics/Training(Step): loss,1610344637524,0.3190884590148926
3064,-Metrics/Training(Step): loss,1610344639876,0.29398417472839355
3066,-Metrics/Training(Step): loss,1610344642087,0.32081127166748047
3068,-Metrics/Training(Step): loss,1610344644218,0.3015322685241699
3070,-Metrics/Training(Step): loss,1610344646233,0.34777355194091797
3072,-Metrics/Training(Step): loss,1610344648340,0.2828410267829895
3074,-Metrics/Training(Step): loss,1610344650352,0.281885027885437
3076,-Metrics/Training(Step): loss,1610344652436,0.3777369260787964
3078,-Metrics/Training(Step): loss,1610344654416,0.347133070230484
3080,-Metrics/Training(Step): loss,1610344656468,0.2655482292175293
3082,-Metrics/Training(Step): loss,1610344658543,0.34598225355148315
3084,-Metrics/Training(Step): loss,1610344660435,0.24929407238960266
3086,-Metrics/Training(Step): loss,1610344662405,0.21372142434120178
3088,-Metrics/Training(Step): loss,1610344664467,0.29569000005722046
3090,-Metrics/Training(Step): loss,1610344666512,0.33263319730758667
3092,-Metrics/Training(Step): loss,1610344668639,0.3230816423892975
3094,-Metrics/Training(Step): loss,1610344670655,0.3551168441772461
3096,-Metrics/Training(Step): loss,1610344672645,0.32143014669418335
3098,-Metrics/Training(Step): loss,1610344685431,0.3526268005371094
3100,-Metrics/Training(Step): loss,1610344689619,0.31581997871398926
3102,-Metrics/Training(Step): loss,1610344693219,0.27726343274116516
3104,-Metrics/Training(Step): loss,1610344697219,0.2691107392311096
3106,-Metrics/Training(Step): loss,1610344701121,0.40260493755340576
3108,-Metrics/Training(Step): loss,1610344705420,0.27561041712760925
3110,-Metrics/Training(Step): loss,1610344709228,0.3328409194946289
3112,-Metrics/Training(Step): loss,1610344712825,0.20982740819454193
3114,-Metrics/Training(Step): loss,1610344716122,0.3649289608001709
3116,-Metrics/Training(Step): loss,1610344720043,0.28446710109710693
3118,-Metrics/Training(Step): loss,1610344723424,0.27198275923728943
3120,-Metrics/Training(Step): loss,1610344726520,0.3398914039134979
3122,-Metrics/Training(Step): loss,1610344730120,0.26649513840675354
3124,-Metrics/Training(Step): loss,1610344733320,0.26314982771873474
3126,-Metrics/Training(Step): loss,1610344737125,0.3162941634654999
3128,-Metrics/Training(Step): loss,1610344741121,0.38609686493873596
3130,-Metrics/Training(Step): loss,1610344744842,0.2718699872493744
3132,-Metrics/Training(Step): loss,1610344748257,0.2757645845413208
3134,-Metrics/Training(Step): loss,1610344752105,0.34965091943740845
3136,-Metrics/Training(Step): loss,1610344755484,0.25798851251602173
3138,-Metrics/Training(Step): loss,1610344759220,0.3006531000137329
3140,-Metrics/Training(Step): loss,1610344762592,0.2783830165863037
3142,-Metrics/Training(Step): loss,1610344765555,0.3107301890850067
3144,-Metrics/Training(Step): loss,1610344768916,0.2708227336406708
3146,-Metrics/Training(Step): loss,1610344771565,0.3647129535675049
3148,-Metrics/Training(Step): loss,1610344773783,0.3218817114830017
3150,-Metrics/Training(Step): loss,1610344775799,0.2885293960571289
3152,-Metrics/Training(Step): loss,1610344777809,0.28258970379829407
3154,-Metrics/Training(Step): loss,1610344779909,0.3604394793510437
3156,-Metrics/Training(Step): loss,1610344781959,0.31330549716949463
3158,-Metrics/Training(Step): loss,1610344784068,0.32990241050720215
3160,-Metrics/Training(Step): loss,1610344786165,0.30457282066345215
3162,-Metrics/Training(Step): loss,1610344788243,0.4287319779396057
3164,-Metrics/Training(Step): loss,1610344790320,0.31710419058799744
3166,-Metrics/Training(Step): loss,1610344792216,0.29831045866012573
3168,-Metrics/Training(Step): loss,1610344794209,0.4056544303894043
3170,-Metrics/Training(Step): loss,1610344796311,0.3593655526638031
3172,-Metrics/Training(Step): loss,1610344798384,0.3068457245826721
3174,-Metrics/Training(Step): loss,1610344800332,0.276404470205307
3176,-Metrics/Training(Step): loss,1610344802382,0.28547483682632446
3178,-Metrics/Training(Step): loss,1610344804388,0.3663790225982666
3180,-Metrics/Training(Step): loss,1610344806419,0.332196980714798
3182,-Metrics/Training(Step): loss,1610344808503,0.37711530923843384
3184,-Metrics/Training(Step): loss,1610344821435,0.2822256088256836
3186,-Metrics/Training(Step): loss,1610344825624,0.3711433708667755
3188,-Metrics/Training(Step): loss,1610344829319,0.2657362222671509
3190,-Metrics/Training(Step): loss,1610344833226,0.290694922208786
3192,-Metrics/Training(Step): loss,1610344837726,0.3646745979785919
3194,-Metrics/Training(Step): loss,1610344841820,0.28066396713256836
3196,-Metrics/Training(Step): loss,1610344846018,0.34841036796569824
3198,-Metrics/Training(Step): loss,1610344849316,0.3756526708602905
3200,-Metrics/Training(Step): loss,1610344852671,0.25530198216438293
3202,-Metrics/Training(Step): loss,1610344856420,0.3628676235675812
3204,-Metrics/Training(Step): loss,1610344860021,0.2795118987560272
3206,-Metrics/Training(Step): loss,1610344863062,0.37807175517082214
3208,-Metrics/Training(Step): loss,1610344866370,0.31797102093696594
3210,-Metrics/Training(Step): loss,1610344869736,0.30621659755706787
3212,-Metrics/Training(Step): loss,1610344873204,0.32209330797195435
3214,-Metrics/Training(Step): loss,1610344876819,0.3290766179561615
3216,-Metrics/Training(Step): loss,1610344879945,0.3555302917957306
3218,-Metrics/Training(Step): loss,1610344883523,0.30436837673187256
3220,-Metrics/Training(Step): loss,1610344887419,0.3412143290042877
3222,-Metrics/Training(Step): loss,1610344890981,0.31992921233177185
3224,-Metrics/Training(Step): loss,1610344894562,0.23013482987880707
3226,-Metrics/Training(Step): loss,1610344898364,0.3466359078884125
3228,-Metrics/Training(Step): loss,1610344901724,0.2945587635040283
3230,-Metrics/Training(Step): loss,1610344904959,0.3232026696205139
3232,-Metrics/Training(Step): loss,1610344907619,0.31118932366371155
3234,-Metrics/Training(Step): loss,1610344909697,0.20803549885749817
3236,-Metrics/Training(Step): loss,1610344911806,0.3706338107585907
3238,-Metrics/Training(Step): loss,1610344913922,0.33679255843162537
3240,-Metrics/Training(Step): loss,1610344915984,0.2959202527999878
3242,-Metrics/Training(Step): loss,1610344918044,0.3230911195278168
3244,-Metrics/Training(Step): loss,1610344920187,0.27826812863349915
3246,-Metrics/Training(Step): loss,1610344922315,0.26689648628234863
3248,-Metrics/Training(Step): loss,1610344924450,0.3138786256313324
3250,-Metrics/Training(Step): loss,1610344926585,0.3034925162792206
3252,-Metrics/Training(Step): loss,1610344928468,0.32278916239738464
3254,-Metrics/Training(Step): loss,1610344930499,0.36395853757858276
3256,-Metrics/Training(Step): loss,1610344932529,0.2633880078792572
3258,-Metrics/Training(Step): loss,1610344934537,0.36664944887161255
3260,-Metrics/Training(Step): loss,1610344936555,0.2823261320590973
3262,-Metrics/Training(Step): loss,1610344938595,0.4583578407764435
3264,-Metrics/Training(Step): loss,1610344940654,0.5118316411972046
3266,-Metrics/Training(Step): loss,1610344942714,0.32204198837280273
3268,-Metrics/Training(Step): loss,1610344944674,0.3024507462978363
3270,-Metrics/Training(Step): loss,1610344957823,0.4540141522884369
3272,-Metrics/Training(Step): loss,1610344962020,0.4341350495815277
3274,-Metrics/Training(Step): loss,1610344965820,0.5939088463783264
3276,-Metrics/Training(Step): loss,1610344970131,0.4975559413433075
3278,-Metrics/Training(Step): loss,1610344974325,0.4086112082004547
3280,-Metrics/Training(Step): loss,1610344977721,0.37845584750175476
3282,-Metrics/Training(Step): loss,1610344981220,0.3554176688194275
3284,-Metrics/Training(Step): loss,1610344984562,0.3592663109302521
3286,-Metrics/Training(Step): loss,1610344988047,0.35154664516448975
3288,-Metrics/Training(Step): loss,1610344991722,0.3401671051979065
3290,-Metrics/Training(Step): loss,1610344995140,0.30797284841537476
3292,-Metrics/Training(Step): loss,1610344998538,0.38507184386253357
3294,-Metrics/Training(Step): loss,1610345002321,0.31186142563819885
3296,-Metrics/Training(Step): loss,1610345005951,0.38382381200790405
3298,-Metrics/Training(Step): loss,1610345009851,0.36880651116371155
3300,-Metrics/Training(Step): loss,1610345012920,0.3002786934375763
3302,-Metrics/Training(Step): loss,1610345016278,0.49312925338745117
3304,-Metrics/Training(Step): loss,1610345019916,0.35032159090042114
3306,-Metrics/Training(Step): loss,1610345023244,0.3369562327861786
3308,-Metrics/Training(Step): loss,1610345026881,0.3884750008583069
3310,-Metrics/Training(Step): loss,1610345030233,0.3562621772289276
3312,-Metrics/Training(Step): loss,1610345034022,0.3812536895275116
3314,-Metrics/Training(Step): loss,1610345037627,0.35165858268737793
3316,-Metrics/Training(Step): loss,1610345041124,0.28407764434814453
3318,-Metrics/Training(Step): loss,1610345043820,0.27678102254867554
3320,-Metrics/Training(Step): loss,1610345046084,0.32283732295036316
3322,-Metrics/Training(Step): loss,1610345048205,0.2693830728530884
3324,-Metrics/Training(Step): loss,1610345050372,0.38006356358528137
3326,-Metrics/Training(Step): loss,1610345052488,0.3463006317615509
3328,-Metrics/Training(Step): loss,1610345054546,0.42536962032318115
3330,-Metrics/Training(Step): loss,1610345056680,0.36240819096565247
3332,-Metrics/Training(Step): loss,1610345058760,0.398685485124588
3334,-Metrics/Training(Step): loss,1610345060781,0.31050604581832886
3336,-Metrics/Training(Step): loss,1610345062891,0.30337992310523987
3338,-Metrics/Training(Step): loss,1610345064988,0.3296155631542206
3340,-Metrics/Training(Step): loss,1610345067075,0.24172891676425934
3342,-Metrics/Training(Step): loss,1610345069062,0.2552514672279358
3344,-Metrics/Training(Step): loss,1610345071096,0.3023994266986847
3346,-Metrics/Training(Step): loss,1610345073132,0.2924022376537323
3348,-Metrics/Training(Step): loss,1610345075232,0.2693309783935547
3350,-Metrics/Training(Step): loss,1610345077326,0.32836946845054626
3352,-Metrics/Training(Step): loss,1610345079329,0.34353455901145935
3354,-Metrics/Training(Step): loss,1610345081427,0.3182674050331116
3356,-Metrics/Training(Step): loss,1610345093938,0.21658454835414886
3358,-Metrics/Training(Step): loss,1610345098420,0.27702659368515015
3360,-Metrics/Training(Step): loss,1610345102637,0.37517762184143066
3362,-Metrics/Training(Step): loss,1610345106831,0.2909379005432129
3364,-Metrics/Training(Step): loss,1610345110523,0.25388824939727783
3366,-Metrics/Training(Step): loss,1610345114520,0.3049921691417694
3368,-Metrics/Training(Step): loss,1610345118219,0.26005396246910095
3370,-Metrics/Training(Step): loss,1610345122240,0.27715936303138733
3372,-Metrics/Training(Step): loss,1610345125720,0.2698949873447418
3374,-Metrics/Training(Step): loss,1610345129020,0.2751633822917938
3376,-Metrics/Training(Step): loss,1610345132797,0.28427979350090027
3378,-Metrics/Training(Step): loss,1610345136329,0.24977949261665344
3380,-Metrics/Training(Step): loss,1610345139943,0.2515351176261902
3382,-Metrics/Training(Step): loss,1610345143222,0.26375365257263184
3384,-Metrics/Training(Step): loss,1610345146620,0.25709304213523865
3386,-Metrics/Training(Step): loss,1610345150321,0.29700154066085815
3388,-Metrics/Training(Step): loss,1610345154105,0.29272738099098206
3390,-Metrics/Training(Step): loss,1610345157242,0.34679335355758667
3392,-Metrics/Training(Step): loss,1610345160223,0.29465582966804504
3394,-Metrics/Training(Step): loss,1610345164188,0.27435269951820374
3396,-Metrics/Training(Step): loss,1610345167931,0.3294166922569275
3398,-Metrics/Training(Step): loss,1610345172224,0.29329949617385864
3400,-Metrics/Training(Step): loss,1610345175722,0.23159919679164886
3402,-Metrics/Training(Step): loss,1610345179066,0.2887008488178253
3404,-Metrics/Training(Step): loss,1610345181337,0.2884264886379242
3406,-Metrics/Training(Step): loss,1610345183593,0.41623878479003906
3408,-Metrics/Training(Step): loss,1610345185700,0.3054203689098358
3410,-Metrics/Training(Step): loss,1610345187811,0.27259209752082825
3412,-Metrics/Training(Step): loss,1610345189808,0.3610493838787079
3414,-Metrics/Training(Step): loss,1610345191846,0.2725066542625427
3416,-Metrics/Training(Step): loss,1610345193967,0.2759721875190735
3418,-Metrics/Training(Step): loss,1610345196051,0.34446004033088684
3420,-Metrics/Training(Step): loss,1610345198111,0.31372979283332825
3422,-Metrics/Training(Step): loss,1610345200041,0.25081560015678406
3424,-Metrics/Training(Step): loss,1610345202046,0.2837243676185608
3426,-Metrics/Training(Step): loss,1610345204150,0.24263915419578552
3428,-Metrics/Training(Step): loss,1610345206239,0.25499847531318665
3430,-Metrics/Training(Step): loss,1610345208167,0.26599711179733276
3432,-Metrics/Training(Step): loss,1610345210089,0.3390074074268341
3434,-Metrics/Training(Step): loss,1610345212193,0.295165479183197
3436,-Metrics/Training(Step): loss,1610345214265,0.2834497094154358
3438,-Metrics/Training(Step): loss,1610345216305,0.17327073216438293
3440,-Metrics/Training(Step): loss,1610345218337,0.320730060338974
3442,-Metrics/Training(Step): loss,1610345230633,0.22332139313220978
3444,-Metrics/Training(Step): loss,1610345234719,0.3417353332042694
3446,-Metrics/Training(Step): loss,1610345238820,0.30629363656044006
3448,-Metrics/Training(Step): loss,1610345242919,0.2846776247024536
3450,-Metrics/Training(Step): loss,1610345247015,0.26349398493766785
3452,-Metrics/Training(Step): loss,1610345251020,0.3025720715522766
3454,-Metrics/Training(Step): loss,1610345254958,0.2624996602535248
3456,-Metrics/Training(Step): loss,1610345258720,0.2913261651992798
3458,-Metrics/Training(Step): loss,1610345262620,0.3048212230205536
3460,-Metrics/Training(Step): loss,1610345265933,0.2931179106235504
3462,-Metrics/Training(Step): loss,1610345269720,0.3303154408931732
3464,-Metrics/Training(Step): loss,1610345272821,0.24624474346637726
3466,-Metrics/Training(Step): loss,1610345276697,0.2652634084224701
3468,-Metrics/Training(Step): loss,1610345280420,0.3607243597507477
3470,-Metrics/Training(Step): loss,1610345284266,0.27621573209762573
3472,-Metrics/Training(Step): loss,1610345287421,0.2234620600938797
3474,-Metrics/Training(Step): loss,1610345290942,0.3126243054866791
3476,-Metrics/Training(Step): loss,1610345294819,0.19271570444107056
3478,-Metrics/Training(Step): loss,1610345297800,0.2822512984275818
3480,-Metrics/Training(Step): loss,1610345300966,0.27511584758758545
3482,-Metrics/Training(Step): loss,1610345304499,0.2378973513841629
3484,-Metrics/Training(Step): loss,1610345308097,0.20920291543006897
3486,-Metrics/Training(Step): loss,1610345312104,0.23857690393924713
3488,-Metrics/Training(Step): loss,1610345315547,0.26492658257484436
3490,-Metrics/Training(Step): loss,1610345317792,0.25548115372657776
3492,-Metrics/Training(Step): loss,1610345319952,0.25043854117393494
3494,-Metrics/Training(Step): loss,1610345322075,0.25377199053764343
3496,-Metrics/Training(Step): loss,1610345324161,0.23101478815078735
3498,-Metrics/Training(Step): loss,1610345326237,0.2457917332649231
3500,-Metrics/Training(Step): loss,1610345328348,0.2226073443889618
3502,-Metrics/Training(Step): loss,1610345330448,0.253262460231781
3504,-Metrics/Training(Step): loss,1610345332476,0.29118457436561584
3506,-Metrics/Training(Step): loss,1610345334186,0.26428183913230896
3508,-Metrics/Training(Step): loss,1610345336284,0.28762921690940857
3510,-Metrics/Training(Step): loss,1610345338200,0.2955380976200104
3512,-Metrics/Training(Step): loss,1610345340074,0.2636675536632538
3514,-Metrics/Training(Step): loss,1610345342123,0.28523561358451843
3516,-Metrics/Training(Step): loss,1610345344081,0.2890400290489197
3518,-Metrics/Training(Step): loss,1610345346207,0.31207942962646484
3520,-Metrics/Training(Step): loss,1610345348230,0.30851227045059204
3522,-Metrics/Training(Step): loss,1610345350331,0.3171054720878601
3524,-Metrics/Training(Step): loss,1610345352364,0.29556748270988464
3526,-Metrics/Training(Step): loss,1610345354418,0.289345383644104
3528,-Metrics/Training(Step): loss,1610345366741,0.23181544244289398
3530,-Metrics/Training(Step): loss,1610345370721,0.34335535764694214
3532,-Metrics/Training(Step): loss,1610345375030,0.2936747968196869
3534,-Metrics/Training(Step): loss,1610345378920,0.3017529249191284
3536,-Metrics/Training(Step): loss,1610345382620,0.2734299600124359
3538,-Metrics/Training(Step): loss,1610345386820,0.27205970883369446
3540,-Metrics/Training(Step): loss,1610345390920,0.24996346235275269
3542,-Metrics/Training(Step): loss,1610345394820,0.28784751892089844
3544,-Metrics/Training(Step): loss,1610345398722,0.2924656271934509
3546,-Metrics/Training(Step): loss,1610345402622,0.2611670196056366
3548,-Metrics/Training(Step): loss,1610345406085,0.2267482876777649
3550,-Metrics/Training(Step): loss,1610345410008,0.28330540657043457
3552,-Metrics/Training(Step): loss,1610345414155,0.25696441531181335
3554,-Metrics/Training(Step): loss,1610345417790,0.2556241750717163
3556,-Metrics/Training(Step): loss,1610345421442,0.2966495752334595
3558,-Metrics/Training(Step): loss,1610345425497,0.2824576795101166
3560,-Metrics/Training(Step): loss,1610345429052,0.24423973262310028
3562,-Metrics/Training(Step): loss,1610345433215,0.1952635943889618
3564,-Metrics/Training(Step): loss,1610345436638,0.22950835525989532
3566,-Metrics/Training(Step): loss,1610345440126,0.2704874277114868
3568,-Metrics/Training(Step): loss,1610345444317,0.28623273968696594
3570,-Metrics/Training(Step): loss,1610345447656,0.28182196617126465
3572,-Metrics/Training(Step): loss,1610345450550,0.2467053085565567
3574,-Metrics/Training(Step): loss,1610345453069,0.24937903881072998
3576,-Metrics/Training(Step): loss,1610345455255,0.27626386284828186
3578,-Metrics/Training(Step): loss,1610345457283,0.22132034599781036
3580,-Metrics/Training(Step): loss,1610345459291,0.2523384392261505
3582,-Metrics/Training(Step): loss,1610345461392,0.18790921568870544
3584,-Metrics/Training(Step): loss,1610345463481,0.24677345156669617
3586,-Metrics/Training(Step): loss,1610345465638,0.29670649766921997
3588,-Metrics/Training(Step): loss,1610345467711,0.25711768865585327
3590,-Metrics/Training(Step): loss,1610345469756,0.23519375920295715
3592,-Metrics/Training(Step): loss,1610345471796,0.2285776436328888
3594,-Metrics/Training(Step): loss,1610345473829,0.18786685168743134
3596,-Metrics/Training(Step): loss,1610345475861,0.2498020976781845
3598,-Metrics/Training(Step): loss,1610345477977,0.278839111328125
3600,-Metrics/Training(Step): loss,1610345480068,0.23072770237922668
3602,-Metrics/Training(Step): loss,1610345482044,0.17366836965084076
3604,-Metrics/Training(Step): loss,1610345484134,0.2854113280773163
3606,-Metrics/Training(Step): loss,1610345486219,0.24850814044475555
3608,-Metrics/Training(Step): loss,1610345488318,0.2881946265697479
3610,-Metrics/Training(Step): loss,1610345490381,0.2773418724536896
3612,-Metrics/Training(Step): loss,1610345492497,0.2172812968492508
3614,-Metrics/Training(Step): loss,1610345505333,0.2755480408668518
3616,-Metrics/Training(Step): loss,1610345509820,0.32136374711990356
3618,-Metrics/Training(Step): loss,1610345513930,0.23640765249729156
3620,-Metrics/Training(Step): loss,1610345517820,0.24580620229244232
3622,-Metrics/Training(Step): loss,1610345521420,0.2929781675338745
3624,-Metrics/Training(Step): loss,1610345525420,0.2923124432563782
3626,-Metrics/Training(Step): loss,1610345529619,0.19946183264255524
3628,-Metrics/Training(Step): loss,1610345533214,0.20086801052093506
3630,-Metrics/Training(Step): loss,1610345536515,0.21272622048854828
3632,-Metrics/Training(Step): loss,1610345539544,0.21258527040481567
3634,-Metrics/Training(Step): loss,1610345543321,0.29945504665374756
3636,-Metrics/Training(Step): loss,1610345547420,0.2455989271402359
3638,-Metrics/Training(Step): loss,1610345551044,0.2536461055278778
3640,-Metrics/Training(Step): loss,1610345554923,0.36605021357536316
3642,-Metrics/Training(Step): loss,1610345558620,0.22994394600391388
3644,-Metrics/Training(Step): loss,1610345561820,0.28142049908638
3646,-Metrics/Training(Step): loss,1610345566020,0.2215782105922699
3648,-Metrics/Training(Step): loss,1610345570050,0.20044977962970734
3650,-Metrics/Training(Step): loss,1610345573986,0.26707136631011963
3652,-Metrics/Training(Step): loss,1610345577734,0.26772263646125793
3654,-Metrics/Training(Step): loss,1610345581320,0.28096598386764526
3656,-Metrics/Training(Step): loss,1610345585145,0.2956305742263794
3658,-Metrics/Training(Step): loss,1610345588367,0.3008417785167694
3660,-Metrics/Training(Step): loss,1610345590900,0.22789061069488525
3662,-Metrics/Training(Step): loss,1610345593126,0.22055868804454803
3664,-Metrics/Training(Step): loss,1610345595432,0.27415746450424194
3666,-Metrics/Training(Step): loss,1610345597414,0.2303069531917572
3668,-Metrics/Training(Step): loss,1610345599457,0.2232540398836136
3670,-Metrics/Training(Step): loss,1610345601559,0.27436521649360657
3672,-Metrics/Training(Step): loss,1610345603654,0.28902244567871094
3674,-Metrics/Training(Step): loss,1610345605803,0.2301463782787323
3676,-Metrics/Training(Step): loss,1610345607678,0.2724478840827942
3678,-Metrics/Training(Step): loss,1610345609692,0.2625732421875
3680,-Metrics/Training(Step): loss,1610345611734,0.2540200650691986
3682,-Metrics/Training(Step): loss,1610345613795,0.20767280459403992
3684,-Metrics/Training(Step): loss,1610345615922,0.2845940589904785
3686,-Metrics/Training(Step): loss,1610345617999,0.2951418459415436
3688,-Metrics/Training(Step): loss,1610345620044,0.26694127917289734
3690,-Metrics/Training(Step): loss,1610345622112,0.22369512915611267
3692,-Metrics/Training(Step): loss,1610345624126,0.23671303689479828
3694,-Metrics/Training(Step): loss,1610345626244,0.38173213601112366
3696,-Metrics/Training(Step): loss,1610345628358,0.2597366273403168
3698,-Metrics/Training(Step): loss,1610345630357,0.3377935588359833
3700,-Metrics/Training(Step): loss,1610345643934,0.27062660455703735
3702,-Metrics/Training(Step): loss,1610345648035,0.35674870014190674
3704,-Metrics/Training(Step): loss,1610345652219,0.257209450006485
3706,-Metrics/Training(Step): loss,1610345656222,0.37015753984451294
3708,-Metrics/Training(Step): loss,1610345660220,0.218522310256958
3710,-Metrics/Training(Step): loss,1610345664053,0.24223265051841736
3712,-Metrics/Training(Step): loss,1610345667684,0.372517466545105
3714,-Metrics/Training(Step): loss,1610345670929,0.24359381198883057
3716,-Metrics/Training(Step): loss,1610345675020,0.2945321500301361
3718,-Metrics/Training(Step): loss,1610345678821,0.24296154081821442
3720,-Metrics/Training(Step): loss,1610345682420,0.3491179049015045
3722,-Metrics/Training(Step): loss,1610345685740,0.26222899556159973
3724,-Metrics/Training(Step): loss,1610345689242,0.20208463072776794
3726,-Metrics/Training(Step): loss,1610345693020,0.27010151743888855
3728,-Metrics/Training(Step): loss,1610345696420,0.23855449259281158
3730,-Metrics/Training(Step): loss,1610345698929,0.29202255606651306
3732,-Metrics/Training(Step): loss,1610345702056,0.31704193353652954
3734,-Metrics/Training(Step): loss,1610345705520,0.28556138277053833
3736,-Metrics/Training(Step): loss,1610345709419,0.2712666392326355
3738,-Metrics/Training(Step): loss,1610345712820,0.23712776601314545
3740,-Metrics/Training(Step): loss,1610345716236,0.26091426610946655
3742,-Metrics/Training(Step): loss,1610345719626,0.2873382270336151
3744,-Metrics/Training(Step): loss,1610345723637,0.26865440607070923
3746,-Metrics/Training(Step): loss,1610345727025,0.2576448917388916
3748,-Metrics/Training(Step): loss,1610345729592,0.2945665419101715
3750,-Metrics/Training(Step): loss,1610345731830,0.2902774512767792
3752,-Metrics/Training(Step): loss,1610345733906,0.27688804268836975
3754,-Metrics/Training(Step): loss,1610345736106,0.27348071336746216
3756,-Metrics/Training(Step): loss,1610345738231,0.21524135768413544
3758,-Metrics/Training(Step): loss,1610345740349,0.2661496698856354
3760,-Metrics/Training(Step): loss,1610345742396,0.26206716895103455
3762,-Metrics/Training(Step): loss,1610345744429,0.23520854115486145
3764,-Metrics/Training(Step): loss,1610345746482,0.24636057019233704
3766,-Metrics/Training(Step): loss,1610345748588,0.23070937395095825
3768,-Metrics/Training(Step): loss,1610345750695,0.2632609009742737
3770,-Metrics/Training(Step): loss,1610345752816,0.2991716265678406
3772,-Metrics/Training(Step): loss,1610345754902,0.22396071255207062
3774,-Metrics/Training(Step): loss,1610345757001,0.2727452218532562
3776,-Metrics/Training(Step): loss,1610345759022,0.2843151390552521
3778,-Metrics/Training(Step): loss,1610345761002,0.24209173023700714
3780,-Metrics/Training(Step): loss,1610345763085,0.3103296160697937
3782,-Metrics/Training(Step): loss,1610345765152,0.26220187544822693
3784,-Metrics/Training(Step): loss,1610345767201,0.2402942031621933
3786,-Metrics/Training(Step): loss,1610345781127,0.2222929298877716
3788,-Metrics/Training(Step): loss,1610345785116,0.25141897797584534
3790,-Metrics/Training(Step): loss,1610345789021,0.2391625940799713
3792,-Metrics/Training(Step): loss,1610345792819,0.2143145352602005
3794,-Metrics/Training(Step): loss,1610345796923,0.19605742394924164
3796,-Metrics/Training(Step): loss,1610345800943,0.19670358300209045
3798,-Metrics/Training(Step): loss,1610345804826,0.24184858798980713
3800,-Metrics/Training(Step): loss,1610345808621,0.2971455156803131
3802,-Metrics/Training(Step): loss,1610345812220,0.2542892396450043
3804,-Metrics/Training(Step): loss,1610345815520,0.29194584488868713
3806,-Metrics/Training(Step): loss,1610345819216,0.2261093407869339
3808,-Metrics/Training(Step): loss,1610345822720,0.23774057626724243
3810,-Metrics/Training(Step): loss,1610345826324,0.22632095217704773
3812,-Metrics/Training(Step): loss,1610345829720,0.2629096210002899
3814,-Metrics/Training(Step): loss,1610345833420,0.2500212490558624
3816,-Metrics/Training(Step): loss,1610345837335,0.27633997797966003
3818,-Metrics/Training(Step): loss,1610345840889,0.2881668210029602
3820,-Metrics/Training(Step): loss,1610345844743,0.2404485046863556
3822,-Metrics/Training(Step): loss,1610345848520,0.3052036166191101
3824,-Metrics/Training(Step): loss,1610345852350,0.23993396759033203
3826,-Metrics/Training(Step): loss,1610345855651,0.36792415380477905
3828,-Metrics/Training(Step): loss,1610345859323,0.2986505329608917
3830,-Metrics/Training(Step): loss,1610345863149,0.26189035177230835
3832,-Metrics/Training(Step): loss,1610345866089,0.25580716133117676
3834,-Metrics/Training(Step): loss,1610345868333,0.2563594579696655
3836,-Metrics/Training(Step): loss,1610345870442,0.3601406216621399
3838,-Metrics/Training(Step): loss,1610345872491,0.306284099817276
3840,-Metrics/Training(Step): loss,1610345874526,0.2208624929189682
3842,-Metrics/Training(Step): loss,1610345876617,0.2369132936000824
3844,-Metrics/Training(Step): loss,1610345878713,0.28334474563598633
3846,-Metrics/Training(Step): loss,1610345880784,0.2895797789096832
3848,-Metrics/Training(Step): loss,1610345882883,0.30903083086013794
3850,-Metrics/Training(Step): loss,1610345884912,0.23949921131134033
3852,-Metrics/Training(Step): loss,1610345887006,0.2931596636772156
3854,-Metrics/Training(Step): loss,1610345889123,0.31609073281288147
3856,-Metrics/Training(Step): loss,1610345891255,0.24173301458358765
3858,-Metrics/Training(Step): loss,1610345893313,0.25354522466659546
3860,-Metrics/Training(Step): loss,1610345895397,0.3131055533885956
3862,-Metrics/Training(Step): loss,1610345897378,0.2809556722640991
3864,-Metrics/Training(Step): loss,1610345899440,0.2977626621723175
3866,-Metrics/Training(Step): loss,1610345901542,0.2246004343032837
3868,-Metrics/Training(Step): loss,1610345903602,0.28548455238342285
3870,-Metrics/Training(Step): loss,1610345905599,0.2730742394924164
3872,-Metrics/Training(Step): loss,1610345919122,0.24212810397148132
3874,-Metrics/Training(Step): loss,1610345923125,0.2918557822704315
3876,-Metrics/Training(Step): loss,1610345927022,0.2553490698337555
3878,-Metrics/Training(Step): loss,1610345930826,0.2428670972585678
3880,-Metrics/Training(Step): loss,1610345935276,0.18321718275547028
3882,-Metrics/Training(Step): loss,1610345938850,0.19618010520935059
3884,-Metrics/Training(Step): loss,1610345943020,0.24238847196102142
3886,-Metrics/Training(Step): loss,1610345946147,0.1861630380153656
3888,-Metrics/Training(Step): loss,1610345949591,0.23891641199588776
3890,-Metrics/Training(Step): loss,1610345953421,0.2746369540691376
3892,-Metrics/Training(Step): loss,1610345957304,0.31509265303611755
3894,-Metrics/Training(Step): loss,1610345960939,0.2524193823337555
3896,-Metrics/Training(Step): loss,1610345964694,0.2482919991016388
3898,-Metrics/Training(Step): loss,1610345968448,0.22761112451553345
3900,-Metrics/Training(Step): loss,1610345971308,0.21243916451931
3902,-Metrics/Training(Step): loss,1610345975020,0.2743777930736542
3904,-Metrics/Training(Step): loss,1610345978820,0.21163681149482727
3906,-Metrics/Training(Step): loss,1610345982520,0.2339271903038025
3908,-Metrics/Training(Step): loss,1610345986023,0.27764520049095154
3910,-Metrics/Training(Step): loss,1610345989420,0.2575153708457947
3912,-Metrics/Training(Step): loss,1610345992542,0.2998386025428772
3914,-Metrics/Training(Step): loss,1610345996232,0.17302361130714417
3916,-Metrics/Training(Step): loss,1610345999919,0.26214519143104553
3918,-Metrics/Training(Step): loss,1610346002918,0.2502343952655792
3920,-Metrics/Training(Step): loss,1610346005157,0.2094642072916031
3922,-Metrics/Training(Step): loss,1610346007140,0.18716152012348175
3924,-Metrics/Training(Step): loss,1610346009155,0.21040834486484528
3926,-Metrics/Training(Step): loss,1610346011251,0.24166284501552582
3928,-Metrics/Training(Step): loss,1610346013391,0.3721153438091278
3930,-Metrics/Training(Step): loss,1610346015489,0.2218073308467865
3932,-Metrics/Training(Step): loss,1610346017575,0.2449609637260437
3934,-Metrics/Training(Step): loss,1610346019713,0.2553703188896179
3936,-Metrics/Training(Step): loss,1610346021807,0.29198017716407776
3938,-Metrics/Training(Step): loss,1610346023756,0.2577843964099884
3940,-Metrics/Training(Step): loss,1610346025825,0.23912926018238068
3942,-Metrics/Training(Step): loss,1610346027802,0.26178449392318726
3944,-Metrics/Training(Step): loss,1610346029710,0.16668002307415009
3946,-Metrics/Training(Step): loss,1610346031717,0.22999398410320282
3948,-Metrics/Training(Step): loss,1610346033806,0.24928177893161774
3950,-Metrics/Training(Step): loss,1610346035850,0.30576813220977783
3952,-Metrics/Training(Step): loss,1610346037925,0.21035248041152954
3954,-Metrics/Training(Step): loss,1610346040002,0.24431666731834412
3956,-Metrics/Training(Step): loss,1610346042036,0.2854937016963959
3958,-Metrics/Training(Step): loss,1610346054327,0.17540566623210907
3960,-Metrics/Training(Step): loss,1610346058215,0.23453806340694427
3962,-Metrics/Training(Step): loss,1610346062020,0.20459800958633423
3964,-Metrics/Training(Step): loss,1610346066220,0.22313667833805084
3966,-Metrics/Training(Step): loss,1610346070016,0.3316120505332947
3968,-Metrics/Training(Step): loss,1610346074233,0.26334673166275024
3970,-Metrics/Training(Step): loss,1610346078335,0.1949026733636856
3972,-Metrics/Training(Step): loss,1610346082102,0.24317893385887146
3974,-Metrics/Training(Step): loss,1610346085685,0.24723003804683685
3976,-Metrics/Training(Step): loss,1610346089023,0.261348158121109
3978,-Metrics/Training(Step): loss,1610346092686,0.2576371133327484
3980,-Metrics/Training(Step): loss,1610346096070,0.24610349535942078
3982,-Metrics/Training(Step): loss,1610346099520,0.2375219315290451
3984,-Metrics/Training(Step): loss,1610346103120,0.19976381957530975
3986,-Metrics/Training(Step): loss,1610346106444,0.2710873484611511
3988,-Metrics/Training(Step): loss,1610346110431,0.21948347985744476
3990,-Metrics/Training(Step): loss,1610346113746,0.2704428434371948
3992,-Metrics/Training(Step): loss,1610346117830,0.2981113791465759
3994,-Metrics/Training(Step): loss,1610346121254,0.21103328466415405
3996,-Metrics/Training(Step): loss,1610346125377,0.18439152836799622
3998,-Metrics/Training(Step): loss,1610346129140,0.2264605462551117
4000,-Metrics/Training(Step): loss,1610346132720,0.17605245113372803
4002,-Metrics/Training(Step): loss,1610346136226,0.2522408962249756
4004,-Metrics/Training(Step): loss,1610346139299,0.2515387535095215
4006,-Metrics/Training(Step): loss,1610346141506,0.2293066680431366
4008,-Metrics/Training(Step): loss,1610346143504,0.22498252987861633
4010,-Metrics/Training(Step): loss,1610346145590,0.19530989229679108
4012,-Metrics/Training(Step): loss,1610346147700,0.25964343547821045
4014,-Metrics/Training(Step): loss,1610346149799,0.2790106534957886
4016,-Metrics/Training(Step): loss,1610346151827,0.2443362921476364
4018,-Metrics/Training(Step): loss,1610346153935,0.20107470452785492
4020,-Metrics/Training(Step): loss,1610346156074,0.24411015212535858
4022,-Metrics/Training(Step): loss,1610346158147,0.22346137464046478
4024,-Metrics/Training(Step): loss,1610346160164,0.24382708966732025
4026,-Metrics/Training(Step): loss,1610346162000,0.24879352748394012
4028,-Metrics/Training(Step): loss,1610346163918,0.2676556408405304
4030,-Metrics/Training(Step): loss,1610346165853,0.23069633543491364
4032,-Metrics/Training(Step): loss,1610346167905,0.23256859183311462
4034,-Metrics/Training(Step): loss,1610346169851,0.2045196294784546
4036,-Metrics/Training(Step): loss,1610346171931,0.2480917125940323
4038,-Metrics/Training(Step): loss,1610346173973,0.23498764634132385
4040,-Metrics/Training(Step): loss,1610346176034,0.21340054273605347
4042,-Metrics/Training(Step): loss,1610346178018,0.18892036378383636
4044,-Metrics/Training(Step): loss,1610346191430,0.1840999573469162
4046,-Metrics/Training(Step): loss,1610346195525,0.26069775223731995
4048,-Metrics/Training(Step): loss,1610346199620,0.2629731595516205
4050,-Metrics/Training(Step): loss,1610346203446,0.2141043096780777
4052,-Metrics/Training(Step): loss,1610346207235,0.20057246088981628
4054,-Metrics/Training(Step): loss,1610346211029,0.2700228691101074
4056,-Metrics/Training(Step): loss,1610346214952,0.2307398021221161
4058,-Metrics/Training(Step): loss,1610346218314,0.21842005848884583
4060,-Metrics/Training(Step): loss,1610346221722,0.20021387934684753
4062,-Metrics/Training(Step): loss,1610346225195,0.29183268547058105
4064,-Metrics/Training(Step): loss,1610346228854,0.16615082323551178
4066,-Metrics/Training(Step): loss,1610346232320,0.2535462975502014
4068,-Metrics/Training(Step): loss,1610346235822,0.275896281003952
4070,-Metrics/Training(Step): loss,1610346239688,0.24675866961479187
4072,-Metrics/Training(Step): loss,1610346243227,0.2550419867038727
4074,-Metrics/Training(Step): loss,1610346246634,0.26650726795196533
4076,-Metrics/Training(Step): loss,1610346250720,0.18646495044231415
4078,-Metrics/Training(Step): loss,1610346254320,0.20940682291984558
4080,-Metrics/Training(Step): loss,1610346257414,0.2591152787208557
4082,-Metrics/Training(Step): loss,1610346261144,0.244268536567688
4084,-Metrics/Training(Step): loss,1610346264141,0.18354877829551697
4086,-Metrics/Training(Step): loss,1610346267870,0.20408622920513153
4088,-Metrics/Training(Step): loss,1610346271522,0.27017998695373535
4090,-Metrics/Training(Step): loss,1610346274648,0.23437467217445374
4092,-Metrics/Training(Step): loss,1610346277101,0.19442175328731537
4094,-Metrics/Training(Step): loss,1610346279238,0.192598357796669
4096,-Metrics/Training(Step): loss,1610346281322,0.2300487756729126
4098,-Metrics/Training(Step): loss,1610346283437,0.2950291335582733
4100,-Metrics/Training(Step): loss,1610346285464,0.22152654826641083
4102,-Metrics/Training(Step): loss,1610346287583,0.28425952792167664
4104,-Metrics/Training(Step): loss,1610346289646,0.21857436001300812
4106,-Metrics/Training(Step): loss,1610346291617,0.18139511346817017
4108,-Metrics/Training(Step): loss,1610346293674,0.2355276197195053
4110,-Metrics/Training(Step): loss,1610346295789,0.20818857848644257
4112,-Metrics/Training(Step): loss,1610346297856,0.1892998069524765
4114,-Metrics/Training(Step): loss,1610346299884,0.19966652989387512
4116,-Metrics/Training(Step): loss,1610346301992,0.17508813738822937
4118,-Metrics/Training(Step): loss,1610346303992,0.19169442355632782
4120,-Metrics/Training(Step): loss,1610346305999,0.2566096782684326
4122,-Metrics/Training(Step): loss,1610346307987,0.23095183074474335
4124,-Metrics/Training(Step): loss,1610346309853,0.22483325004577637
4126,-Metrics/Training(Step): loss,1610346311906,0.2722233831882477
4128,-Metrics/Training(Step): loss,1610346313936,0.23928424715995789
4130,-Metrics/Training(Step): loss,1610346326232,0.23810550570487976
4132,-Metrics/Training(Step): loss,1610346330216,0.2285611927509308
4134,-Metrics/Training(Step): loss,1610346334015,0.2478262037038803
4136,-Metrics/Training(Step): loss,1610346337919,0.24827158451080322
4138,-Metrics/Training(Step): loss,1610346342019,0.20261454582214355
4140,-Metrics/Training(Step): loss,1610346345820,0.2285211980342865
4142,-Metrics/Training(Step): loss,1610346349637,0.2402809113264084
4144,-Metrics/Training(Step): loss,1610346353620,0.18390198051929474
4146,-Metrics/Training(Step): loss,1610346357732,0.2233942300081253
4148,-Metrics/Training(Step): loss,1610346361145,0.18082170188426971
4150,-Metrics/Training(Step): loss,1610346365243,0.23939542472362518
4152,-Metrics/Training(Step): loss,1610346368811,0.2682635486125946
4154,-Metrics/Training(Step): loss,1610346372621,0.22142542898654938
4156,-Metrics/Training(Step): loss,1610346376426,0.22005228698253632
4158,-Metrics/Training(Step): loss,1610346379720,0.30406317114830017
4160,-Metrics/Training(Step): loss,1610346382943,0.28167134523391724
4162,-Metrics/Training(Step): loss,1610346386720,0.2725709080696106
4164,-Metrics/Training(Step): loss,1610346390166,0.3142450153827667
4166,-Metrics/Training(Step): loss,1610346392983,0.39230725169181824
4168,-Metrics/Training(Step): loss,1610346396327,0.21744507551193237
4170,-Metrics/Training(Step): loss,1610346400019,0.21769852936267853
4172,-Metrics/Training(Step): loss,1610346403677,0.2955470681190491
4174,-Metrics/Training(Step): loss,1610346407343,0.24514058232307434
4176,-Metrics/Training(Step): loss,1610346410616,0.1991993635892868
4178,-Metrics/Training(Step): loss,1610346412835,0.2222505658864975
4180,-Metrics/Training(Step): loss,1610346414852,0.2805832326412201
4182,-Metrics/Training(Step): loss,1610346417062,0.4181828498840332
4184,-Metrics/Training(Step): loss,1610346419210,0.27029654383659363
4186,-Metrics/Training(Step): loss,1610346421295,0.30008402466773987
4188,-Metrics/Training(Step): loss,1610346423299,0.3474855124950409
4190,-Metrics/Training(Step): loss,1610346425393,0.2523767948150635
4192,-Metrics/Training(Step): loss,1610346427397,0.2351294308900833
4194,-Metrics/Training(Step): loss,1610346429320,0.20938195288181305
4196,-Metrics/Training(Step): loss,1610346431429,0.24362511932849884
4198,-Metrics/Training(Step): loss,1610346433090,0.24476146697998047
4200,-Metrics/Training(Step): loss,1610346435136,0.20462296903133392
4202,-Metrics/Training(Step): loss,1610346437251,0.2286754548549652
4204,-Metrics/Training(Step): loss,1610346439197,0.24152831733226776
4206,-Metrics/Training(Step): loss,1610346441241,0.20380045473575592
4208,-Metrics/Training(Step): loss,1610346443343,0.3009885251522064
4210,-Metrics/Training(Step): loss,1610346445401,0.2625834345817566
4212,-Metrics/Training(Step): loss,1610346447387,0.2820717692375183
4214,-Metrics/Training(Step): loss,1610346449421,0.2739371657371521
4216,-Metrics/Training(Step): loss,1610346461745,0.17419525980949402
4218,-Metrics/Training(Step): loss,1610346465815,0.297282338142395
4220,-Metrics/Training(Step): loss,1610346469742,0.2524561882019043
4222,-Metrics/Training(Step): loss,1610346474044,0.2551007866859436
4224,-Metrics/Training(Step): loss,1610346478130,0.2827487289905548
4226,-Metrics/Training(Step): loss,1610346482548,0.25378718972206116
4228,-Metrics/Training(Step): loss,1610346486841,0.3059084415435791
4230,-Metrics/Training(Step): loss,1610346490357,0.21982108056545258
4232,-Metrics/Training(Step): loss,1610346493716,0.2725924849510193
4234,-Metrics/Training(Step): loss,1610346497557,0.1991225779056549
4236,-Metrics/Training(Step): loss,1610346501382,0.2654629945755005
4238,-Metrics/Training(Step): loss,1610346505355,0.22803764045238495
4240,-Metrics/Training(Step): loss,1610346508811,0.23805224895477295
4242,-Metrics/Training(Step): loss,1610346512420,0.23698952794075012
4244,-Metrics/Training(Step): loss,1610346515568,0.22076837718486786
4246,-Metrics/Training(Step): loss,1610346518914,0.2816917598247528
4248,-Metrics/Training(Step): loss,1610346522185,0.22530953586101532
4250,-Metrics/Training(Step): loss,1610346526020,0.2365376204252243
4252,-Metrics/Training(Step): loss,1610346529326,0.23210586607456207
4254,-Metrics/Training(Step): loss,1610346532620,0.22257092595100403
4256,-Metrics/Training(Step): loss,1610346535935,0.23688845336437225
4258,-Metrics/Training(Step): loss,1610346539320,0.24835684895515442
4260,-Metrics/Training(Step): loss,1610346543157,0.22236567735671997
4262,-Metrics/Training(Step): loss,1610346546458,0.32958129048347473
4264,-Metrics/Training(Step): loss,1610346548597,0.1884838044643402
4266,-Metrics/Training(Step): loss,1610346550693,0.20034515857696533
4268,-Metrics/Training(Step): loss,1610346552735,0.2391863912343979
4270,-Metrics/Training(Step): loss,1610346554643,0.25688236951828003
4272,-Metrics/Training(Step): loss,1610346556727,0.2747473120689392
4274,-Metrics/Training(Step): loss,1610346558788,0.1818646937608719
4276,-Metrics/Training(Step): loss,1610346560869,0.2770107686519623
4278,-Metrics/Training(Step): loss,1610346562962,0.30665865540504456
4280,-Metrics/Training(Step): loss,1610346564848,0.30492159724235535
4282,-Metrics/Training(Step): loss,1610346566782,0.23923876881599426
4284,-Metrics/Training(Step): loss,1610346568882,0.24751709401607513
4286,-Metrics/Training(Step): loss,1610346570914,0.26825931668281555
4288,-Metrics/Training(Step): loss,1610346573004,0.2512017786502838
4290,-Metrics/Training(Step): loss,1610346575059,0.2660229802131653
4292,-Metrics/Training(Step): loss,1610346577091,0.32762113213539124
4294,-Metrics/Training(Step): loss,1610346579062,0.31920087337493896
4296,-Metrics/Training(Step): loss,1610346581084,0.2782588005065918
4298,-Metrics/Training(Step): loss,1610346583144,0.3011529743671417
4300,-Metrics/Training(Step): loss,1610346585228,0.254920095205307
4302,-Metrics/Training(Step): loss,1610346598543,0.21446505188941956
4304,-Metrics/Training(Step): loss,1610346602721,0.24473683536052704
4306,-Metrics/Training(Step): loss,1610346606742,0.23539237678050995
4308,-Metrics/Training(Step): loss,1610346610830,0.19099198281764984
4310,-Metrics/Training(Step): loss,1610346614623,0.30006301403045654
4312,-Metrics/Training(Step): loss,1610346618539,0.2777363657951355
4314,-Metrics/Training(Step): loss,1610346622419,0.2228602170944214
4316,-Metrics/Training(Step): loss,1610346626422,0.21078209578990936
4318,-Metrics/Training(Step): loss,1610346630120,0.18561556935310364
4320,-Metrics/Training(Step): loss,1610346633492,0.24202926456928253
4322,-Metrics/Training(Step): loss,1610346636931,0.31198152899742126
4324,-Metrics/Training(Step): loss,1610346640819,0.2232455611228943
4326,-Metrics/Training(Step): loss,1610346644420,0.1944117248058319
4328,-Metrics/Training(Step): loss,1610346648243,0.22205518186092377
4330,-Metrics/Training(Step): loss,1610346651655,0.2117663323879242
4332,-Metrics/Training(Step): loss,1610346655280,0.27704232931137085
4334,-Metrics/Training(Step): loss,1610346659158,0.25165045261383057
4336,-Metrics/Training(Step): loss,1610346663122,0.22853878140449524
4338,-Metrics/Training(Step): loss,1610346666515,0.2136726975440979
4340,-Metrics/Training(Step): loss,1610346670767,0.22174064815044403
4342,-Metrics/Training(Step): loss,1610346674533,0.22397351264953613
4344,-Metrics/Training(Step): loss,1610346677920,0.2155287116765976
4346,-Metrics/Training(Step): loss,1610346681066,0.2667335569858551
4348,-Metrics/Training(Step): loss,1610346683557,0.1837722361087799
4350,-Metrics/Training(Step): loss,1610346685830,0.3156851530075073
4352,-Metrics/Training(Step): loss,1610346687957,0.28636297583580017
4354,-Metrics/Training(Step): loss,1610346690065,0.20868250727653503
4356,-Metrics/Training(Step): loss,1610346692164,0.2551769018173218
4358,-Metrics/Training(Step): loss,1610346694214,0.26015421748161316
4360,-Metrics/Training(Step): loss,1610346696309,0.2574620544910431
4362,-Metrics/Training(Step): loss,1610346698387,0.22224807739257812
4364,-Metrics/Training(Step): loss,1610346700476,0.2806702256202698
4366,-Metrics/Training(Step): loss,1610346702533,0.26501426100730896
4368,-Metrics/Training(Step): loss,1610346704646,0.19304804503917694
4370,-Metrics/Training(Step): loss,1610346706700,0.28608250617980957
4372,-Metrics/Training(Step): loss,1610346708789,0.2850934565067291
4374,-Metrics/Training(Step): loss,1610346710833,0.22112633287906647
4376,-Metrics/Training(Step): loss,1610346712850,0.2798798978328705
4378,-Metrics/Training(Step): loss,1610346714752,0.29687392711639404
4380,-Metrics/Training(Step): loss,1610346716831,0.23553606867790222
4382,-Metrics/Training(Step): loss,1610346718899,0.24049851298332214
4384,-Metrics/Training(Step): loss,1610346720946,0.3089793920516968
4386,-Metrics/Training(Step): loss,1610346723008,0.26419416069984436
4388,-Metrics/Training(Step): loss,1610346735130,0.30152371525764465
4390,-Metrics/Training(Step): loss,1610346739120,0.24879640340805054
4392,-Metrics/Training(Step): loss,1610346743229,0.24622200429439545
4394,-Metrics/Training(Step): loss,1610346747226,0.24158120155334473
4396,-Metrics/Training(Step): loss,1610346751052,0.2512172758579254
4398,-Metrics/Training(Step): loss,1610346755224,0.32830554246902466
4400,-Metrics/Training(Step): loss,1610346759270,0.2670910358428955
4402,-Metrics/Training(Step): loss,1610346763320,0.2676851451396942
4404,-Metrics/Training(Step): loss,1610346767147,0.22684413194656372
4406,-Metrics/Training(Step): loss,1610346770734,0.29020076990127563
4408,-Metrics/Training(Step): loss,1610346774236,0.24057362973690033
4410,-Metrics/Training(Step): loss,1610346777820,0.25207722187042236
4412,-Metrics/Training(Step): loss,1610346781420,0.1884663999080658
4414,-Metrics/Training(Step): loss,1610346785377,0.2162632942199707
4416,-Metrics/Training(Step): loss,1610346789218,0.25320789217948914
4418,-Metrics/Training(Step): loss,1610346792697,0.21397295594215393
4420,-Metrics/Training(Step): loss,1610346796799,0.20094333589076996
4422,-Metrics/Training(Step): loss,1610346800622,0.17578358948230743
4424,-Metrics/Training(Step): loss,1610346804071,0.2895885407924652
4426,-Metrics/Training(Step): loss,1610346807544,0.17146822810173035
4428,-Metrics/Training(Step): loss,1610346810820,0.22735726833343506
4430,-Metrics/Training(Step): loss,1610346814430,0.1808176338672638
4432,-Metrics/Training(Step): loss,1610346817553,0.2362397015094757
4434,-Metrics/Training(Step): loss,1610346820392,0.2886706590652466
4436,-Metrics/Training(Step): loss,1610346822485,0.1757466048002243
4438,-Metrics/Training(Step): loss,1610346824560,0.17825469374656677
4440,-Metrics/Training(Step): loss,1610346826553,0.2322154939174652
4442,-Metrics/Training(Step): loss,1610346828662,0.20182648301124573
4444,-Metrics/Training(Step): loss,1610346830802,0.182035431265831
4446,-Metrics/Training(Step): loss,1610346832893,0.23629897832870483
4448,-Metrics/Training(Step): loss,1610346834969,0.21057197451591492
4450,-Metrics/Training(Step): loss,1610346837042,0.19995693862438202
4452,-Metrics/Training(Step): loss,1610346839101,0.22187496721744537
4454,-Metrics/Training(Step): loss,1610346841218,0.2490728795528412
4456,-Metrics/Training(Step): loss,1610346843322,0.26137274503707886
4458,-Metrics/Training(Step): loss,1610346845456,0.16136015951633453
4460,-Metrics/Training(Step): loss,1610346847548,0.20936068892478943
4462,-Metrics/Training(Step): loss,1610346849553,0.22930671274662018
4464,-Metrics/Training(Step): loss,1610346851432,0.25398942828178406
4466,-Metrics/Training(Step): loss,1610346853397,0.2406761199235916
4468,-Metrics/Training(Step): loss,1610346855428,0.2387257069349289
4470,-Metrics/Training(Step): loss,1610346857483,0.21124504506587982
4472,-Metrics/Training(Step): loss,1610346859549,0.23810844123363495
4474,-Metrics/Training(Step): loss,1610346872538,0.18613596260547638
4476,-Metrics/Training(Step): loss,1610346876528,0.2009640336036682
4478,-Metrics/Training(Step): loss,1610346880954,0.2341105043888092
4480,-Metrics/Training(Step): loss,1610346884836,0.19757668673992157
4482,-Metrics/Training(Step): loss,1610346888720,0.2055784910917282
4484,-Metrics/Training(Step): loss,1610346892620,0.2317846119403839
4486,-Metrics/Training(Step): loss,1610346896720,0.2076888084411621
4488,-Metrics/Training(Step): loss,1610346900420,0.21921780705451965
4490,-Metrics/Training(Step): loss,1610346903920,0.25877806544303894
4492,-Metrics/Training(Step): loss,1610346907211,0.2140042930841446
4494,-Metrics/Training(Step): loss,1610346910821,0.22813941538333893
4496,-Metrics/Training(Step): loss,1610346914877,0.1841530054807663
4498,-Metrics/Training(Step): loss,1610346918478,0.21745409071445465
4500,-Metrics/Training(Step): loss,1610346922052,0.24282903969287872
4502,-Metrics/Training(Step): loss,1610346925919,0.18815037608146667
4504,-Metrics/Training(Step): loss,1610346929420,0.23899275064468384
4506,-Metrics/Training(Step): loss,1610346932644,0.1775556355714798
4508,-Metrics/Training(Step): loss,1610346936220,0.20793749392032623
4510,-Metrics/Training(Step): loss,1610346939777,0.19808273017406464
4512,-Metrics/Training(Step): loss,1610346942728,0.23853647708892822
4514,-Metrics/Training(Step): loss,1610346945989,0.2626257836818695
4516,-Metrics/Training(Step): loss,1610346949720,0.1904635727405548
4518,-Metrics/Training(Step): loss,1610346953323,0.2270699292421341
4520,-Metrics/Training(Step): loss,1610346956391,0.2263006567955017
4522,-Metrics/Training(Step): loss,1610346958717,0.23317857086658478
4524,-Metrics/Training(Step): loss,1610346960856,0.21863535046577454
4526,-Metrics/Training(Step): loss,1610346963004,0.23410464823246002
4528,-Metrics/Training(Step): loss,1610346965139,0.1975250542163849
4530,-Metrics/Training(Step): loss,1610346967267,0.20889942348003387
4532,-Metrics/Training(Step): loss,1610346969385,0.2598932087421417
4534,-Metrics/Training(Step): loss,1610346971503,0.2290372997522354
4536,-Metrics/Training(Step): loss,1610346973655,0.17944280803203583
4538,-Metrics/Training(Step): loss,1610346975494,0.2163703441619873
4540,-Metrics/Training(Step): loss,1610346977585,0.200873002409935
4542,-Metrics/Training(Step): loss,1610346979661,0.28022727370262146
4544,-Metrics/Training(Step): loss,1610346981432,0.23964008688926697
4546,-Metrics/Training(Step): loss,1610346983437,0.18413402140140533
4548,-Metrics/Training(Step): loss,1610346985525,0.18888115882873535
4550,-Metrics/Training(Step): loss,1610346987467,0.1966814547777176
4552,-Metrics/Training(Step): loss,1610346989541,0.25118914246559143
4554,-Metrics/Training(Step): loss,1610346991608,0.21806737780570984
4556,-Metrics/Training(Step): loss,1610346993662,0.2675894796848297
4558,-Metrics/Training(Step): loss,1610346995696,0.25643694400787354
4560,-Metrics/Training(Step): loss,1610347007936,0.21615217626094818
4562,-Metrics/Training(Step): loss,1610347011829,0.21393772959709167
4564,-Metrics/Training(Step): loss,1610347015820,0.25644752383232117
4566,-Metrics/Training(Step): loss,1610347019620,0.2233860343694687
4568,-Metrics/Training(Step): loss,1610347023120,0.20845675468444824
4570,-Metrics/Training(Step): loss,1610347027221,0.1968313306570053
4572,-Metrics/Training(Step): loss,1610347031086,0.29125550389289856
4574,-Metrics/Training(Step): loss,1610347034921,0.21850527822971344
4576,-Metrics/Training(Step): loss,1610347038321,0.2265903353691101
4578,-Metrics/Training(Step): loss,1610347041967,0.23158013820648193
4580,-Metrics/Training(Step): loss,1610347045315,0.29816022515296936
4582,-Metrics/Training(Step): loss,1610347048718,0.21374456584453583
4584,-Metrics/Training(Step): loss,1610347052221,0.1811414211988449
4586,-Metrics/Training(Step): loss,1610347055626,0.2166876345872879
4588,-Metrics/Training(Step): loss,1610347059120,0.16545113921165466
4590,-Metrics/Training(Step): loss,1610347062421,0.22962895035743713
4592,-Metrics/Training(Step): loss,1610347066220,0.23302583396434784
4594,-Metrics/Training(Step): loss,1610347069726,0.22902192175388336
4596,-Metrics/Training(Step): loss,1610347073981,0.22072184085845947
4598,-Metrics/Training(Step): loss,1610347077421,0.18347206711769104
4600,-Metrics/Training(Step): loss,1610347081398,0.21939411759376526
4602,-Metrics/Training(Step): loss,1610347084665,0.2161596268415451
4604,-Metrics/Training(Step): loss,1610347088066,0.22684422135353088
4606,-Metrics/Training(Step): loss,1610347091090,0.23043721914291382
4608,-Metrics/Training(Step): loss,1610347093557,0.24517762660980225
4610,-Metrics/Training(Step): loss,1610347095809,0.1910780519247055
4612,-Metrics/Training(Step): loss,1610347098047,0.25700926780700684
4614,-Metrics/Training(Step): loss,1610347100149,0.2202729880809784
4616,-Metrics/Training(Step): loss,1610347102245,0.21784482896327972
4618,-Metrics/Training(Step): loss,1610347104304,0.15984751284122467
4620,-Metrics/Training(Step): loss,1610347106391,0.23402486741542816
4622,-Metrics/Training(Step): loss,1610347108509,0.2025166153907776
4624,-Metrics/Training(Step): loss,1610347110620,0.2793540060520172
4626,-Metrics/Training(Step): loss,1610347112709,0.26734790205955505
4628,-Metrics/Training(Step): loss,1610347114604,0.2638740539550781
4630,-Metrics/Training(Step): loss,1610347116650,0.2698614299297333
4632,-Metrics/Training(Step): loss,1610347118735,0.3428290784358978
4634,-Metrics/Training(Step): loss,1610347120661,0.19856083393096924
4636,-Metrics/Training(Step): loss,1610347122684,0.2451200634241104
4638,-Metrics/Training(Step): loss,1610347124690,0.22629408538341522
4640,-Metrics/Training(Step): loss,1610347126755,0.20138078927993774
4642,-Metrics/Training(Step): loss,1610347128764,0.23513253033161163
4644,-Metrics/Training(Step): loss,1610347130803,0.18384434282779694
4646,-Metrics/Training(Step): loss,1610347144038,0.27626627683639526
4648,-Metrics/Training(Step): loss,1610347148121,0.210759699344635
4650,-Metrics/Training(Step): loss,1610347152025,0.2198585420846939
4652,-Metrics/Training(Step): loss,1610347155919,0.411354660987854
4654,-Metrics/Training(Step): loss,1610347160020,0.2215006947517395
4656,-Metrics/Training(Step): loss,1610347164019,0.17346777021884918
4658,-Metrics/Training(Step): loss,1610347167716,0.2394944131374359
4660,-Metrics/Training(Step): loss,1610347171615,0.23974385857582092
4662,-Metrics/Training(Step): loss,1610347175117,0.20255163311958313
4664,-Metrics/Training(Step): loss,1610347178324,0.27182304859161377
4666,-Metrics/Training(Step): loss,1610347181815,0.25618481636047363
4668,-Metrics/Training(Step): loss,1610347184921,0.16945847868919373
4670,-Metrics/Training(Step): loss,1610347188320,0.25869882106781006
4672,-Metrics/Training(Step): loss,1610347191920,0.27337485551834106
4674,-Metrics/Training(Step): loss,1610347195221,0.22077035903930664
4676,-Metrics/Training(Step): loss,1610347198521,0.239174947142601
4678,-Metrics/Training(Step): loss,1610347202238,0.2348635494709015
4680,-Metrics/Training(Step): loss,1610347205675,0.24372601509094238
4682,-Metrics/Training(Step): loss,1610347209424,0.21713148057460785
4684,-Metrics/Training(Step): loss,1610347212899,0.20574818551540375
4686,-Metrics/Training(Step): loss,1610347216420,0.21902276575565338
4688,-Metrics/Training(Step): loss,1610347219920,0.24587446451187134
4690,-Metrics/Training(Step): loss,1610347222820,0.21717235445976257
4692,-Metrics/Training(Step): loss,1610347226317,0.17288030683994293
4694,-Metrics/Training(Step): loss,1610347229522,0.25057706236839294
4696,-Metrics/Training(Step): loss,1610347231714,0.1956711709499359
4698,-Metrics/Training(Step): loss,1610347233852,0.21105758845806122
4700,-Metrics/Training(Step): loss,1610347235798,0.22273166477680206
4702,-Metrics/Training(Step): loss,1610347237883,0.18532678484916687
4704,-Metrics/Training(Step): loss,1610347240026,0.24119217693805695
4706,-Metrics/Training(Step): loss,1610347241975,0.24502085149288177
4708,-Metrics/Training(Step): loss,1610347243997,0.2397938221693039
4710,-Metrics/Training(Step): loss,1610347246124,0.1956203728914261
4712,-Metrics/Training(Step): loss,1610347248243,0.1845984309911728
4714,-Metrics/Training(Step): loss,1610347250241,0.21033665537834167
4716,-Metrics/Training(Step): loss,1610347252181,0.2224387675523758
4718,-Metrics/Training(Step): loss,1610347254223,0.18522225320339203
4720,-Metrics/Training(Step): loss,1610347256301,0.18500134348869324
4722,-Metrics/Training(Step): loss,1610347258250,0.2200969010591507
4724,-Metrics/Training(Step): loss,1610347260224,0.18465937674045563
4726,-Metrics/Training(Step): loss,1610347262208,0.17003367841243744
4728,-Metrics/Training(Step): loss,1610347264191,0.22792211174964905
4730,-Metrics/Training(Step): loss,1610347266173,0.23924224078655243
4732,-Metrics/Training(Step): loss,1610347278629,0.3414463400840759
4734,-Metrics/Training(Step): loss,1610347282419,0.20502594113349915
4736,-Metrics/Training(Step): loss,1610347286415,0.2186194360256195
4738,-Metrics/Training(Step): loss,1610347290537,0.19590595364570618
4740,-Metrics/Training(Step): loss,1610347294417,0.2013791799545288
4742,-Metrics/Training(Step): loss,1610347298134,0.20533603429794312
4744,-Metrics/Training(Step): loss,1610347302220,0.2548025846481323
4746,-Metrics/Training(Step): loss,1610347305821,0.2380010485649109
4748,-Metrics/Training(Step): loss,1610347309624,0.23411913216114044
4750,-Metrics/Training(Step): loss,1610347312621,0.2022605836391449
4752,-Metrics/Training(Step): loss,1610347316420,0.20217426121234894
4754,-Metrics/Training(Step): loss,1610347320434,0.2278916984796524
4756,-Metrics/Training(Step): loss,1610347324221,0.1832103282213211
4758,-Metrics/Training(Step): loss,1610347327819,0.16674676537513733
4760,-Metrics/Training(Step): loss,1610347332220,0.18740545213222504
4762,-Metrics/Training(Step): loss,1610347335916,0.26192328333854675
4764,-Metrics/Training(Step): loss,1610347339606,0.21064598858356476
4766,-Metrics/Training(Step): loss,1610347343066,0.2523139417171478
4768,-Metrics/Training(Step): loss,1610347346121,0.203805610537529
4770,-Metrics/Training(Step): loss,1610347349916,0.20071573555469513
4772,-Metrics/Training(Step): loss,1610347353327,0.23102957010269165
4774,-Metrics/Training(Step): loss,1610347357020,0.1923193782567978
4776,-Metrics/Training(Step): loss,1610347360019,0.20670509338378906
4778,-Metrics/Training(Step): loss,1610347362823,0.18959099054336548
4780,-Metrics/Training(Step): loss,1610347365246,0.16800831258296967
4782,-Metrics/Training(Step): loss,1610347367359,0.17570556700229645
4784,-Metrics/Training(Step): loss,1610347369338,0.16210149228572845
4786,-Metrics/Training(Step): loss,1610347371434,0.24785752594470978
4788,-Metrics/Training(Step): loss,1610347373585,0.2182985246181488
4790,-Metrics/Training(Step): loss,1610347375676,0.17608760297298431
4792,-Metrics/Training(Step): loss,1610347377785,0.1325954645872116
4794,-Metrics/Training(Step): loss,1610347379909,0.16714462637901306
4796,-Metrics/Training(Step): loss,1610347382025,0.19686996936798096
4798,-Metrics/Training(Step): loss,1610347384077,0.2158077210187912
4800,-Metrics/Training(Step): loss,1610347386173,0.18384793400764465
4802,-Metrics/Training(Step): loss,1610347388218,0.16945621371269226
4804,-Metrics/Training(Step): loss,1610347390134,0.20580124855041504
4806,-Metrics/Training(Step): loss,1610347392252,0.26764121651649475
4808,-Metrics/Training(Step): loss,1610347394325,0.21236589550971985
4810,-Metrics/Training(Step): loss,1610347396449,0.19535261392593384
4812,-Metrics/Training(Step): loss,1610347398426,0.1971488744020462
4814,-Metrics/Training(Step): loss,1610347400535,0.21678335964679718
4816,-Metrics/Training(Step): loss,1610347402585,0.1700475811958313
4818,-Metrics/Training(Step): loss,1610347416025,0.16654126346111298
4820,-Metrics/Training(Step): loss,1610347420320,0.217110738158226
4822,-Metrics/Training(Step): loss,1610347424320,0.1609642207622528
4824,-Metrics/Training(Step): loss,1610347428125,0.1848587691783905
4826,-Metrics/Training(Step): loss,1610347432149,0.13706174492835999
4828,-Metrics/Training(Step): loss,1610347436238,0.1890573501586914
4830,-Metrics/Training(Step): loss,1610347439843,0.19361881911754608
4832,-Metrics/Training(Step): loss,1610347443304,0.21846149861812592
4834,-Metrics/Training(Step): loss,1610347446821,0.20630967617034912
4836,-Metrics/Training(Step): loss,1610347450619,0.19292351603507996
4838,-Metrics/Training(Step): loss,1610347453916,0.18241623044013977
4840,-Metrics/Training(Step): loss,1610347458020,0.2781573534011841
4842,-Metrics/Training(Step): loss,1610347461544,0.18809880316257477
4844,-Metrics/Training(Step): loss,1610347465720,0.2087482511997223
4846,-Metrics/Training(Step): loss,1610347469551,0.22768931090831757
4848,-Metrics/Training(Step): loss,1610347473219,0.1759646087884903
4850,-Metrics/Training(Step): loss,1610347476344,0.18587365746498108
4852,-Metrics/Training(Step): loss,1610347479943,0.2331210970878601
4854,-Metrics/Training(Step): loss,1610347483425,0.2171485275030136
4856,-Metrics/Training(Step): loss,1610347487124,0.18000710010528564
4858,-Metrics/Training(Step): loss,1610347490658,0.24613364040851593
4860,-Metrics/Training(Step): loss,1610347493520,0.208427295088768
4862,-Metrics/Training(Step): loss,1610347497035,0.19030596315860748
4864,-Metrics/Training(Step): loss,1610347499528,0.22416166961193085
4866,-Metrics/Training(Step): loss,1610347501889,0.15733616054058075
4868,-Metrics/Training(Step): loss,1610347504035,0.2339363396167755
4870,-Metrics/Training(Step): loss,1610347505945,0.23757553100585938
4872,-Metrics/Training(Step): loss,1610347508043,0.19914795458316803
4874,-Metrics/Training(Step): loss,1610347510159,0.20146578550338745
4876,-Metrics/Training(Step): loss,1610347512239,0.22183696925640106
4878,-Metrics/Training(Step): loss,1610347514323,0.19026146829128265
4880,-Metrics/Training(Step): loss,1610347516318,0.17860065400600433
4882,-Metrics/Training(Step): loss,1610347518372,0.16726526618003845
4884,-Metrics/Training(Step): loss,1610347520367,0.22047626972198486
4886,-Metrics/Training(Step): loss,1610347522302,0.19280873239040375
4888,-Metrics/Training(Step): loss,1610347524390,0.2341809719800949
4890,-Metrics/Training(Step): loss,1610347526354,0.18514443933963776
4892,-Metrics/Training(Step): loss,1610347528300,0.20243073999881744
4894,-Metrics/Training(Step): loss,1610347530215,0.20393455028533936
4896,-Metrics/Training(Step): loss,1610347531827,0.20903639495372772
4898,-Metrics/Training(Step): loss,1610347533851,0.2359858900308609
4900,-Metrics/Training(Step): loss,1610347535900,0.22393395006656647
4902,-Metrics/Training(Step): loss,1610347537942,0.22109171748161316
4904,-Metrics/Training(Step): loss,1610347551036,0.18925854563713074
4906,-Metrics/Training(Step): loss,1610347555334,0.17530371248722076
4908,-Metrics/Training(Step): loss,1610347559851,0.20003178715705872
4910,-Metrics/Training(Step): loss,1610347563825,0.26306089758872986
4912,-Metrics/Training(Step): loss,1610347567519,0.17386408150196075
4914,-Metrics/Training(Step): loss,1610347571355,0.1650959700345993
4916,-Metrics/Training(Step): loss,1610347575021,0.18813149631023407
4918,-Metrics/Training(Step): loss,1610347578722,0.3102429211139679
4920,-Metrics/Training(Step): loss,1610347582721,0.19395335018634796
4922,-Metrics/Training(Step): loss,1610347586792,0.22240282595157623
4924,-Metrics/Training(Step): loss,1610347590340,0.17340540885925293
4926,-Metrics/Training(Step): loss,1610347593919,0.21906571090221405
4928,-Metrics/Training(Step): loss,1610347596910,0.1618281900882721
4930,-Metrics/Training(Step): loss,1610347600844,0.23962579667568207
4932,-Metrics/Training(Step): loss,1610347604039,0.21073363721370697
4934,-Metrics/Training(Step): loss,1610347607467,0.23261967301368713
4936,-Metrics/Training(Step): loss,1610347610556,0.23451830446720123
4938,-Metrics/Training(Step): loss,1610347614020,0.18360932171344757
4940,-Metrics/Training(Step): loss,1610347617058,0.18154391646385193
4942,-Metrics/Training(Step): loss,1610347621133,0.21119502186775208
4944,-Metrics/Training(Step): loss,1610347624769,0.16550524532794952
4946,-Metrics/Training(Step): loss,1610347628320,0.20364327728748322
4948,-Metrics/Training(Step): loss,1610347631640,0.2040669322013855
4950,-Metrics/Training(Step): loss,1610347634058,0.21804091334342957
4952,-Metrics/Training(Step): loss,1610347636174,0.20114122331142426
4954,-Metrics/Training(Step): loss,1610347638170,0.16403652727603912
4956,-Metrics/Training(Step): loss,1610347640219,0.19069668650627136
4958,-Metrics/Training(Step): loss,1610347642225,0.18113066256046295
4960,-Metrics/Training(Step): loss,1610347644314,0.18403173983097076
4962,-Metrics/Training(Step): loss,1610347646416,0.2258182317018509
4964,-Metrics/Training(Step): loss,1610347648547,0.23200716078281403
4966,-Metrics/Training(Step): loss,1610347650628,0.1836548000574112
4968,-Metrics/Training(Step): loss,1610347652686,0.20860424637794495
4970,-Metrics/Training(Step): loss,1610347654636,0.19115084409713745
4972,-Metrics/Training(Step): loss,1610347656731,0.16775257885456085
4974,-Metrics/Training(Step): loss,1610347658793,0.1885940581560135
4976,-Metrics/Training(Step): loss,1610347660841,0.164101704955101
4978,-Metrics/Training(Step): loss,1610347662781,0.1658104509115219
4980,-Metrics/Training(Step): loss,1610347664672,0.199114128947258
4982,-Metrics/Training(Step): loss,1610347666733,0.20019495487213135
4984,-Metrics/Training(Step): loss,1610347668635,0.20427092909812927
4986,-Metrics/Training(Step): loss,1610347670678,0.21918979287147522
4988,-Metrics/Training(Step): loss,1610347672679,0.24994654953479767
4990,-Metrics/Training(Step): loss,1610347685734,0.15458542108535767
4992,-Metrics/Training(Step): loss,1610347689815,0.20223534107208252
4994,-Metrics/Training(Step): loss,1610347693817,0.20952247083187103
4996,-Metrics/Training(Step): loss,1610347697950,0.17989866435527802
4998,-Metrics/Training(Step): loss,1610347701917,0.2514389157295227
5000,-Metrics/Training(Step): loss,1610347706141,0.21383759379386902
5002,-Metrics/Training(Step): loss,1610347710242,0.20718729496002197
5004,-Metrics/Training(Step): loss,1610347713721,0.2717744708061218
5006,-Metrics/Training(Step): loss,1610347717820,0.18964028358459473
5008,-Metrics/Training(Step): loss,1610347722022,0.2240956872701645
5010,-Metrics/Training(Step): loss,1610347725120,0.19828280806541443
5012,-Metrics/Training(Step): loss,1610347728420,0.22615964710712433
5014,-Metrics/Training(Step): loss,1610347731920,0.18042564392089844
5016,-Metrics/Training(Step): loss,1610347735577,0.22552596032619476
5018,-Metrics/Training(Step): loss,1610347739656,0.16344930231571198
5020,-Metrics/Training(Step): loss,1610347743135,0.16849276423454285
5022,-Metrics/Training(Step): loss,1610347746620,0.2022918462753296
5024,-Metrics/Training(Step): loss,1610347750020,0.22376511991024017
5026,-Metrics/Training(Step): loss,1610347753667,0.2370709627866745
5028,-Metrics/Training(Step): loss,1610347757420,0.18039241433143616
5030,-Metrics/Training(Step): loss,1610347760424,0.18682456016540527
5032,-Metrics/Training(Step): loss,1610347763619,0.16129305958747864
5034,-Metrics/Training(Step): loss,1610347766522,0.15966810286045074
5036,-Metrics/Training(Step): loss,1610347768927,0.189634770154953
5038,-Metrics/Training(Step): loss,1610347771201,0.18854032456874847
5040,-Metrics/Training(Step): loss,1610347773216,0.19229790568351746
5042,-Metrics/Training(Step): loss,1610347775270,0.21878023445606232
5044,-Metrics/Training(Step): loss,1610347777235,0.20852291584014893
5046,-Metrics/Training(Step): loss,1610347779341,0.20990023016929626
5048,-Metrics/Training(Step): loss,1610347781396,0.18018792569637299
5050,-Metrics/Training(Step): loss,1610347783417,0.2078647017478943
5052,-Metrics/Training(Step): loss,1610347785514,0.14333012700080872
5054,-Metrics/Training(Step): loss,1610347787590,0.15152209997177124
5056,-Metrics/Training(Step): loss,1610347789646,0.21387672424316406
5058,-Metrics/Training(Step): loss,1610347791746,0.14221885800361633
5060,-Metrics/Training(Step): loss,1610347793636,0.2215142399072647
5062,-Metrics/Training(Step): loss,1610347795704,0.22003988921642303
5064,-Metrics/Training(Step): loss,1610347797586,0.15368930995464325
5066,-Metrics/Training(Step): loss,1610347799602,0.2250840663909912
5068,-Metrics/Training(Step): loss,1610347801581,0.14641624689102173
5070,-Metrics/Training(Step): loss,1610347803638,0.18857331573963165
5072,-Metrics/Training(Step): loss,1610347805666,0.20057271420955658
5074,-Metrics/Training(Step): loss,1610347807731,0.21343794465065002
5076,-Metrics/Training(Step): loss,1610347821640,0.18311499059200287
5078,-Metrics/Training(Step): loss,1610347825419,0.17410369217395782
5080,-Metrics/Training(Step): loss,1610347829549,0.17713741958141327
5082,-Metrics/Training(Step): loss,1610347833638,0.19708982110023499
5084,-Metrics/Training(Step): loss,1610347837520,0.17513810098171234
5086,-Metrics/Training(Step): loss,1610347841319,0.20056948065757751
5088,-Metrics/Training(Step): loss,1610347844968,0.17947931587696075
5090,-Metrics/Training(Step): loss,1610347848119,0.18585969507694244
5092,-Metrics/Training(Step): loss,1610347851221,0.19767770171165466
5094,-Metrics/Training(Step): loss,1610347854652,0.24937556684017181
5096,-Metrics/Training(Step): loss,1610347858447,0.21810968220233917
5098,-Metrics/Training(Step): loss,1610347862520,0.28951242566108704
5100,-Metrics/Training(Step): loss,1610347865625,0.20554056763648987
5102,-Metrics/Training(Step): loss,1610347868541,0.1912946254014969
5104,-Metrics/Training(Step): loss,1610347872122,0.17877694964408875
5106,-Metrics/Training(Step): loss,1610347875819,0.1738561987876892
5108,-Metrics/Training(Step): loss,1610347879332,0.23142914474010468
5110,-Metrics/Training(Step): loss,1610347883045,0.18879885971546173
5112,-Metrics/Training(Step): loss,1610347886346,0.1373257339000702
5114,-Metrics/Training(Step): loss,1610347890690,0.17979349195957184
5116,-Metrics/Training(Step): loss,1610347893941,0.18463829159736633
5118,-Metrics/Training(Step): loss,1610347897298,0.2715427279472351
5120,-Metrics/Training(Step): loss,1610347901234,0.23244298994541168
5122,-Metrics/Training(Step): loss,1610347904297,0.22102829813957214
5124,-Metrics/Training(Step): loss,1610347907261,0.21803699433803558
5126,-Metrics/Training(Step): loss,1610347909347,0.2339162677526474
5128,-Metrics/Training(Step): loss,1610347911406,0.16955724358558655
5130,-Metrics/Training(Step): loss,1610347913456,0.21057471632957458
5132,-Metrics/Training(Step): loss,1610347915428,0.15577282011508942
5134,-Metrics/Training(Step): loss,1610347917343,0.16537030041217804
5136,-Metrics/Training(Step): loss,1610347919422,0.1826624721288681
5138,-Metrics/Training(Step): loss,1610347921553,0.19096097350120544
5140,-Metrics/Training(Step): loss,1610347923649,0.16828936338424683
5142,-Metrics/Training(Step): loss,1610347925567,0.2022743821144104
5144,-Metrics/Training(Step): loss,1610347927603,0.21316206455230713
5146,-Metrics/Training(Step): loss,1610347929554,0.17479214072227478
5148,-Metrics/Training(Step): loss,1610347931464,0.17660431563854218
5150,-Metrics/Training(Step): loss,1610347933365,0.2152426838874817
5152,-Metrics/Training(Step): loss,1610347935402,0.20931339263916016
5154,-Metrics/Training(Step): loss,1610347937467,0.18903110921382904
5156,-Metrics/Training(Step): loss,1610347939514,0.18100617825984955
5158,-Metrics/Training(Step): loss,1610347941480,0.14555241167545319
5160,-Metrics/Training(Step): loss,1610347943421,0.20445884764194489
5162,-Metrics/Training(Step): loss,1610347968035,0.24204447865486145
5164,-Metrics/Training(Step): loss,1610347972519,0.18545681238174438
5166,-Metrics/Training(Step): loss,1610347976920,0.21304957568645477
5168,-Metrics/Training(Step): loss,1610347980520,0.18269185721874237
5170,-Metrics/Training(Step): loss,1610347984729,0.1599425971508026
5172,-Metrics/Training(Step): loss,1610347988819,0.16633327305316925
5174,-Metrics/Training(Step): loss,1610347992621,0.25240567326545715
5176,-Metrics/Training(Step): loss,1610347996776,0.19091741740703583
5178,-Metrics/Training(Step): loss,1610347999919,0.2248409539461136
5180,-Metrics/Training(Step): loss,1610348003535,0.1511630266904831
5182,-Metrics/Training(Step): loss,1610348008020,0.15956853330135345
5184,-Metrics/Training(Step): loss,1610348011919,0.1827828288078308
5186,-Metrics/Training(Step): loss,1610348015219,0.2036624252796173
5188,-Metrics/Training(Step): loss,1610348018719,0.1615513414144516
5190,-Metrics/Training(Step): loss,1610348022098,0.18401634693145752
5192,-Metrics/Training(Step): loss,1610348025120,0.19334416091442108
5194,-Metrics/Training(Step): loss,1610348028222,0.2243214100599289
5196,-Metrics/Training(Step): loss,1610348032061,0.2607782781124115
5198,-Metrics/Training(Step): loss,1610348035441,0.20239439606666565
5200,-Metrics/Training(Step): loss,1610348039020,0.15037083625793457
5202,-Metrics/Training(Step): loss,1610348042723,0.19248318672180176
5204,-Metrics/Training(Step): loss,1610348046606,0.21615366637706757
5206,-Metrics/Training(Step): loss,1610348049723,0.22371959686279297
5208,-Metrics/Training(Step): loss,1610348052668,0.21888576447963715
5210,-Metrics/Training(Step): loss,1610348054871,0.19808383285999298
5212,-Metrics/Training(Step): loss,1610348056947,0.2047485113143921
5214,-Metrics/Training(Step): loss,1610348058935,0.21164637804031372
5216,-Metrics/Training(Step): loss,1610348060994,0.1998116374015808
5218,-Metrics/Training(Step): loss,1610348063079,0.2384737879037857
5220,-Metrics/Training(Step): loss,1610348064991,0.20179295539855957
5222,-Metrics/Training(Step): loss,1610348067089,0.18085046112537384
5224,-Metrics/Training(Step): loss,1610348069041,0.19308234751224518
5226,-Metrics/Training(Step): loss,1610348071077,0.2037273794412613
5228,-Metrics/Training(Step): loss,1610348073092,0.2182362824678421
5230,-Metrics/Training(Step): loss,1610348075173,0.1749195009469986
5232,-Metrics/Training(Step): loss,1610348077098,0.15239018201828003
5234,-Metrics/Training(Step): loss,1610348079194,0.23023276031017303
5236,-Metrics/Training(Step): loss,1610348081218,0.1998455971479416
5238,-Metrics/Training(Step): loss,1610348083287,0.2295610010623932
5240,-Metrics/Training(Step): loss,1610348085327,0.17713427543640137
5242,-Metrics/Training(Step): loss,1610348087368,0.2323484569787979
5244,-Metrics/Training(Step): loss,1610348089432,0.19845493137836456
5246,-Metrics/Training(Step): loss,1610348091415,0.1788913607597351
5248,-Metrics/Training(Step): loss,1610348103631,0.13509078323841095
5250,-Metrics/Training(Step): loss,1610348107515,0.17116142809391022
5252,-Metrics/Training(Step): loss,1610348111445,0.20724380016326904
5254,-Metrics/Training(Step): loss,1610348115429,0.1411329060792923
5256,-Metrics/Training(Step): loss,1610348119453,0.15204750001430511
5258,-Metrics/Training(Step): loss,1610348123619,0.14541779458522797
5260,-Metrics/Training(Step): loss,1610348127646,0.20189596712589264
5262,-Metrics/Training(Step): loss,1610348131838,0.17704911530017853
5264,-Metrics/Training(Step): loss,1610348135621,0.17693068087100983
5266,-Metrics/Training(Step): loss,1610348139519,0.20803816616535187
5268,-Metrics/Training(Step): loss,1610348143464,0.2361934781074524
5270,-Metrics/Training(Step): loss,1610348146996,0.1894327700138092
5272,-Metrics/Training(Step): loss,1610348150806,0.21365107595920563
5274,-Metrics/Training(Step): loss,1610348154344,0.20174770057201385
5276,-Metrics/Training(Step): loss,1610348157342,0.27116668224334717
5278,-Metrics/Training(Step): loss,1610348161122,0.22142481803894043
5280,-Metrics/Training(Step): loss,1610348165159,0.23677761852741241
5282,-Metrics/Training(Step): loss,1610348168499,0.20422732830047607
5284,-Metrics/Training(Step): loss,1610348171786,0.2621001899242401
5286,-Metrics/Training(Step): loss,1610348175307,0.19135968387126923
5288,-Metrics/Training(Step): loss,1610348179220,0.2022930085659027
5290,-Metrics/Training(Step): loss,1610348182443,0.1756322681903839
5292,-Metrics/Training(Step): loss,1610348185620,0.21481238305568695
5294,-Metrics/Training(Step): loss,1610348189135,0.22903972864151
5296,-Metrics/Training(Step): loss,1610348191530,0.17127564549446106
5298,-Metrics/Training(Step): loss,1610348193636,0.1943516582250595
5300,-Metrics/Training(Step): loss,1610348195703,0.2600126564502716
5302,-Metrics/Training(Step): loss,1610348197810,0.1659417748451233
5304,-Metrics/Training(Step): loss,1610348199849,0.15597853064537048
5306,-Metrics/Training(Step): loss,1610348201971,0.16917675733566284
5308,-Metrics/Training(Step): loss,1610348204109,0.187188982963562
5310,-Metrics/Training(Step): loss,1610348206221,0.19559283554553986
5312,-Metrics/Training(Step): loss,1610348208321,0.24547453224658966
5314,-Metrics/Training(Step): loss,1610348210428,0.22980383038520813
5316,-Metrics/Training(Step): loss,1610348212379,0.17423580586910248
5318,-Metrics/Training(Step): loss,1610348214415,0.25876203179359436
5320,-Metrics/Training(Step): loss,1610348216552,0.18339413404464722
5322,-Metrics/Training(Step): loss,1610348218651,0.22155562043190002
5324,-Metrics/Training(Step): loss,1610348220694,0.20147986710071564
5326,-Metrics/Training(Step): loss,1610348222633,0.16085679829120636
5328,-Metrics/Training(Step): loss,1610348224719,0.20976224541664124
5330,-Metrics/Training(Step): loss,1610348226779,0.20852477848529816
5332,-Metrics/Training(Step): loss,1610348228836,0.25179487466812134
5334,-Metrics/Training(Step): loss,1610348240539,0.228036031126976
5336,-Metrics/Training(Step): loss,1610348244619,0.1678142100572586
5338,-Metrics/Training(Step): loss,1610348248621,0.21811164915561676
5340,-Metrics/Training(Step): loss,1610348252620,0.23729926347732544
5342,-Metrics/Training(Step): loss,1610348256320,0.21759513020515442
5344,-Metrics/Training(Step): loss,1610348260223,0.16127575933933258
5346,-Metrics/Training(Step): loss,1610348264228,0.22127877175807953
5348,-Metrics/Training(Step): loss,1610348268720,0.18810775876045227
5350,-Metrics/Training(Step): loss,1610348272819,0.20380523800849915
5352,-Metrics/Training(Step): loss,1610348276265,0.15738879144191742
5354,-Metrics/Training(Step): loss,1610348280414,0.2069527506828308
5356,-Metrics/Training(Step): loss,1610348284219,0.20278827846050262
5358,-Metrics/Training(Step): loss,1610348287519,0.16398298740386963
5360,-Metrics/Training(Step): loss,1610348290722,0.2003779262304306
5362,-Metrics/Training(Step): loss,1610348294116,0.20182296633720398
5364,-Metrics/Training(Step): loss,1610348297560,0.198272705078125
5366,-Metrics/Training(Step): loss,1610348301319,0.1754760593175888
5368,-Metrics/Training(Step): loss,1610348305147,0.18733428418636322
5370,-Metrics/Training(Step): loss,1610348308404,0.1686030477285385
5372,-Metrics/Training(Step): loss,1610348311420,0.18427123129367828
5374,-Metrics/Training(Step): loss,1610348314219,0.1568465381860733
5376,-Metrics/Training(Step): loss,1610348317892,0.2175590544939041
5378,-Metrics/Training(Step): loss,1610348321120,0.18868118524551392
5380,-Metrics/Training(Step): loss,1610348324316,0.20383594930171967
5382,-Metrics/Training(Step): loss,1610348326930,0.225799098610878
5384,-Metrics/Training(Step): loss,1610348329059,0.25185340642929077
5386,-Metrics/Training(Step): loss,1610348331116,0.21037410199642181
5388,-Metrics/Training(Step): loss,1610348333237,0.21674397587776184
5390,-Metrics/Training(Step): loss,1610348335277,0.21479877829551697
5392,-Metrics/Training(Step): loss,1610348337322,0.19200816750526428
5394,-Metrics/Training(Step): loss,1610348339442,0.18020948767662048
5396,-Metrics/Training(Step): loss,1610348341498,0.23227176070213318
5398,-Metrics/Training(Step): loss,1610348343552,0.1565377414226532
5400,-Metrics/Training(Step): loss,1610348345589,0.3065446615219116
5402,-Metrics/Training(Step): loss,1610348347503,0.20757032930850983
5404,-Metrics/Training(Step): loss,1610348349482,0.23591701686382294
5406,-Metrics/Training(Step): loss,1610348351379,0.21547023952007294
5408,-Metrics/Training(Step): loss,1610348353348,0.19496063888072968
5410,-Metrics/Training(Step): loss,1610348355416,0.23909099400043488
5412,-Metrics/Training(Step): loss,1610348357443,0.2396305948495865
5414,-Metrics/Training(Step): loss,1610348359509,0.18380440771579742
5416,-Metrics/Training(Step): loss,1610348361527,0.24684883654117584
5418,-Metrics/Training(Step): loss,1610348363568,0.2221018224954605
5420,-Metrics/Training(Step): loss,1610348376237,0.331646203994751
5422,-Metrics/Training(Step): loss,1610348380322,0.23531660437583923
5424,-Metrics/Training(Step): loss,1610348384158,0.22782781720161438
5426,-Metrics/Training(Step): loss,1610348388226,0.279641330242157
5428,-Metrics/Training(Step): loss,1610348392026,0.22350949048995972
5430,-Metrics/Training(Step): loss,1610348396327,0.21131493151187897
5432,-Metrics/Training(Step): loss,1610348400618,0.26007384061813354
5434,-Metrics/Training(Step): loss,1610348404647,0.27231478691101074
5436,-Metrics/Training(Step): loss,1610348408926,0.17482337355613708
5438,-Metrics/Training(Step): loss,1610348412321,0.3089803159236908
5440,-Metrics/Training(Step): loss,1610348416020,0.20790167152881622
5442,-Metrics/Training(Step): loss,1610348419420,0.23539939522743225
5444,-Metrics/Training(Step): loss,1610348422743,0.21281938254833221
5446,-Metrics/Training(Step): loss,1610348426019,0.2534639239311218
5448,-Metrics/Training(Step): loss,1610348429774,0.24899020791053772
5450,-Metrics/Training(Step): loss,1610348433216,0.22211414575576782
5452,-Metrics/Training(Step): loss,1610348436242,0.2706740200519562
5454,-Metrics/Training(Step): loss,1610348440003,0.22606414556503296
5456,-Metrics/Training(Step): loss,1610348443720,0.18757402896881104
5458,-Metrics/Training(Step): loss,1610348447463,0.2259654849767685
5460,-Metrics/Training(Step): loss,1610348450885,0.21889473497867584
5462,-Metrics/Training(Step): loss,1610348454637,0.24874189496040344
5464,-Metrics/Training(Step): loss,1610348457819,0.2532188594341278
5466,-Metrics/Training(Step): loss,1610348460576,0.308757483959198
5468,-Metrics/Training(Step): loss,1610348462942,0.20775549113750458
5470,-Metrics/Training(Step): loss,1610348465205,0.20098820328712463
5472,-Metrics/Training(Step): loss,1610348467265,0.18596142530441284
5474,-Metrics/Training(Step): loss,1610348469432,0.24965783953666687
5476,-Metrics/Training(Step): loss,1610348471502,0.20436489582061768
5478,-Metrics/Training(Step): loss,1610348473595,0.2745262682437897
5480,-Metrics/Training(Step): loss,1610348475697,0.23110653460025787
5482,-Metrics/Training(Step): loss,1610348477787,0.21568416059017181
5484,-Metrics/Training(Step): loss,1610348479736,0.20488859713077545
5486,-Metrics/Training(Step): loss,1610348481709,0.20180660486221313
5488,-Metrics/Training(Step): loss,1610348483627,0.2291119247674942
5490,-Metrics/Training(Step): loss,1610348485556,0.2053588628768921
5492,-Metrics/Training(Step): loss,1610348487471,0.21621829271316528
5494,-Metrics/Training(Step): loss,1610348489525,0.1621958613395691
5496,-Metrics/Training(Step): loss,1610348491528,0.15705157816410065
5498,-Metrics/Training(Step): loss,1610348493585,0.20023499429225922
5500,-Metrics/Training(Step): loss,1610348495490,0.1761632114648819
5502,-Metrics/Training(Step): loss,1610348497538,0.17049209773540497
5504,-Metrics/Training(Step): loss,1610348499601,0.19599488377571106
5506,-Metrics/Training(Step): loss,1610348512846,0.15077827870845795
5508,-Metrics/Training(Step): loss,1610348517022,0.15934553742408752
5510,-Metrics/Training(Step): loss,1610348521115,0.27046260237693787
5512,-Metrics/Training(Step): loss,1610348525420,0.18790973722934723
5514,-Metrics/Training(Step): loss,1610348529919,0.20080749690532684
5516,-Metrics/Training(Step): loss,1610348534134,0.1761208474636078
5518,-Metrics/Training(Step): loss,1610348538020,0.20924857258796692
5520,-Metrics/Training(Step): loss,1610348541515,0.20654040575027466
5522,-Metrics/Training(Step): loss,1610348544723,0.18441078066825867
5524,-Metrics/Training(Step): loss,1610348548217,0.18231558799743652
5526,-Metrics/Training(Step): loss,1610348552016,0.20312723517417908
5528,-Metrics/Training(Step): loss,1610348555233,0.19002185761928558
5530,-Metrics/Training(Step): loss,1610348558867,0.19337895512580872
5532,-Metrics/Training(Step): loss,1610348562139,0.1589253544807434
5534,-Metrics/Training(Step): loss,1610348565720,0.18271419405937195
5536,-Metrics/Training(Step): loss,1610348569322,0.20651540160179138
5538,-Metrics/Training(Step): loss,1610348572439,0.19495061039924622
5540,-Metrics/Training(Step): loss,1610348575667,0.15604205429553986
5542,-Metrics/Training(Step): loss,1610348579620,0.15245221555233002
5544,-Metrics/Training(Step): loss,1610348582820,0.2067602574825287
5546,-Metrics/Training(Step): loss,1610348586799,0.2006373256444931
5548,-Metrics/Training(Step): loss,1610348590538,0.14442381262779236
5550,-Metrics/Training(Step): loss,1610348593902,0.21777255833148956
5552,-Metrics/Training(Step): loss,1610348596785,0.14701661467552185
5554,-Metrics/Training(Step): loss,1610348599003,0.21017342805862427
5556,-Metrics/Training(Step): loss,1610348601242,0.2072373628616333
5558,-Metrics/Training(Step): loss,1610348603285,0.19838207960128784
5560,-Metrics/Training(Step): loss,1610348605388,0.19204622507095337
5562,-Metrics/Training(Step): loss,1610348607497,0.22180430591106415
5564,-Metrics/Training(Step): loss,1610348609553,0.20591986179351807
5566,-Metrics/Training(Step): loss,1610348611678,0.1804656684398651
5568,-Metrics/Training(Step): loss,1610348613748,0.12605531513690948
5570,-Metrics/Training(Step): loss,1610348615652,0.17208562791347504
5572,-Metrics/Training(Step): loss,1610348617670,0.18617309629917145
5574,-Metrics/Training(Step): loss,1610348619691,0.19309356808662415
5576,-Metrics/Training(Step): loss,1610348621846,0.15545552968978882
5578,-Metrics/Training(Step): loss,1610348623888,0.202091783285141
5580,-Metrics/Training(Step): loss,1610348625792,0.1753551959991455
5582,-Metrics/Training(Step): loss,1610348627882,0.17191891372203827
5584,-Metrics/Training(Step): loss,1610348629892,0.15705661475658417
5586,-Metrics/Training(Step): loss,1610348631926,0.16829408705234528
5588,-Metrics/Training(Step): loss,1610348634066,0.151387557387352
5590,-Metrics/Training(Step): loss,1610348636132,0.15011504292488098
5592,-Metrics/Training(Step): loss,1610348648433,0.18108534812927246
5594,-Metrics/Training(Step): loss,1610348652520,0.18356747925281525
5596,-Metrics/Training(Step): loss,1610348656319,0.19609582424163818
5598,-Metrics/Training(Step): loss,1610348660425,0.1932181715965271
5600,-Metrics/Training(Step): loss,1610348664725,0.19133765995502472
5602,-Metrics/Training(Step): loss,1610348669022,0.18267029523849487
5604,-Metrics/Training(Step): loss,1610348672819,0.20157942175865173
5606,-Metrics/Training(Step): loss,1610348675820,0.18726582825183868
5608,-Metrics/Training(Step): loss,1610348679688,0.22205442190170288
5610,-Metrics/Training(Step): loss,1610348683119,0.18201017379760742
5612,-Metrics/Training(Step): loss,1610348686345,0.1990126222372055
5614,-Metrics/Training(Step): loss,1610348690218,0.22159916162490845
5616,-Metrics/Training(Step): loss,1610348694428,0.17950548231601715
5618,-Metrics/Training(Step): loss,1610348698221,0.19854670763015747
5620,-Metrics/Training(Step): loss,1610348702058,0.17287500202655792
5622,-Metrics/Training(Step): loss,1610348705219,0.1890912801027298
5624,-Metrics/Training(Step): loss,1610348708542,0.19486846029758453
5626,-Metrics/Training(Step): loss,1610348712333,0.19442324340343475
5628,-Metrics/Training(Step): loss,1610348715630,0.17831385135650635
5630,-Metrics/Training(Step): loss,1610348718904,0.25700438022613525
5632,-Metrics/Training(Step): loss,1610348722419,0.17410849034786224
5634,-Metrics/Training(Step): loss,1610348725820,0.13213689625263214
5636,-Metrics/Training(Step): loss,1610348729519,0.1785113364458084
5638,-Metrics/Training(Step): loss,1610348732243,0.1815551072359085
5640,-Metrics/Training(Step): loss,1610348734660,0.15033288300037384
5642,-Metrics/Training(Step): loss,1610348736639,0.1966678649187088
5644,-Metrics/Training(Step): loss,1610348738762,0.1862799972295761
5646,-Metrics/Training(Step): loss,1610348740866,0.1958293467760086
5648,-Metrics/Training(Step): loss,1610348742973,0.18093709647655487
5650,-Metrics/Training(Step): loss,1610348745004,0.13511237502098083
5652,-Metrics/Training(Step): loss,1610348747121,0.17342735826969147
5654,-Metrics/Training(Step): loss,1610348749238,0.17205442488193512
5656,-Metrics/Training(Step): loss,1610348751291,0.2152099460363388
5658,-Metrics/Training(Step): loss,1610348753429,0.16853313148021698
5660,-Metrics/Training(Step): loss,1610348755500,0.20693589746952057
5662,-Metrics/Training(Step): loss,1610348757498,0.2596541941165924
5664,-Metrics/Training(Step): loss,1610348759536,0.18504786491394043
5666,-Metrics/Training(Step): loss,1610348761447,0.20447970926761627
5668,-Metrics/Training(Step): loss,1610348763437,0.22210176289081573
5670,-Metrics/Training(Step): loss,1610348765507,0.1774570643901825
5672,-Metrics/Training(Step): loss,1610348767587,0.16442269086837769
5674,-Metrics/Training(Step): loss,1610348769584,0.1976858377456665
5676,-Metrics/Training(Step): loss,1610348771653,0.19169405102729797
5678,-Metrics/Training(Step): loss,1610348783838,0.22017011046409607
5680,-Metrics/Training(Step): loss,1610348787736,0.18050049245357513
5682,-Metrics/Training(Step): loss,1610348791601,0.162794828414917
5684,-Metrics/Training(Step): loss,1610348796330,0.1800355166196823
5686,-Metrics/Training(Step): loss,1610348800559,0.19157899916172028
5688,-Metrics/Training(Step): loss,1610348804421,0.21232835948467255
5690,-Metrics/Training(Step): loss,1610348809039,0.168572798371315
5692,-Metrics/Training(Step): loss,1610348813235,0.20188279449939728
5694,-Metrics/Training(Step): loss,1610348817020,0.2224118709564209
5696,-Metrics/Training(Step): loss,1610348820721,0.2002311646938324
5698,-Metrics/Training(Step): loss,1610348824468,0.18459105491638184
5700,-Metrics/Training(Step): loss,1610348828221,0.16986942291259766
5702,-Metrics/Training(Step): loss,1610348832021,0.18453773856163025
5704,-Metrics/Training(Step): loss,1610348835922,0.17095071077346802
5706,-Metrics/Training(Step): loss,1610348839020,0.1889515072107315
5708,-Metrics/Training(Step): loss,1610348842746,0.18235625326633453
5710,-Metrics/Training(Step): loss,1610348846150,0.17514239251613617
5712,-Metrics/Training(Step): loss,1610348849220,0.1584090143442154
5714,-Metrics/Training(Step): loss,1610348852887,0.18343453109264374
5716,-Metrics/Training(Step): loss,1610348856419,0.152448832988739
5718,-Metrics/Training(Step): loss,1610348860119,0.15965880453586578
5720,-Metrics/Training(Step): loss,1610348863727,0.18307313323020935
5722,-Metrics/Training(Step): loss,1610348867044,0.18003173172473907
5724,-Metrics/Training(Step): loss,1610348869395,0.1804778128862381
5726,-Metrics/Training(Step): loss,1610348871467,0.2420607954263687
5728,-Metrics/Training(Step): loss,1610348873541,0.15337395668029785
5730,-Metrics/Training(Step): loss,1610348875605,0.14290092885494232
5732,-Metrics/Training(Step): loss,1610348877699,0.19493752717971802
5734,-Metrics/Training(Step): loss,1610348879798,0.1889783889055252
5736,-Metrics/Training(Step): loss,1610348881810,0.16901244223117828
5738,-Metrics/Training(Step): loss,1610348883737,0.1514100283384323
5740,-Metrics/Training(Step): loss,1610348885796,0.16147463023662567
5742,-Metrics/Training(Step): loss,1610348887894,0.1627897173166275
5744,-Metrics/Training(Step): loss,1610348889951,0.2158837765455246
5746,-Metrics/Training(Step): loss,1610348892095,0.1558469980955124
5748,-Metrics/Training(Step): loss,1610348893850,0.13615714013576508
5750,-Metrics/Training(Step): loss,1610348895900,0.14603137969970703
5752,-Metrics/Training(Step): loss,1610348897899,0.18318934738636017
5754,-Metrics/Training(Step): loss,1610348899887,0.18719755113124847
5756,-Metrics/Training(Step): loss,1610348901932,0.18442468345165253
5758,-Metrics/Training(Step): loss,1610348903973,0.16451121866703033
5760,-Metrics/Training(Step): loss,1610348906008,0.14403994381427765
5762,-Metrics/Training(Step): loss,1610348907954,0.168741375207901
5764,-Metrics/Training(Step): loss,1610348920324,0.2006884515285492
5766,-Metrics/Training(Step): loss,1610348924631,0.1791132688522339
5768,-Metrics/Training(Step): loss,1610348928736,0.1685151308774948
5770,-Metrics/Training(Step): loss,1610348932623,0.17823229730129242
5772,-Metrics/Training(Step): loss,1610348936446,0.1882457733154297
5774,-Metrics/Training(Step): loss,1610348940044,0.21190232038497925
5776,-Metrics/Training(Step): loss,1610348943943,0.18114624917507172
5778,-Metrics/Training(Step): loss,1610348947715,0.15754657983779907
5780,-Metrics/Training(Step): loss,1610348951520,0.22020840644836426
5782,-Metrics/Training(Step): loss,1610348954838,0.16855670511722565
5784,-Metrics/Training(Step): loss,1610348958720,0.1905895620584488
5786,-Metrics/Training(Step): loss,1610348962223,0.13432852923870087
5788,-Metrics/Training(Step): loss,1610348966271,0.2220514565706253
5790,-Metrics/Training(Step): loss,1610348969913,0.18571966886520386
5792,-Metrics/Training(Step): loss,1610348973409,0.15782752633094788
5794,-Metrics/Training(Step): loss,1610348976516,0.18660733103752136
5796,-Metrics/Training(Step): loss,1610348979920,0.1418105959892273
5798,-Metrics/Training(Step): loss,1610348983466,0.17546169459819794
5800,-Metrics/Training(Step): loss,1610348986866,0.14238855242729187
5802,-Metrics/Training(Step): loss,1610348990720,0.20091839134693146
5804,-Metrics/Training(Step): loss,1610348994219,0.19615335762500763
5806,-Metrics/Training(Step): loss,1610348997545,0.1067792996764183
5808,-Metrics/Training(Step): loss,1610349001046,0.17352713644504547
5810,-Metrics/Training(Step): loss,1610349004443,0.17940008640289307
5812,-Metrics/Training(Step): loss,1610349006615,0.17715300619602203
5814,-Metrics/Training(Step): loss,1610349008889,0.17941750586032867
5816,-Metrics/Training(Step): loss,1610349011088,0.12866030633449554
5818,-Metrics/Training(Step): loss,1610349013178,0.20087851583957672
5820,-Metrics/Training(Step): loss,1610349015262,0.19040243327617645
5822,-Metrics/Training(Step): loss,1610349017366,0.18035216629505157
5824,-Metrics/Training(Step): loss,1610349019412,0.19716660678386688
5826,-Metrics/Training(Step): loss,1610349021433,0.17358696460723877
5828,-Metrics/Training(Step): loss,1610349023479,0.17892298102378845
5830,-Metrics/Training(Step): loss,1610349025442,0.20052258670330048
5832,-Metrics/Training(Step): loss,1610349027436,0.16410432755947113
5834,-Metrics/Training(Step): loss,1610349029232,0.19451138377189636
5836,-Metrics/Training(Step): loss,1610349031358,0.15744079649448395
5838,-Metrics/Training(Step): loss,1610349033445,0.14544916152954102
5840,-Metrics/Training(Step): loss,1610349035476,0.18914824724197388
5842,-Metrics/Training(Step): loss,1610349037529,0.1756085306406021
5844,-Metrics/Training(Step): loss,1610349039304,0.18965579569339752
5846,-Metrics/Training(Step): loss,1610349041371,0.23383909463882446
5848,-Metrics/Training(Step): loss,1610349043423,0.18197093904018402
5850,-Metrics/Training(Step): loss,1610349056352,0.2071170061826706
5852,-Metrics/Training(Step): loss,1610349060737,0.20980720221996307
5854,-Metrics/Training(Step): loss,1610349064958,0.20769411325454712
5856,-Metrics/Training(Step): loss,1610349069042,0.17579808831214905
5858,-Metrics/Training(Step): loss,1610349072933,0.1860230267047882
5860,-Metrics/Training(Step): loss,1610349076657,0.1696324348449707
5862,-Metrics/Training(Step): loss,1610349080412,0.23899276554584503
5864,-Metrics/Training(Step): loss,1610349084265,0.18106989562511444
5866,-Metrics/Training(Step): loss,1610349088020,0.1768181025981903
5868,-Metrics/Training(Step): loss,1610349091672,0.2537168562412262
5870,-Metrics/Training(Step): loss,1610349095461,0.2756178379058838
5872,-Metrics/Training(Step): loss,1610349099219,0.18472550809383392
5874,-Metrics/Training(Step): loss,1610349102756,0.23108215630054474
5876,-Metrics/Training(Step): loss,1610349106629,0.18094956874847412
5878,-Metrics/Training(Step): loss,1610349111022,0.21175487339496613
5880,-Metrics/Training(Step): loss,1610349114627,0.1922149658203125
5882,-Metrics/Training(Step): loss,1610349117954,0.15807592868804932
5884,-Metrics/Training(Step): loss,1610349121204,0.20731857419013977
5886,-Metrics/Training(Step): loss,1610349124628,0.22041113674640656
5888,-Metrics/Training(Step): loss,1610349128178,0.22187039256095886
5890,-Metrics/Training(Step): loss,1610349131721,0.19370003044605255
5892,-Metrics/Training(Step): loss,1610349135119,0.16628775000572205
5894,-Metrics/Training(Step): loss,1610349138564,0.15478920936584473
5896,-Metrics/Training(Step): loss,1610349141502,0.21609382331371307
5898,-Metrics/Training(Step): loss,1610349143576,0.1907353401184082
5900,-Metrics/Training(Step): loss,1610349145652,0.21117720007896423
5902,-Metrics/Training(Step): loss,1610349147788,0.27325546741485596
5904,-Metrics/Training(Step): loss,1610349149888,0.2519584596157074
5906,-Metrics/Training(Step): loss,1610349152019,0.18924592435359955
5908,-Metrics/Training(Step): loss,1610349154116,0.19654114544391632
5910,-Metrics/Training(Step): loss,1610349156195,0.15637601912021637
5912,-Metrics/Training(Step): loss,1610349158245,0.21171419322490692
5914,-Metrics/Training(Step): loss,1610349160066,0.18463297188282013
5916,-Metrics/Training(Step): loss,1610349162221,0.15778955817222595
5918,-Metrics/Training(Step): loss,1610349164202,0.18065054714679718
5920,-Metrics/Training(Step): loss,1610349166281,0.19374696910381317
5922,-Metrics/Training(Step): loss,1610349168290,0.21985521912574768
5924,-Metrics/Training(Step): loss,1610349170398,0.21548478305339813
5926,-Metrics/Training(Step): loss,1610349172393,0.19315952062606812
5928,-Metrics/Training(Step): loss,1610349174312,0.1974647492170334
5930,-Metrics/Training(Step): loss,1610349176338,0.21833692491054535
5932,-Metrics/Training(Step): loss,1610349178409,0.19309468567371368
5934,-Metrics/Training(Step): loss,1610349180489,0.16597819328308105
5936,-Metrics/Training(Step): loss,1610349193231,0.20691965520381927
5938,-Metrics/Training(Step): loss,1610349197421,0.15848736464977264
5940,-Metrics/Training(Step): loss,1610349201222,0.17187972366809845
5942,-Metrics/Training(Step): loss,1610349205120,0.2085171341896057
5944,-Metrics/Training(Step): loss,1610349208920,0.15266478061676025
5946,-Metrics/Training(Step): loss,1610349213317,0.21114443242549896
5948,-Metrics/Training(Step): loss,1610349217120,0.17074823379516602
5950,-Metrics/Training(Step): loss,1610349221352,0.19925054907798767
5952,-Metrics/Training(Step): loss,1610349225621,0.19691208004951477
5954,-Metrics/Training(Step): loss,1610349229456,0.1785343587398529
5956,-Metrics/Training(Step): loss,1610349232727,0.15641546249389648
5958,-Metrics/Training(Step): loss,1610349236020,0.1865188181400299
5960,-Metrics/Training(Step): loss,1610349239356,0.17989873886108398
5962,-Metrics/Training(Step): loss,1610349242421,0.19929219782352448
5964,-Metrics/Training(Step): loss,1610349245941,0.17384512722492218
5966,-Metrics/Training(Step): loss,1610349249048,0.22120554745197296
5968,-Metrics/Training(Step): loss,1610349252739,0.17016012966632843
5970,-Metrics/Training(Step): loss,1610349256520,0.16695834696292877
5972,-Metrics/Training(Step): loss,1610349259619,0.1833658516407013
5974,-Metrics/Training(Step): loss,1610349263120,0.1710100769996643
5976,-Metrics/Training(Step): loss,1610349266619,0.19014506042003632
5978,-Metrics/Training(Step): loss,1610349269771,0.18672174215316772
5980,-Metrics/Training(Step): loss,1610349273020,0.17808248102664948
5982,-Metrics/Training(Step): loss,1610349275925,0.16478629410266876
5984,-Metrics/Training(Step): loss,1610349278939,0.20928379893302917
5986,-Metrics/Training(Step): loss,1610349281141,0.1512700915336609
5988,-Metrics/Training(Step): loss,1610349283365,0.19782836735248566
5990,-Metrics/Training(Step): loss,1610349285488,0.19451653957366943
5992,-Metrics/Training(Step): loss,1610349287500,0.21351855993270874
5994,-Metrics/Training(Step): loss,1610349289620,0.21409018337726593
5996,-Metrics/Training(Step): loss,1610349291718,0.14581476151943207
5998,-Metrics/Training(Step): loss,1610349293804,0.14958514273166656
6000,-Metrics/Training(Step): loss,1610349295851,0.17801368236541748
6002,-Metrics/Training(Step): loss,1610349297980,0.16157063841819763
6004,-Metrics/Training(Step): loss,1610349300055,0.12419158220291138
6006,-Metrics/Training(Step): loss,1610349302007,0.17674174904823303
6008,-Metrics/Training(Step): loss,1610349304075,0.14555175602436066
6010,-Metrics/Training(Step): loss,1610349306022,0.11920369416475296
6012,-Metrics/Training(Step): loss,1610349308119,0.17526166141033173
6014,-Metrics/Training(Step): loss,1610349310072,0.15196987986564636
6016,-Metrics/Training(Step): loss,1610349312155,0.16436465084552765
6018,-Metrics/Training(Step): loss,1610349314246,0.20317021012306213
6020,-Metrics/Training(Step): loss,1610349316303,0.17809583246707916
6022,-Metrics/Training(Step): loss,1610349329737,0.16285105049610138
6024,-Metrics/Training(Step): loss,1610349333620,0.19017575681209564
6026,-Metrics/Training(Step): loss,1610349337319,0.177300363779068
6028,-Metrics/Training(Step): loss,1610349341826,0.22660472989082336
6030,-Metrics/Training(Step): loss,1610349345745,0.16838452219963074
6032,-Metrics/Training(Step): loss,1610349349834,0.24938710033893585
6034,-Metrics/Training(Step): loss,1610349353922,0.1841062307357788
6036,-Metrics/Training(Step): loss,1610349357644,0.18111677467823029
6038,-Metrics/Training(Step): loss,1610349361153,0.21193136274814606
6040,-Metrics/Training(Step): loss,1610349364319,0.16721124947071075
6042,-Metrics/Training(Step): loss,1610349367904,0.1795884072780609
6044,-Metrics/Training(Step): loss,1610349371423,0.23460619151592255
6046,-Metrics/Training(Step): loss,1610349375716,0.175002783536911
6048,-Metrics/Training(Step): loss,1610349378920,0.17324170470237732
6050,-Metrics/Training(Step): loss,1610349382219,0.147555872797966
6052,-Metrics/Training(Step): loss,1610349385818,0.18320807814598083
6054,-Metrics/Training(Step): loss,1610349389220,0.21658708155155182
6056,-Metrics/Training(Step): loss,1610349392819,0.20320065319538116
6058,-Metrics/Training(Step): loss,1610349396512,0.1892809420824051
6060,-Metrics/Training(Step): loss,1610349399820,0.19544531404972076
6062,-Metrics/Training(Step): loss,1610349403024,0.1527467519044876
6064,-Metrics/Training(Step): loss,1610349406867,0.18978853523731232
6066,-Metrics/Training(Step): loss,1610349410811,0.17404159903526306
6068,-Metrics/Training(Step): loss,1610349413580,0.18017764389514923
6070,-Metrics/Training(Step): loss,1610349416003,0.15544018149375916
6072,-Metrics/Training(Step): loss,1610349418212,0.1866394728422165
6074,-Metrics/Training(Step): loss,1610349420263,0.18599383533000946
6076,-Metrics/Training(Step): loss,1610349422324,0.17110028862953186
6078,-Metrics/Training(Step): loss,1610349424430,0.15885847806930542
6080,-Metrics/Training(Step): loss,1610349426535,0.18066498637199402
6082,-Metrics/Training(Step): loss,1610349428663,0.16606096923351288
6084,-Metrics/Training(Step): loss,1610349430755,0.1850937455892563
6086,-Metrics/Training(Step): loss,1610349432879,0.1790725141763687
6088,-Metrics/Training(Step): loss,1610349434757,0.2035248726606369
6090,-Metrics/Training(Step): loss,1610349436868,0.16863791644573212
6092,-Metrics/Training(Step): loss,1610349438996,0.168911874294281
6094,-Metrics/Training(Step): loss,1610349441016,0.17038007080554962
6096,-Metrics/Training(Step): loss,1610349443154,0.19263024628162384
6098,-Metrics/Training(Step): loss,1610349445171,0.18122050166130066
6100,-Metrics/Training(Step): loss,1610349447245,0.21514727175235748
6102,-Metrics/Training(Step): loss,1610349449333,0.18782424926757812
6104,-Metrics/Training(Step): loss,1610349451400,0.14465965330600739
6106,-Metrics/Training(Step): loss,1610349453457,0.17897582054138184
6108,-Metrics/Training(Step): loss,1610349466223,0.14369308948516846
6110,-Metrics/Training(Step): loss,1610349470519,0.19814032316207886
6112,-Metrics/Training(Step): loss,1610349474618,0.1860273778438568
6114,-Metrics/Training(Step): loss,1610349479034,0.171293705701828
6116,-Metrics/Training(Step): loss,1610349483418,0.17231324315071106
6118,-Metrics/Training(Step): loss,1610349487320,0.12365466356277466
6120,-Metrics/Training(Step): loss,1610349491618,0.1575412154197693
6122,-Metrics/Training(Step): loss,1610349495526,0.1810263693332672
6124,-Metrics/Training(Step): loss,1610349498921,0.197378009557724
6126,-Metrics/Training(Step): loss,1610349502560,0.13896377384662628
6128,-Metrics/Training(Step): loss,1610349506035,0.1791570782661438
6130,-Metrics/Training(Step): loss,1610349509620,0.19021719694137573
6132,-Metrics/Training(Step): loss,1610349513163,0.18306192755699158
6134,-Metrics/Training(Step): loss,1610349517020,0.1784934252500534
6136,-Metrics/Training(Step): loss,1610349520628,0.17984721064567566
6138,-Metrics/Training(Step): loss,1610349524058,0.12041844427585602
6140,-Metrics/Training(Step): loss,1610349527918,0.18514443933963776
6142,-Metrics/Training(Step): loss,1610349531592,0.18810036778450012
6144,-Metrics/Training(Step): loss,1610349534765,0.18138761818408966
6146,-Metrics/Training(Step): loss,1610349538352,0.20205162465572357
6148,-Metrics/Training(Step): loss,1610349542219,0.1870986521244049
6150,-Metrics/Training(Step): loss,1610349545433,0.18555037677288055
6152,-Metrics/Training(Step): loss,1610349548524,0.1283472776412964
6154,-Metrics/Training(Step): loss,1610349551415,0.17729049921035767
6156,-Metrics/Training(Step): loss,1610349553486,0.16008360683918
6158,-Metrics/Training(Step): loss,1610349555548,0.17039619386196136
6160,-Metrics/Training(Step): loss,1610349557578,0.1354481726884842
6162,-Metrics/Training(Step): loss,1610349559668,0.17841538786888123
6164,-Metrics/Training(Step): loss,1610349561759,0.18821920454502106
6166,-Metrics/Training(Step): loss,1610349563771,0.1440097540616989
6168,-Metrics/Training(Step): loss,1610349565885,0.14084814488887787
6170,-Metrics/Training(Step): loss,1610349568001,0.19298890233039856
6172,-Metrics/Training(Step): loss,1610349570141,0.21370017528533936
6174,-Metrics/Training(Step): loss,1610349572190,0.1598699539899826
6176,-Metrics/Training(Step): loss,1610349574145,0.17835472524166107
6178,-Metrics/Training(Step): loss,1610349576261,0.2041691690683365
6180,-Metrics/Training(Step): loss,1610349578307,0.13955920934677124
6182,-Metrics/Training(Step): loss,1610349580266,0.19868548214435577
6184,-Metrics/Training(Step): loss,1610349582354,0.17320720851421356
6186,-Metrics/Training(Step): loss,1610349584408,0.20201605558395386
6188,-Metrics/Training(Step): loss,1610349586314,0.1948385238647461
6190,-Metrics/Training(Step): loss,1610349588263,0.17747905850410461
6192,-Metrics/Training(Step): loss,1610349590323,0.17111507058143616
6194,-Metrics/Training(Step): loss,1610349602345,0.16908153891563416
6196,-Metrics/Training(Step): loss,1610349606529,0.13846221566200256
6198,-Metrics/Training(Step): loss,1610349610322,0.11097627878189087
6200,-Metrics/Training(Step): loss,1610349614121,0.1561329960823059
6202,-Metrics/Training(Step): loss,1610349618215,0.20506934821605682
6204,-Metrics/Training(Step): loss,1610349622620,0.15967851877212524
6206,-Metrics/Training(Step): loss,1610349626719,0.1575837880373001
6208,-Metrics/Training(Step): loss,1610349630516,0.17881493270397186
6210,-Metrics/Training(Step): loss,1610349633622,0.15287365019321442
6212,-Metrics/Training(Step): loss,1610349637322,0.165632426738739
6214,-Metrics/Training(Step): loss,1610349640919,0.13879047334194183
6216,-Metrics/Training(Step): loss,1610349644820,0.16066713631153107
6218,-Metrics/Training(Step): loss,1610349648572,0.17654351890087128
6220,-Metrics/Training(Step): loss,1610349652015,0.187993124127388
6222,-Metrics/Training(Step): loss,1610349655536,0.16705729067325592
6224,-Metrics/Training(Step): loss,1610349659274,0.17340119183063507
6226,-Metrics/Training(Step): loss,1610349662843,0.1704123616218567
6228,-Metrics/Training(Step): loss,1610349666242,0.19245699048042297
6230,-Metrics/Training(Step): loss,1610349670124,0.1425083577632904
6232,-Metrics/Training(Step): loss,1610349673319,0.1438894271850586
6234,-Metrics/Training(Step): loss,1610349676118,0.162469282746315
6236,-Metrics/Training(Step): loss,1610349679441,0.17173120379447937
6238,-Metrics/Training(Step): loss,1610349683020,0.16185134649276733
6240,-Metrics/Training(Step): loss,1610349686227,0.20887663960456848
6242,-Metrics/Training(Step): loss,1610349688588,0.18000945448875427
6244,-Metrics/Training(Step): loss,1610349690638,0.18907597661018372
6246,-Metrics/Training(Step): loss,1610349692688,0.18837158381938934
6248,-Metrics/Training(Step): loss,1610349694806,0.21995888650417328
6250,-Metrics/Training(Step): loss,1610349696920,0.21463991701602936
6252,-Metrics/Training(Step): loss,1610349698997,0.18142381310462952
6254,-Metrics/Training(Step): loss,1610349701130,0.1349354088306427
6256,-Metrics/Training(Step): loss,1610349703139,0.15547896921634674
6258,-Metrics/Training(Step): loss,1610349705081,0.1885470747947693
6260,-Metrics/Training(Step): loss,1610349707143,0.16900935769081116
6262,-Metrics/Training(Step): loss,1610349709078,0.12795008718967438
6264,-Metrics/Training(Step): loss,1610349711130,0.18130004405975342
6266,-Metrics/Training(Step): loss,1610349713086,0.1441832333803177
6268,-Metrics/Training(Step): loss,1610349715186,0.17126193642616272
6270,-Metrics/Training(Step): loss,1610349717248,0.15120358765125275
6272,-Metrics/Training(Step): loss,1610349719204,0.17889392375946045
6274,-Metrics/Training(Step): loss,1610349721257,0.15090268850326538
6276,-Metrics/Training(Step): loss,1610349723350,0.1656409502029419
6278,-Metrics/Training(Step): loss,1610349725378,0.16641497611999512
6280,-Metrics/Training(Step): loss,1610349737742,0.12327755987644196
6282,-Metrics/Training(Step): loss,1610349742120,0.17395617067813873
6284,-Metrics/Training(Step): loss,1610349745758,0.1805298775434494
6286,-Metrics/Training(Step): loss,1610349749720,0.1514206975698471
6288,-Metrics/Training(Step): loss,1610349753844,0.16579091548919678
6290,-Metrics/Training(Step): loss,1610349757657,0.1526857167482376
6292,-Metrics/Training(Step): loss,1610349761620,0.18085046112537384
6294,-Metrics/Training(Step): loss,1610349765320,0.14636413753032684
6296,-Metrics/Training(Step): loss,1610349768824,0.17415183782577515
6298,-Metrics/Training(Step): loss,1610349772822,0.15103259682655334
6300,-Metrics/Training(Step): loss,1610349775928,0.18928591907024384
6302,-Metrics/Training(Step): loss,1610349779419,0.1473885327577591
6304,-Metrics/Training(Step): loss,1610349783223,0.15538965165615082
6306,-Metrics/Training(Step): loss,1610349786973,0.1659872829914093
6308,-Metrics/Training(Step): loss,1610349791193,0.1460399329662323
6310,-Metrics/Training(Step): loss,1610349794943,0.17682093381881714
6312,-Metrics/Training(Step): loss,1610349798519,0.15095527470111847
6314,-Metrics/Training(Step): loss,1610349802019,0.12042368948459625
6316,-Metrics/Training(Step): loss,1610349805341,0.19307765364646912
6318,-Metrics/Training(Step): loss,1610349808920,0.13162927329540253
6320,-Metrics/Training(Step): loss,1610349812645,0.14245732128620148
6322,-Metrics/Training(Step): loss,1610349816171,0.19360877573490143
6324,-Metrics/Training(Step): loss,1610349819319,0.21780884265899658
6326,-Metrics/Training(Step): loss,1610349822013,0.16184654831886292
6328,-Metrics/Training(Step): loss,1610349824034,0.13895070552825928
6330,-Metrics/Training(Step): loss,1610349826153,0.14145511388778687
6332,-Metrics/Training(Step): loss,1610349828232,0.1955673098564148
6334,-Metrics/Training(Step): loss,1610349830200,0.20419174432754517
6336,-Metrics/Training(Step): loss,1610349832277,0.1625012755393982
6338,-Metrics/Training(Step): loss,1610349834421,0.19336551427841187
6340,-Metrics/Training(Step): loss,1610349836449,0.15763907134532928
6342,-Metrics/Training(Step): loss,1610349838486,0.18892252445220947
6344,-Metrics/Training(Step): loss,1610349840588,0.14720429480075836
6346,-Metrics/Training(Step): loss,1610349842623,0.1618921458721161
6348,-Metrics/Training(Step): loss,1610349844606,0.18993349373340607
6350,-Metrics/Training(Step): loss,1610349846694,0.15470640361309052
6352,-Metrics/Training(Step): loss,1610349848794,0.19604100286960602
6354,-Metrics/Training(Step): loss,1610349850897,0.15074676275253296
6356,-Metrics/Training(Step): loss,1610349852734,0.14109300076961517
6358,-Metrics/Training(Step): loss,1610349854838,0.20105382800102234
6360,-Metrics/Training(Step): loss,1610349856949,0.16453267633914948
6362,-Metrics/Training(Step): loss,1610349858886,0.17787790298461914
6364,-Metrics/Training(Step): loss,1610349860934,0.1683526337146759
6366,-Metrics/Training(Step): loss,1610349875034,0.17656998336315155
6368,-Metrics/Training(Step): loss,1610349878915,0.1714014708995819
6370,-Metrics/Training(Step): loss,1610349883142,0.18697895109653473
6372,-Metrics/Training(Step): loss,1610349887022,0.18322758376598358
6374,-Metrics/Training(Step): loss,1610349890844,0.19553866982460022
6376,-Metrics/Training(Step): loss,1610349894619,0.17018459737300873
6378,-Metrics/Training(Step): loss,1610349898718,0.1830676943063736
6380,-Metrics/Training(Step): loss,1610349902720,0.1217489242553711
6382,-Metrics/Training(Step): loss,1610349906663,0.13833273947238922
6384,-Metrics/Training(Step): loss,1610349910222,0.15518175065517426
6386,-Metrics/Training(Step): loss,1610349914066,0.13729040324687958
6388,-Metrics/Training(Step): loss,1610349917680,0.19090837240219116
6390,-Metrics/Training(Step): loss,1610349921235,0.17929193377494812
6392,-Metrics/Training(Step): loss,1610349924525,0.17988264560699463
6394,-Metrics/Training(Step): loss,1610349928141,0.11231625825166702
6396,-Metrics/Training(Step): loss,1610349931626,0.15989340841770172
6398,-Metrics/Training(Step): loss,1610349935069,0.17544357478618622
6400,-Metrics/Training(Step): loss,1610349938436,0.1627379059791565
6402,-Metrics/Training(Step): loss,1610349941452,0.2153344601392746
6404,-Metrics/Training(Step): loss,1610349944919,0.14915892481803894
6406,-Metrics/Training(Step): loss,1610349948943,0.20467208325862885
6408,-Metrics/Training(Step): loss,1610349952619,0.12547242641448975
6410,-Metrics/Training(Step): loss,1610349955473,0.14104419946670532
6412,-Metrics/Training(Step): loss,1610349957915,0.11938754469156265
6414,-Metrics/Training(Step): loss,1610349960298,0.19318552315235138
6416,-Metrics/Training(Step): loss,1610349962445,0.1559561938047409
6418,-Metrics/Training(Step): loss,1610349964391,0.16301646828651428
6420,-Metrics/Training(Step): loss,1610349966499,0.1910077929496765
6422,-Metrics/Training(Step): loss,1610349968594,0.1413523554801941
6424,-Metrics/Training(Step): loss,1610349970727,0.15329602360725403
6426,-Metrics/Training(Step): loss,1610349972608,0.15733535587787628
6428,-Metrics/Training(Step): loss,1610349974643,0.145630344748497
6430,-Metrics/Training(Step): loss,1610349976693,0.16796565055847168
6432,-Metrics/Training(Step): loss,1610349978837,0.127623051404953
6434,-Metrics/Training(Step): loss,1610349980905,0.1797674298286438
6436,-Metrics/Training(Step): loss,1610349982969,0.13891348242759705
6438,-Metrics/Training(Step): loss,1610349985028,0.1391836255788803
6440,-Metrics/Training(Step): loss,1610349987124,0.18917496502399445
6442,-Metrics/Training(Step): loss,1610349989158,0.17820918560028076
6444,-Metrics/Training(Step): loss,1610349991227,0.15690846741199493
6446,-Metrics/Training(Step): loss,1610349993263,0.15656974911689758
6448,-Metrics/Training(Step): loss,1610349995350,0.1889304667711258
6450,-Metrics/Training(Step): loss,1610349997400,0.1759997457265854
6452,-Metrics/Training(Step): loss,1610350011134,0.16133619844913483
6454,-Metrics/Training(Step): loss,1610350015030,0.13524292409420013
6456,-Metrics/Training(Step): loss,1610350019017,0.1733352243900299
6458,-Metrics/Training(Step): loss,1610350022832,0.13977399468421936
6460,-Metrics/Training(Step): loss,1610350026921,0.17483006417751312
6462,-Metrics/Training(Step): loss,1610350030921,0.18073497712612152
6464,-Metrics/Training(Step): loss,1610350034754,0.1652299463748932
6466,-Metrics/Training(Step): loss,1610350038359,0.15941795706748962
6468,-Metrics/Training(Step): loss,1610350042122,0.15337185561656952
6470,-Metrics/Training(Step): loss,1610350045515,0.1719374656677246
6472,-Metrics/Training(Step): loss,1610350048958,0.17149849236011505
6474,-Metrics/Training(Step): loss,1610350052058,0.16675688326358795
6476,-Metrics/Training(Step): loss,1610350055223,0.1798934042453766
6478,-Metrics/Training(Step): loss,1610350058627,0.1586293876171112
6480,-Metrics/Training(Step): loss,1610350062721,0.18113504350185394
6482,-Metrics/Training(Step): loss,1610350066120,0.11102636158466339
6484,-Metrics/Training(Step): loss,1610350069920,0.17786291241645813
6486,-Metrics/Training(Step): loss,1610350073938,0.13828696310520172
6488,-Metrics/Training(Step): loss,1610350077412,0.20173604786396027
6490,-Metrics/Training(Step): loss,1610350081220,0.16889414191246033
6492,-Metrics/Training(Step): loss,1610350085008,0.19077306985855103
6494,-Metrics/Training(Step): loss,1610350088450,0.17324325442314148
6496,-Metrics/Training(Step): loss,1610350091737,0.16740745306015015
6498,-Metrics/Training(Step): loss,1610350094721,0.1696508675813675
6500,-Metrics/Training(Step): loss,1610350097238,0.19812063872814178
6502,-Metrics/Training(Step): loss,1610350099231,0.15759898722171783
6504,-Metrics/Training(Step): loss,1610350101293,0.18423034250736237
6506,-Metrics/Training(Step): loss,1610350103347,0.16160793602466583
6508,-Metrics/Training(Step): loss,1610350105410,0.1554008424282074
6510,-Metrics/Training(Step): loss,1610350107419,0.17520113289356232
6512,-Metrics/Training(Step): loss,1610350109440,0.14756877720355988
6514,-Metrics/Training(Step): loss,1610350111447,0.17026366293430328
6516,-Metrics/Training(Step): loss,1610350113518,0.1512763798236847
6518,-Metrics/Training(Step): loss,1610350115591,0.1326315701007843
6520,-Metrics/Training(Step): loss,1610350117703,0.17010441422462463
6522,-Metrics/Training(Step): loss,1610350119644,0.18068012595176697
6524,-Metrics/Training(Step): loss,1610350121654,0.1556047797203064
6526,-Metrics/Training(Step): loss,1610350123639,0.16248703002929688
6528,-Metrics/Training(Step): loss,1610350125730,0.1779613345861435
6530,-Metrics/Training(Step): loss,1610350127784,0.1504715085029602
6532,-Metrics/Training(Step): loss,1610350129860,0.17612598836421967
6534,-Metrics/Training(Step): loss,1610350131903,0.14803816378116608
6536,-Metrics/Training(Step): loss,1610350133947,0.14864158630371094
6538,-Metrics/Training(Step): loss,1610350146232,0.17639212310314178
6540,-Metrics/Training(Step): loss,1610350150220,0.16402040421962738
6542,-Metrics/Training(Step): loss,1610350153915,0.1475839614868164
6544,-Metrics/Training(Step): loss,1610350157726,0.23064303398132324
6546,-Metrics/Training(Step): loss,1610350161539,0.14148829877376556
6548,-Metrics/Training(Step): loss,1610350165533,0.1709502637386322
6550,-Metrics/Training(Step): loss,1610350169519,0.1658555418252945
6552,-Metrics/Training(Step): loss,1610350173649,0.17846089601516724
6554,-Metrics/Training(Step): loss,1610350177221,0.13350901007652283
6556,-Metrics/Training(Step): loss,1610350180855,0.14936800301074982
6558,-Metrics/Training(Step): loss,1610350183920,0.18758806586265564
6560,-Metrics/Training(Step): loss,1610350187620,0.162793830037117
6562,-Metrics/Training(Step): loss,1610350191320,0.15392856299877167
6564,-Metrics/Training(Step): loss,1610350195359,0.18656118214130402
6566,-Metrics/Training(Step): loss,1610350198892,0.21476370096206665
6568,-Metrics/Training(Step): loss,1610350202720,0.18436148762702942
6570,-Metrics/Training(Step): loss,1610350206435,0.1911170780658722
6572,-Metrics/Training(Step): loss,1610350209862,0.16076678037643433
6574,-Metrics/Training(Step): loss,1610350213114,0.1648830771446228
6576,-Metrics/Training(Step): loss,1610350216534,0.1563854217529297
6578,-Metrics/Training(Step): loss,1610350220120,0.16161006689071655
6580,-Metrics/Training(Step): loss,1610350223237,0.19174352288246155
6582,-Metrics/Training(Step): loss,1610350226641,0.1868346482515335
6584,-Metrics/Training(Step): loss,1610350229436,0.16275903582572937
6586,-Metrics/Training(Step): loss,1610350231534,0.14403247833251953
6588,-Metrics/Training(Step): loss,1610350233772,0.1433965265750885
6590,-Metrics/Training(Step): loss,1610350235859,0.21707750856876373
6592,-Metrics/Training(Step): loss,1610350237897,0.21807141602039337
6594,-Metrics/Training(Step): loss,1610350239972,0.13331356644630432
6596,-Metrics/Training(Step): loss,1610350242089,0.18657828867435455
6598,-Metrics/Training(Step): loss,1610350244023,0.19240503013134003
6600,-Metrics/Training(Step): loss,1610350246076,0.20122531056404114
6602,-Metrics/Training(Step): loss,1610350248133,0.15946520864963531
6604,-Metrics/Training(Step): loss,1610350250245,0.1778259128332138
6606,-Metrics/Training(Step): loss,1610350252353,0.2223687469959259
6608,-Metrics/Training(Step): loss,1610350254448,0.16042472422122955
6610,-Metrics/Training(Step): loss,1610350256382,0.19245345890522003
6612,-Metrics/Training(Step): loss,1610350258438,0.15875880420207977
6614,-Metrics/Training(Step): loss,1610350260488,0.18144363164901733
6616,-Metrics/Training(Step): loss,1610350262539,0.12947627902030945
6618,-Metrics/Training(Step): loss,1610350264626,0.16223889589309692
6620,-Metrics/Training(Step): loss,1610350266570,0.13295133411884308
6622,-Metrics/Training(Step): loss,1610350268529,0.14100176095962524
6624,-Metrics/Training(Step): loss,1610350281738,0.17213787138462067
6626,-Metrics/Training(Step): loss,1610350285915,0.1927018165588379
6628,-Metrics/Training(Step): loss,1610350289620,0.15874017775058746
6630,-Metrics/Training(Step): loss,1610350293619,0.1863589584827423
6632,-Metrics/Training(Step): loss,1610350297558,0.18445251882076263
6634,-Metrics/Training(Step): loss,1610350301520,0.14902879297733307
6636,-Metrics/Training(Step): loss,1610350305349,0.17854508757591248
6638,-Metrics/Training(Step): loss,1610350308620,0.20054537057876587
6640,-Metrics/Training(Step): loss,1610350312122,0.13542024791240692
6642,-Metrics/Training(Step): loss,1610350315920,0.1395920366048813
6644,-Metrics/Training(Step): loss,1610350319043,0.11417592316865921
6646,-Metrics/Training(Step): loss,1610350322720,0.17030902206897736
6648,-Metrics/Training(Step): loss,1610350326031,0.16366247832775116
6650,-Metrics/Training(Step): loss,1610350330219,0.17349109053611755
6652,-Metrics/Training(Step): loss,1610350333820,0.1848854273557663
6654,-Metrics/Training(Step): loss,1610350337607,0.1691373735666275
6656,-Metrics/Training(Step): loss,1610350341246,0.18682071566581726
6658,-Metrics/Training(Step): loss,1610350344237,0.15354996919631958
6660,-Metrics/Training(Step): loss,1610350347634,0.15023444592952728
6662,-Metrics/Training(Step): loss,1610350350721,0.1881333291530609
6664,-Metrics/Training(Step): loss,1610350354440,0.20374421775341034
6666,-Metrics/Training(Step): loss,1610350357852,0.1460987627506256
6668,-Metrics/Training(Step): loss,1610350360720,0.14355432987213135
6670,-Metrics/Training(Step): loss,1610350363916,0.23031634092330933
6672,-Metrics/Training(Step): loss,1610350366382,0.12256409972906113
6674,-Metrics/Training(Step): loss,1610350368555,0.13723929226398468
6676,-Metrics/Training(Step): loss,1610350370663,0.19597981870174408
6678,-Metrics/Training(Step): loss,1610350372749,0.16380327939987183
6680,-Metrics/Training(Step): loss,1610350374896,0.1533125340938568
6682,-Metrics/Training(Step): loss,1610350376923,0.1730201542377472
6684,-Metrics/Training(Step): loss,1610350379056,0.13115733861923218
6686,-Metrics/Training(Step): loss,1610350381000,0.16365064680576324
6688,-Metrics/Training(Step): loss,1610350383102,0.1768614798784256
6690,-Metrics/Training(Step): loss,1610350385239,0.17385227978229523
6692,-Metrics/Training(Step): loss,1610350387317,0.1511216014623642
6694,-Metrics/Training(Step): loss,1610350389331,0.1848149448633194
6696,-Metrics/Training(Step): loss,1610350391381,0.1628822237253189
6698,-Metrics/Training(Step): loss,1610350393399,0.10059221088886261
6700,-Metrics/Training(Step): loss,1610350395507,0.16895461082458496
6702,-Metrics/Training(Step): loss,1610350397575,0.18534162640571594
6704,-Metrics/Training(Step): loss,1610350399643,0.15679465234279633
6706,-Metrics/Training(Step): loss,1610350401663,0.16780826449394226
6708,-Metrics/Training(Step): loss,1610350403722,0.15127147734165192
6710,-Metrics/Training(Step): loss,1610350416237,0.1380641758441925
6712,-Metrics/Training(Step): loss,1610350420322,0.1207759827375412
6714,-Metrics/Training(Step): loss,1610350424220,0.16445523500442505
6716,-Metrics/Training(Step): loss,1610350428020,0.1405968964099884
6718,-Metrics/Training(Step): loss,1610350431820,0.19125933945178986
6720,-Metrics/Training(Step): loss,1610350435839,0.1450812667608261
6722,-Metrics/Training(Step): loss,1610350439420,0.17038097977638245
6724,-Metrics/Training(Step): loss,1610350443020,0.17292621731758118
6726,-Metrics/Training(Step): loss,1610350446846,0.15713338553905487
6728,-Metrics/Training(Step): loss,1610350450416,0.14394204318523407
6730,-Metrics/Training(Step): loss,1610350453922,0.1723431497812271
6732,-Metrics/Training(Step): loss,1610350457898,0.16328276693820953
6734,-Metrics/Training(Step): loss,1610350461229,0.14313475787639618
6736,-Metrics/Training(Step): loss,1610350465119,0.1481764167547226
6738,-Metrics/Training(Step): loss,1610350468357,0.15136682987213135
6740,-Metrics/Training(Step): loss,1610350471434,0.11684765666723251
6742,-Metrics/Training(Step): loss,1610350474831,0.17687638103961945
6744,-Metrics/Training(Step): loss,1610350478922,0.15626023709774017
6746,-Metrics/Training(Step): loss,1610350482702,0.18488270044326782
6748,-Metrics/Training(Step): loss,1610350486032,0.18710386753082275
6750,-Metrics/Training(Step): loss,1610350489361,0.14806224405765533
6752,-Metrics/Training(Step): loss,1610350492923,0.17158618569374084
6754,-Metrics/Training(Step): loss,1610350496838,0.14871707558631897
6756,-Metrics/Training(Step): loss,1610350499584,0.23366692662239075
6758,-Metrics/Training(Step): loss,1610350501961,0.1637173593044281
6760,-Metrics/Training(Step): loss,1610350504292,0.1768394112586975
6762,-Metrics/Training(Step): loss,1610350506251,0.18514856696128845
6764,-Metrics/Training(Step): loss,1610350508370,0.15576162934303284
6766,-Metrics/Training(Step): loss,1610350510450,0.16591250896453857
6768,-Metrics/Training(Step): loss,1610350512560,0.18088985979557037
6770,-Metrics/Training(Step): loss,1610350514644,0.1515151858329773
6772,-Metrics/Training(Step): loss,1610350516719,0.1633274108171463
6774,-Metrics/Training(Step): loss,1610350518743,0.15917189419269562
6776,-Metrics/Training(Step): loss,1610350520608,0.17743933200836182
6778,-Metrics/Training(Step): loss,1610350522528,0.12274228036403656
6780,-Metrics/Training(Step): loss,1610350524476,0.13383235037326813
6782,-Metrics/Training(Step): loss,1610350526490,0.16751836240291595
6784,-Metrics/Training(Step): loss,1610350528550,0.16994455456733704
6786,-Metrics/Training(Step): loss,1610350530583,0.11693219840526581
6788,-Metrics/Training(Step): loss,1610350532447,0.1421644687652588
6790,-Metrics/Training(Step): loss,1610350534539,0.1460382342338562
6792,-Metrics/Training(Step): loss,1610350536594,0.20752005279064178
6794,-Metrics/Training(Step): loss,1610350538653,0.1297602653503418
6796,-Metrics/Training(Step): loss,1610350552026,0.17240877449512482
6798,-Metrics/Training(Step): loss,1610350555755,0.1945590078830719
6800,-Metrics/Training(Step): loss,1610350559819,0.1583053320646286
6802,-Metrics/Training(Step): loss,1610350563927,0.16431911289691925
6804,-Metrics/Training(Step): loss,1610350568052,0.18736153841018677
6806,-Metrics/Training(Step): loss,1610350572226,0.13652953505516052
6808,-Metrics/Training(Step): loss,1610350576220,0.1543685495853424
6810,-Metrics/Training(Step): loss,1610350579952,0.1688113659620285
6812,-Metrics/Training(Step): loss,1610350583417,0.15442702174186707
6814,-Metrics/Training(Step): loss,1610350586740,0.1562034785747528
6816,-Metrics/Training(Step): loss,1610350590288,0.18328464031219482
6818,-Metrics/Training(Step): loss,1610350594325,0.15755867958068848
6820,-Metrics/Training(Step): loss,1610350597698,0.1742815375328064
6822,-Metrics/Training(Step): loss,1610350601670,0.1563923954963684
6824,-Metrics/Training(Step): loss,1610350605284,0.15841111540794373
6826,-Metrics/Training(Step): loss,1610350608859,0.1615932583808899
6828,-Metrics/Training(Step): loss,1610350612432,0.1505136638879776
6830,-Metrics/Training(Step): loss,1610350616120,0.19457775354385376
6832,-Metrics/Training(Step): loss,1610350619345,0.16370651125907898
6834,-Metrics/Training(Step): loss,1610350623020,0.1644328534603119
6836,-Metrics/Training(Step): loss,1610350626521,0.12968315184116364
6838,-Metrics/Training(Step): loss,1610350630358,0.14057712256908417
6840,-Metrics/Training(Step): loss,1610350633628,0.17895251512527466
6842,-Metrics/Training(Step): loss,1610350635851,0.15512734651565552
6844,-Metrics/Training(Step): loss,1610350637946,0.11850614100694656
6846,-Metrics/Training(Step): loss,1610350640156,0.16380394995212555
6848,-Metrics/Training(Step): loss,1610350642211,0.12406326085329056
6850,-Metrics/Training(Step): loss,1610350644188,0.15737570822238922
6852,-Metrics/Training(Step): loss,1610350646165,0.16277223825454712
6854,-Metrics/Training(Step): loss,1610350648285,0.16003336012363434
6856,-Metrics/Training(Step): loss,1610350650412,0.17570605874061584
6858,-Metrics/Training(Step): loss,1610350652450,0.1791955828666687
6860,-Metrics/Training(Step): loss,1610350654479,0.16932210326194763
6862,-Metrics/Training(Step): loss,1610350656651,0.18590347468852997
6864,-Metrics/Training(Step): loss,1610350658776,0.16599184274673462
6866,-Metrics/Training(Step): loss,1610350660771,0.156147301197052
6868,-Metrics/Training(Step): loss,1610350662596,0.15022976696491241
6870,-Metrics/Training(Step): loss,1610350664489,0.16791094839572906
6872,-Metrics/Training(Step): loss,1610350666601,0.1546686887741089
6874,-Metrics/Training(Step): loss,1610350668615,0.1387573927640915
6876,-Metrics/Training(Step): loss,1610350670550,0.17181192338466644
6878,-Metrics/Training(Step): loss,1610350672615,0.15943147242069244
6880,-Metrics/Training(Step): loss,1610350674709,0.18598179519176483
6882,-Metrics/Training(Step): loss,1610350688129,0.1679961383342743
6884,-Metrics/Training(Step): loss,1610350692520,0.17032292485237122
6886,-Metrics/Training(Step): loss,1610350696115,0.156539648771286
6888,-Metrics/Training(Step): loss,1610350700015,0.1739642322063446
6890,-Metrics/Training(Step): loss,1610350703916,0.1582125574350357
6892,-Metrics/Training(Step): loss,1610350707821,0.12102190405130386
6894,-Metrics/Training(Step): loss,1610350711519,0.13766951858997345
6896,-Metrics/Training(Step): loss,1610350715620,0.1801532357931137
6898,-Metrics/Training(Step): loss,1610350719315,0.16684283316135406
6900,-Metrics/Training(Step): loss,1610350723020,0.1770515739917755
6902,-Metrics/Training(Step): loss,1610350726210,0.13288244605064392
6904,-Metrics/Training(Step): loss,1610350729920,0.1337059587240219
6906,-Metrics/Training(Step): loss,1610350733320,0.20214851200580597
6908,-Metrics/Training(Step): loss,1610350736759,0.17371131479740143
6910,-Metrics/Training(Step): loss,1610350740530,0.16103072464466095
6912,-Metrics/Training(Step): loss,1610350744017,0.2013179361820221
6914,-Metrics/Training(Step): loss,1610350747515,0.1420823633670807
6916,-Metrics/Training(Step): loss,1610350751119,0.17315222322940826
6918,-Metrics/Training(Step): loss,1610350754718,0.17221763730049133
6920,-Metrics/Training(Step): loss,1610350758119,0.1245150938630104
6922,-Metrics/Training(Step): loss,1610350761655,0.17041386663913727
6924,-Metrics/Training(Step): loss,1610350764912,0.1503361016511917
6926,-Metrics/Training(Step): loss,1610350768105,0.17225585877895355
6928,-Metrics/Training(Step): loss,1610350771643,0.15076300501823425
6930,-Metrics/Training(Step): loss,1610350774209,0.14200353622436523
6932,-Metrics/Training(Step): loss,1610350776602,0.14903636276721954
6934,-Metrics/Training(Step): loss,1610350778746,0.17757950723171234
6936,-Metrics/Training(Step): loss,1610350780833,0.1516888588666916
6938,-Metrics/Training(Step): loss,1610350782938,0.1768244057893753
6940,-Metrics/Training(Step): loss,1610350785077,0.17112357914447784
6942,-Metrics/Training(Step): loss,1610350787195,0.14053143560886383
6944,-Metrics/Training(Step): loss,1610350789250,0.11804500967264175
6946,-Metrics/Training(Step): loss,1610350791267,0.15354397892951965
6948,-Metrics/Training(Step): loss,1610350793328,0.14466185867786407
6950,-Metrics/Training(Step): loss,1610350795410,0.1651454120874405
6952,-Metrics/Training(Step): loss,1610350797489,0.18696801364421844
6954,-Metrics/Training(Step): loss,1610350799396,0.16534316539764404
6956,-Metrics/Training(Step): loss,1610350801380,0.121217280626297
6958,-Metrics/Training(Step): loss,1610350803396,0.16123084723949432
6960,-Metrics/Training(Step): loss,1610350805490,0.14127418398857117
6962,-Metrics/Training(Step): loss,1610350807540,0.164875790476799
6964,-Metrics/Training(Step): loss,1610350809560,0.1566619873046875
6966,-Metrics/Training(Step): loss,1610350811631,0.17642343044281006
6968,-Metrics/Training(Step): loss,1610350825622,0.1274569183588028
6970,-Metrics/Training(Step): loss,1610350829632,0.1611384004354477
6972,-Metrics/Training(Step): loss,1610350833620,0.17159734666347504
6974,-Metrics/Training(Step): loss,1610350837532,0.12442052364349365
6976,-Metrics/Training(Step): loss,1610350841530,0.13680745661258698
6978,-Metrics/Training(Step): loss,1610350845516,0.13338105380535126
6980,-Metrics/Training(Step): loss,1610350848920,0.15316390991210938
6982,-Metrics/Training(Step): loss,1610350852327,0.1590266078710556
6984,-Metrics/Training(Step): loss,1610350856054,0.17326098680496216
6986,-Metrics/Training(Step): loss,1610350859520,0.15152934193611145
6988,-Metrics/Training(Step): loss,1610350863419,0.12556008994579315
6990,-Metrics/Training(Step): loss,1610350867160,0.14200624823570251
6992,-Metrics/Training(Step): loss,1610350871078,0.19460207223892212
6994,-Metrics/Training(Step): loss,1610350874936,0.1952596753835678
6996,-Metrics/Training(Step): loss,1610350878621,0.1495664119720459
6998,-Metrics/Training(Step): loss,1610350882414,0.14630891382694244
7000,-Metrics/Training(Step): loss,1610350885321,0.1514120250940323
7002,-Metrics/Training(Step): loss,1610350888619,0.17608234286308289
7004,-Metrics/Training(Step): loss,1610350892331,0.157155379652977
7006,-Metrics/Training(Step): loss,1610350896369,0.12011048942804337
7008,-Metrics/Training(Step): loss,1610350899867,0.20360501110553741
7010,-Metrics/Training(Step): loss,1610350903722,0.14099295437335968
7012,-Metrics/Training(Step): loss,1610350907102,0.17715971171855927
7014,-Metrics/Training(Step): loss,1610350909700,0.12453112751245499
7016,-Metrics/Training(Step): loss,1610350911812,0.12832143902778625
7018,-Metrics/Training(Step): loss,1610350913779,0.17976053059101105
7020,-Metrics/Training(Step): loss,1610350915800,0.18295405805110931
7022,-Metrics/Training(Step): loss,1610350917888,0.19251732528209686
7024,-Metrics/Training(Step): loss,1610350919990,0.15668387711048126
7026,-Metrics/Training(Step): loss,1610350922080,0.16939038038253784
7028,-Metrics/Training(Step): loss,1610350924150,0.15664580464363098
7030,-Metrics/Training(Step): loss,1610350926071,0.15434253215789795
7032,-Metrics/Training(Step): loss,1610350928163,0.15806427597999573
7034,-Metrics/Training(Step): loss,1610350930240,0.1662674844264984
7036,-Metrics/Training(Step): loss,1610350932289,0.17338381707668304
7038,-Metrics/Training(Step): loss,1610350934078,0.131363645195961
7040,-Metrics/Training(Step): loss,1610350936126,0.1463739424943924
7042,-Metrics/Training(Step): loss,1610350938229,0.19113917648792267
7044,-Metrics/Training(Step): loss,1610350940230,0.14294791221618652
7046,-Metrics/Training(Step): loss,1610350942355,0.18080952763557434
7048,-Metrics/Training(Step): loss,1610350944417,0.13980959355831146
7050,-Metrics/Training(Step): loss,1610350946499,0.16257518529891968
7052,-Metrics/Training(Step): loss,1610350948575,0.16001825034618378
7054,-Metrics/Training(Step): loss,1610350961627,0.1555926352739334
7056,-Metrics/Training(Step): loss,1610350965822,0.2027953714132309
7058,-Metrics/Training(Step): loss,1610350969920,0.20264071226119995
7060,-Metrics/Training(Step): loss,1610350973757,0.14713916182518005
7062,-Metrics/Training(Step): loss,1610350977820,0.16561029851436615
7064,-Metrics/Training(Step): loss,1610350981420,0.1234765574336052
7066,-Metrics/Training(Step): loss,1610350985417,0.18601347506046295
7068,-Metrics/Training(Step): loss,1610350989319,0.1653432548046112
7070,-Metrics/Training(Step): loss,1610350993020,0.13461636006832123
7072,-Metrics/Training(Step): loss,1610350996421,0.18593265116214752
7074,-Metrics/Training(Step): loss,1610351000118,0.13767893612384796
7076,-Metrics/Training(Step): loss,1610351004019,0.150979146361351
7078,-Metrics/Training(Step): loss,1610351007420,0.14795583486557007
7080,-Metrics/Training(Step): loss,1610351011020,0.24920052289962769
7082,-Metrics/Training(Step): loss,1610351014544,0.1945771425962448
7084,-Metrics/Training(Step): loss,1610351018255,0.1502559334039688
7086,-Metrics/Training(Step): loss,1610351021631,0.16455264389514923
7088,-Metrics/Training(Step): loss,1610351024820,0.12239475548267365
7090,-Metrics/Training(Step): loss,1610351028457,0.17411276698112488
7092,-Metrics/Training(Step): loss,1610351031720,0.14944182336330414
7094,-Metrics/Training(Step): loss,1610351035521,0.1615525186061859
7096,-Metrics/Training(Step): loss,1610351039095,0.17982430756092072
7098,-Metrics/Training(Step): loss,1610351042330,0.1721848100423813
7100,-Metrics/Training(Step): loss,1610351045545,0.1663939207792282
7102,-Metrics/Training(Step): loss,1610351047969,0.2527600824832916
7104,-Metrics/Training(Step): loss,1610351050063,0.12480337172746658
7106,-Metrics/Training(Step): loss,1610351052134,0.1507258266210556
7108,-Metrics/Training(Step): loss,1610351054259,0.15743665397167206
7110,-Metrics/Training(Step): loss,1610351056295,0.1688697338104248
7112,-Metrics/Training(Step): loss,1610351058399,0.15969163179397583
7114,-Metrics/Training(Step): loss,1610351060510,0.1448080688714981
7116,-Metrics/Training(Step): loss,1610351062472,0.1440449357032776
7118,-Metrics/Training(Step): loss,1610351064552,0.17757917940616608
7120,-Metrics/Training(Step): loss,1610351066597,0.17198367416858673
7122,-Metrics/Training(Step): loss,1610351068641,0.142817884683609
7124,-Metrics/Training(Step): loss,1610351070723,0.14320702850818634
7126,-Metrics/Training(Step): loss,1610351072704,0.17794401943683624
7128,-Metrics/Training(Step): loss,1610351074590,0.11690982431173325
7130,-Metrics/Training(Step): loss,1610351076528,0.1978059709072113
7132,-Metrics/Training(Step): loss,1610351078570,0.13687923550605774
7134,-Metrics/Training(Step): loss,1610351080479,0.17551271617412567
7136,-Metrics/Training(Step): loss,1610351082454,0.15469345450401306
7138,-Metrics/Training(Step): loss,1610351084513,0.14112022519111633
7140,-Metrics/Training(Step): loss,1610351096926,0.15719754993915558
7142,-Metrics/Training(Step): loss,1610351100915,0.18392109870910645
7144,-Metrics/Training(Step): loss,1610351105027,0.16347388923168182
7146,-Metrics/Training(Step): loss,1610351109029,0.1779998391866684
7148,-Metrics/Training(Step): loss,1610351112947,0.13565492630004883
7150,-Metrics/Training(Step): loss,1610351117024,0.14726188778877258
7152,-Metrics/Training(Step): loss,1610351120669,0.19089198112487793
7154,-Metrics/Training(Step): loss,1610351124205,0.1418788731098175
7156,-Metrics/Training(Step): loss,1610351128033,0.18147216737270355
7158,-Metrics/Training(Step): loss,1610351132052,0.1447741687297821
7160,-Metrics/Training(Step): loss,1610351135974,0.16726058721542358
7162,-Metrics/Training(Step): loss,1610351139433,0.1601114124059677
7164,-Metrics/Training(Step): loss,1610351142757,0.1257786899805069
7166,-Metrics/Training(Step): loss,1610351146519,0.17722514271736145
7168,-Metrics/Training(Step): loss,1610351149822,0.10561015456914902
7170,-Metrics/Training(Step): loss,1610351153366,0.15938255190849304
7172,-Metrics/Training(Step): loss,1610351156360,0.12862597405910492
7174,-Metrics/Training(Step): loss,1610351159658,0.1708974838256836
7176,-Metrics/Training(Step): loss,1610351163420,0.12746764719486237
7178,-Metrics/Training(Step): loss,1610351167330,0.13319125771522522
7180,-Metrics/Training(Step): loss,1610351170890,0.13720795512199402
7182,-Metrics/Training(Step): loss,1610351174820,0.12264912575483322
7184,-Metrics/Training(Step): loss,1610351178517,0.16611243784427643
7186,-Metrics/Training(Step): loss,1610351181363,0.13515347242355347
7188,-Metrics/Training(Step): loss,1610351183408,0.14718316495418549
7190,-Metrics/Training(Step): loss,1610351185508,0.1888028383255005
7192,-Metrics/Training(Step): loss,1610351187561,0.1534494161605835
7194,-Metrics/Training(Step): loss,1610351189507,0.14144010841846466
7196,-Metrics/Training(Step): loss,1610351191577,0.12890931963920593
7198,-Metrics/Training(Step): loss,1610351193592,0.16332502663135529
7200,-Metrics/Training(Step): loss,1610351195595,0.170230895280838
7202,-Metrics/Training(Step): loss,1610351197640,0.14890329539775848
7204,-Metrics/Training(Step): loss,1610351199544,0.15146319568157196
7206,-Metrics/Training(Step): loss,1610351201460,0.15876390039920807
7208,-Metrics/Training(Step): loss,1610351203447,0.1585806906223297
7210,-Metrics/Training(Step): loss,1610351205440,0.14074541628360748
7212,-Metrics/Training(Step): loss,1610351207417,0.18518701195716858
7214,-Metrics/Training(Step): loss,1610351209353,0.15523186326026917
7216,-Metrics/Training(Step): loss,1610351211357,0.18521936237812042
7218,-Metrics/Training(Step): loss,1610351213408,0.15302763879299164
7220,-Metrics/Training(Step): loss,1610351215459,0.20599763095378876
7222,-Metrics/Training(Step): loss,1610351217498,0.1693589985370636
7224,-Metrics/Training(Step): loss,1610351219557,0.17664453387260437
7226,-Metrics/Training(Step): loss,1610351232126,0.16195549070835114
7228,-Metrics/Training(Step): loss,1610351236215,0.1277400553226471
7230,-Metrics/Training(Step): loss,1610351240320,0.17939507961273193
7232,-Metrics/Training(Step): loss,1610351244729,0.13996201753616333
7234,-Metrics/Training(Step): loss,1610351248644,0.1305309534072876
7236,-Metrics/Training(Step): loss,1610351252739,0.17971093952655792
7238,-Metrics/Training(Step): loss,1610351257219,0.19217441976070404
7240,-Metrics/Training(Step): loss,1610351261023,0.12827149033546448
7242,-Metrics/Training(Step): loss,1610351264620,0.2225853055715561
7244,-Metrics/Training(Step): loss,1610351267858,0.145223468542099
7246,-Metrics/Training(Step): loss,1610351270739,0.1516112983226776
7248,-Metrics/Training(Step): loss,1610351274496,0.1454956829547882
7250,-Metrics/Training(Step): loss,1610351277545,0.11493870615959167
7252,-Metrics/Training(Step): loss,1610351280832,0.15676669776439667
7254,-Metrics/Training(Step): loss,1610351284443,0.15059205889701843
7256,-Metrics/Training(Step): loss,1610351288271,0.16967405378818512
7258,-Metrics/Training(Step): loss,1610351291844,0.15576906502246857
7260,-Metrics/Training(Step): loss,1610351295120,0.152663916349411
7262,-Metrics/Training(Step): loss,1610351298789,0.177602156996727
7264,-Metrics/Training(Step): loss,1610351301917,0.1740100234746933
7266,-Metrics/Training(Step): loss,1610351305020,0.18852286040782928
7268,-Metrics/Training(Step): loss,1610351308437,0.14672018587589264
7270,-Metrics/Training(Step): loss,1610351312100,0.1385684609413147
7272,-Metrics/Training(Step): loss,1610351315759,0.1632785201072693
7274,-Metrics/Training(Step): loss,1610351318189,0.13175508379936218
7276,-Metrics/Training(Step): loss,1610351320235,0.20726796984672546
7278,-Metrics/Training(Step): loss,1610351322434,0.13400068879127502
7280,-Metrics/Training(Step): loss,1610351324411,0.21330387890338898
7282,-Metrics/Training(Step): loss,1610351326501,0.15101774036884308
7284,-Metrics/Training(Step): loss,1610351328628,0.1144857108592987
7286,-Metrics/Training(Step): loss,1610351330755,0.14350351691246033
7288,-Metrics/Training(Step): loss,1610351332854,0.1461845487356186
7290,-Metrics/Training(Step): loss,1610351334953,0.11645589023828506
7292,-Metrics/Training(Step): loss,1610351337057,0.200630784034729
7294,-Metrics/Training(Step): loss,1610351339196,0.14495305716991425
7296,-Metrics/Training(Step): loss,1610351341089,0.16804495453834534
7298,-Metrics/Training(Step): loss,1610351343003,0.14235875010490417
7300,-Metrics/Training(Step): loss,1610351345116,0.12965811789035797
7302,-Metrics/Training(Step): loss,1610351347137,0.1541597992181778
7304,-Metrics/Training(Step): loss,1610351349120,0.13563388586044312
7306,-Metrics/Training(Step): loss,1610351350970,0.1260518878698349
7308,-Metrics/Training(Step): loss,1610351352994,0.17953574657440186
7310,-Metrics/Training(Step): loss,1610351355060,0.1647062748670578
7312,-Metrics/Training(Step): loss,1610351368630,0.14774291217327118
7314,-Metrics/Training(Step): loss,1610351372923,0.12753482162952423
7316,-Metrics/Training(Step): loss,1610351376919,0.138894721865654
7318,-Metrics/Training(Step): loss,1610351380522,0.16366882622241974
7320,-Metrics/Training(Step): loss,1610351384540,0.1550387740135193
7322,-Metrics/Training(Step): loss,1610351388520,0.12615624070167542
7324,-Metrics/Training(Step): loss,1610351392536,0.17202022671699524
7326,-Metrics/Training(Step): loss,1610351396515,0.17280663549900055
7328,-Metrics/Training(Step): loss,1610351400316,0.16168710589408875
7330,-Metrics/Training(Step): loss,1610351403850,0.13065163791179657
7332,-Metrics/Training(Step): loss,1610351407912,0.1403428167104721
7334,-Metrics/Training(Step): loss,1610351411619,0.17271676659584045
7336,-Metrics/Training(Step): loss,1610351415138,0.15544834733009338
7338,-Metrics/Training(Step): loss,1610351419207,0.18590593338012695
7340,-Metrics/Training(Step): loss,1610351422946,0.16112837195396423
7342,-Metrics/Training(Step): loss,1610351426675,0.16080184280872345
7344,-Metrics/Training(Step): loss,1610351430520,0.1864185631275177
7346,-Metrics/Training(Step): loss,1610351433866,0.14503678679466248
7348,-Metrics/Training(Step): loss,1610351436919,0.15279263257980347
7350,-Metrics/Training(Step): loss,1610351441055,0.14952918887138367
7352,-Metrics/Training(Step): loss,1610351444419,0.1621224582195282
7354,-Metrics/Training(Step): loss,1610351448209,0.16945040225982666
7356,-Metrics/Training(Step): loss,1610351450976,0.16888530552387238
7358,-Metrics/Training(Step): loss,1610351453290,0.15859182178974152
7360,-Metrics/Training(Step): loss,1610351455347,0.20931267738342285
7362,-Metrics/Training(Step): loss,1610351457261,0.17035503685474396
7364,-Metrics/Training(Step): loss,1610351459298,0.17920659482479095
7366,-Metrics/Training(Step): loss,1610351461393,0.16823086142539978
7368,-Metrics/Training(Step): loss,1610351463465,0.16145683825016022
7370,-Metrics/Training(Step): loss,1610351465454,0.1871417611837387
7372,-Metrics/Training(Step): loss,1610351467579,0.16688703000545502
7374,-Metrics/Training(Step): loss,1610351469323,0.20103250443935394
7376,-Metrics/Training(Step): loss,1610351471290,0.20794877409934998
7378,-Metrics/Training(Step): loss,1610351473360,0.1698591560125351
7380,-Metrics/Training(Step): loss,1610351475306,0.22071635723114014
7382,-Metrics/Training(Step): loss,1610351477348,0.23784829676151276
7384,-Metrics/Training(Step): loss,1610351479338,0.20375218987464905
7386,-Metrics/Training(Step): loss,1610351481351,0.2070491909980774
7388,-Metrics/Training(Step): loss,1610351483434,0.20809592306613922
7390,-Metrics/Training(Step): loss,1610351485479,0.22645293176174164
7392,-Metrics/Training(Step): loss,1610351487367,0.18865008652210236
7394,-Metrics/Training(Step): loss,1610351489391,0.17564192414283752
7396,-Metrics/Training(Step): loss,1610351491299,0.18029451370239258
7398,-Metrics/Training(Step): loss,1610351503656,0.14972145855426788
7400,-Metrics/Training(Step): loss,1610351507920,0.1738305240869522
7402,-Metrics/Training(Step): loss,1610351511958,0.17107431590557098
7404,-Metrics/Training(Step): loss,1610351515819,0.18825432658195496
7406,-Metrics/Training(Step): loss,1610351519620,0.17423902451992035
7408,-Metrics/Training(Step): loss,1610351524049,0.14744938910007477
7410,-Metrics/Training(Step): loss,1610351527925,0.2123059630393982
7412,-Metrics/Training(Step): loss,1610351531753,0.14478877186775208
7414,-Metrics/Training(Step): loss,1610351535944,0.17356957495212555
7416,-Metrics/Training(Step): loss,1610351539620,0.2017463892698288
7418,-Metrics/Training(Step): loss,1610351543315,0.17993691563606262
7420,-Metrics/Training(Step): loss,1610351546878,0.18113209307193756
7422,-Metrics/Training(Step): loss,1610351550541,0.1606750339269638
7424,-Metrics/Training(Step): loss,1610351554851,0.17189563810825348
7426,-Metrics/Training(Step): loss,1610351558682,0.16531860828399658
7428,-Metrics/Training(Step): loss,1610351562232,0.16188693046569824
7430,-Metrics/Training(Step): loss,1610351566326,0.15499813854694366
7432,-Metrics/Training(Step): loss,1610351570492,0.157312273979187
7434,-Metrics/Training(Step): loss,1610351574335,0.13193170726299286
7436,-Metrics/Training(Step): loss,1610351578620,0.15983185172080994
7438,-Metrics/Training(Step): loss,1610351582511,0.15305690467357635
7440,-Metrics/Training(Step): loss,1610351585919,0.16883647441864014
7442,-Metrics/Training(Step): loss,1610351588935,0.12986871600151062
7444,-Metrics/Training(Step): loss,1610351591026,0.11787236481904984
7446,-Metrics/Training(Step): loss,1610351593204,0.17726516723632812
7448,-Metrics/Training(Step): loss,1610351595327,0.16951602697372437
7450,-Metrics/Training(Step): loss,1610351597427,0.18007010221481323
7452,-Metrics/Training(Step): loss,1610351599561,0.21010830998420715
7454,-Metrics/Training(Step): loss,1610351601666,0.19043900072574615
7456,-Metrics/Training(Step): loss,1610351603745,0.130634605884552
7458,-Metrics/Training(Step): loss,1610351605806,0.15886931121349335
7460,-Metrics/Training(Step): loss,1610351607865,0.14740735292434692
7462,-Metrics/Training(Step): loss,1610351609980,0.1637890487909317
7464,-Metrics/Training(Step): loss,1610351612077,0.1675238460302353
7466,-Metrics/Training(Step): loss,1610351614216,0.16092555224895477
7468,-Metrics/Training(Step): loss,1610351615973,0.12790821492671967
7470,-Metrics/Training(Step): loss,1610351617965,0.1643265187740326
7472,-Metrics/Training(Step): loss,1610351619971,0.15023837983608246
7474,-Metrics/Training(Step): loss,1610351621966,0.19033321738243103
7476,-Metrics/Training(Step): loss,1610351623839,0.1730908453464508
7478,-Metrics/Training(Step): loss,1610351625836,0.12352229654788971
7480,-Metrics/Training(Step): loss,1610351627925,0.17653337121009827
7482,-Metrics/Training(Step): loss,1610351629958,0.1620902717113495
7484,-Metrics/Training(Step): loss,1610351642426,0.15678781270980835
7486,-Metrics/Training(Step): loss,1610351646520,0.14241880178451538
7488,-Metrics/Training(Step): loss,1610351650222,0.158845916390419
7490,-Metrics/Training(Step): loss,1610351653915,0.1856827735900879
7492,-Metrics/Training(Step): loss,1610351657948,0.13755030930042267
7494,-Metrics/Training(Step): loss,1610351662020,0.13118600845336914
7496,-Metrics/Training(Step): loss,1610351666151,0.12890006601810455
7498,-Metrics/Training(Step): loss,1610351669719,0.1913512647151947
7500,-Metrics/Training(Step): loss,1610351673279,0.1494215577840805
7502,-Metrics/Training(Step): loss,1610351676524,0.17251993715763092
7504,-Metrics/Training(Step): loss,1610351679922,0.150252103805542
7506,-Metrics/Training(Step): loss,1610351683390,0.17474831640720367
7508,-Metrics/Training(Step): loss,1610351686925,0.1483103334903717
7510,-Metrics/Training(Step): loss,1610351690721,0.1258932650089264
7512,-Metrics/Training(Step): loss,1610351694315,0.1365153193473816
7514,-Metrics/Training(Step): loss,1610351698133,0.16723406314849854
7516,-Metrics/Training(Step): loss,1610351701444,0.1505948156118393
7518,-Metrics/Training(Step): loss,1610351705246,0.14949095249176025
7520,-Metrics/Training(Step): loss,1610351708829,0.17146794497966766
7522,-Metrics/Training(Step): loss,1610351712352,0.1234104186296463
7524,-Metrics/Training(Step): loss,1610351715922,0.15750785171985626
7526,-Metrics/Training(Step): loss,1610351719744,0.14318007230758667
7528,-Metrics/Training(Step): loss,1610351723195,0.13762778043746948
7530,-Metrics/Training(Step): loss,1610351726649,0.15577653050422668
7532,-Metrics/Training(Step): loss,1610351729292,0.15140870213508606
7534,-Metrics/Training(Step): loss,1610351731362,0.1537812501192093
7536,-Metrics/Training(Step): loss,1610351733467,0.13435351848602295
7538,-Metrics/Training(Step): loss,1610351735571,0.1331934779882431
7540,-Metrics/Training(Step): loss,1610351737694,0.12840625643730164
7542,-Metrics/Training(Step): loss,1610351739781,0.1444493979215622
7544,-Metrics/Training(Step): loss,1610351741780,0.14940722286701202
7546,-Metrics/Training(Step): loss,1610351743827,0.14770996570587158
7548,-Metrics/Training(Step): loss,1610351745906,0.14170511066913605
7550,-Metrics/Training(Step): loss,1610351748007,0.17366965115070343
7552,-Metrics/Training(Step): loss,1610351750103,0.17119480669498444
7554,-Metrics/Training(Step): loss,1610351752182,0.19025687873363495
7556,-Metrics/Training(Step): loss,1610351754197,0.12850068509578705
7558,-Metrics/Training(Step): loss,1610351756169,0.1492435783147812
7560,-Metrics/Training(Step): loss,1610351758205,0.17267808318138123
7562,-Metrics/Training(Step): loss,1610351760173,0.13260789215564728
7564,-Metrics/Training(Step): loss,1610351762193,0.12980185449123383
7566,-Metrics/Training(Step): loss,1610351764216,0.13194946944713593
7568,-Metrics/Training(Step): loss,1610351766237,0.16073894500732422
7570,-Metrics/Training(Step): loss,1610351779125,0.17751210927963257
7572,-Metrics/Training(Step): loss,1610351783324,0.17082428932189941
7574,-Metrics/Training(Step): loss,1610351787220,0.17531748116016388
7576,-Metrics/Training(Step): loss,1610351790920,0.15209472179412842
7578,-Metrics/Training(Step): loss,1610351795537,0.14334401488304138
7580,-Metrics/Training(Step): loss,1610351799815,0.14466099441051483
7582,-Metrics/Training(Step): loss,1610351803854,0.15020877122879028
7584,-Metrics/Training(Step): loss,1610351807687,0.17509430646896362
7586,-Metrics/Training(Step): loss,1610351811089,0.18319107592105865
7588,-Metrics/Training(Step): loss,1610351814601,0.13555148243904114
7590,-Metrics/Training(Step): loss,1610351818278,0.13978426158428192
7592,-Metrics/Training(Step): loss,1610351821807,0.16813711822032928
7594,-Metrics/Training(Step): loss,1610351825136,0.15638618171215057
7596,-Metrics/Training(Step): loss,1610351828820,0.2131451964378357
7598,-Metrics/Training(Step): loss,1610351832646,0.16330000758171082
7600,-Metrics/Training(Step): loss,1610351836220,0.15487442910671234
7602,-Metrics/Training(Step): loss,1610351839520,0.13721606135368347
7604,-Metrics/Training(Step): loss,1610351842714,0.17246021330356598
7606,-Metrics/Training(Step): loss,1610351846394,0.14291532337665558
7608,-Metrics/Training(Step): loss,1610351849709,0.15157368779182434
7610,-Metrics/Training(Step): loss,1610351853747,0.1764698475599289
7612,-Metrics/Training(Step): loss,1610351856845,0.17495974898338318
7614,-Metrics/Training(Step): loss,1610351860044,0.1433354765176773
7616,-Metrics/Training(Step): loss,1610351862792,0.1073230430483818
7618,-Metrics/Training(Step): loss,1610351865026,0.10844556987285614
7620,-Metrics/Training(Step): loss,1610351867271,0.14069126546382904
7622,-Metrics/Training(Step): loss,1610351869266,0.1399836540222168
7624,-Metrics/Training(Step): loss,1610351871341,0.13656651973724365
7626,-Metrics/Training(Step): loss,1610351873442,0.1296515166759491
7628,-Metrics/Training(Step): loss,1610351875532,0.10996462404727936
7630,-Metrics/Training(Step): loss,1610351877604,0.12281295657157898
7632,-Metrics/Training(Step): loss,1610351879654,0.163550466299057
7634,-Metrics/Training(Step): loss,1610351881604,0.15144596993923187
7636,-Metrics/Training(Step): loss,1610351883626,0.11811281740665436
7638,-Metrics/Training(Step): loss,1610351885670,0.13938592374324799
7640,-Metrics/Training(Step): loss,1610351887649,0.14727629721164703
7642,-Metrics/Training(Step): loss,1610351889656,0.12544184923171997
7644,-Metrics/Training(Step): loss,1610351891737,0.16690091788768768
7646,-Metrics/Training(Step): loss,1610351893752,0.1328389048576355
7648,-Metrics/Training(Step): loss,1610351895612,0.09810706228017807
7650,-Metrics/Training(Step): loss,1610351897655,0.13843828439712524
7652,-Metrics/Training(Step): loss,1610351899690,0.1386096328496933
7654,-Metrics/Training(Step): loss,1610351901748,0.14681796729564667
7656,-Metrics/Training(Step): loss,1610351914340,0.15054838359355927
7658,-Metrics/Training(Step): loss,1610351918224,0.13747692108154297
7660,-Metrics/Training(Step): loss,1610351921919,0.12381725758314133
7662,-Metrics/Training(Step): loss,1610351925719,0.16145989298820496
7664,-Metrics/Training(Step): loss,1610351929718,0.1470511555671692
7666,-Metrics/Training(Step): loss,1610351933520,0.14531321823596954
7668,-Metrics/Training(Step): loss,1610351937520,0.15365663170814514
7670,-Metrics/Training(Step): loss,1610351941043,0.1684589982032776
7672,-Metrics/Training(Step): loss,1610351944945,0.20030289888381958
7674,-Metrics/Training(Step): loss,1610351948320,0.1502448469400406
7676,-Metrics/Training(Step): loss,1610351951423,0.1370742917060852
7678,-Metrics/Training(Step): loss,1610351955120,0.15336479246616364
7680,-Metrics/Training(Step): loss,1610351958772,0.13498441874980927
7682,-Metrics/Training(Step): loss,1610351962444,0.15452495217323303
7684,-Metrics/Training(Step): loss,1610351965937,0.14434601366519928
7686,-Metrics/Training(Step): loss,1610351969224,0.13646365702152252
7688,-Metrics/Training(Step): loss,1610351972146,0.15969818830490112
7690,-Metrics/Training(Step): loss,1610351975947,0.15414753556251526
7692,-Metrics/Training(Step): loss,1610351979732,0.16850139200687408
7694,-Metrics/Training(Step): loss,1610351983419,0.18955901265144348
7696,-Metrics/Training(Step): loss,1610351986920,0.1640481799840927
7698,-Metrics/Training(Step): loss,1610351990033,0.10563451796770096
7700,-Metrics/Training(Step): loss,1610351993520,0.14769725501537323
7702,-Metrics/Training(Step): loss,1610351996520,0.13711658120155334
7704,-Metrics/Training(Step): loss,1610351999269,0.13549953699111938
7706,-Metrics/Training(Step): loss,1610352001465,0.1402759552001953
7708,-Metrics/Training(Step): loss,1610352003644,0.13854867219924927
7710,-Metrics/Training(Step): loss,1610352005641,0.15164542198181152
7712,-Metrics/Training(Step): loss,1610352007758,0.15471139550209045
7714,-Metrics/Training(Step): loss,1610352009762,0.1546078771352768
7716,-Metrics/Training(Step): loss,1610352011900,0.11985240131616592
7718,-Metrics/Training(Step): loss,1610352014001,0.14738713204860687
7720,-Metrics/Training(Step): loss,1610352016029,0.15096154808998108
7722,-Metrics/Training(Step): loss,1610352018107,0.13143078982830048
7724,-Metrics/Training(Step): loss,1610352020201,0.12866567075252533
7726,-Metrics/Training(Step): loss,1610352022299,0.08901047706604004
7728,-Metrics/Training(Step): loss,1610352024392,0.14679254591464996
7730,-Metrics/Training(Step): loss,1610352026465,0.10298121720552444
7732,-Metrics/Training(Step): loss,1610352028521,0.13732846081256866
7734,-Metrics/Training(Step): loss,1610352030492,0.13065584003925323
7736,-Metrics/Training(Step): loss,1610352032454,0.1243954598903656
7738,-Metrics/Training(Step): loss,1610352034447,0.12805643677711487
7740,-Metrics/Training(Step): loss,1610352036325,0.14270438253879547
7742,-Metrics/Training(Step): loss,1610352060022,0.18102534115314484
7744,-Metrics/Training(Step): loss,1610352063920,0.1458059698343277
7746,-Metrics/Training(Step): loss,1610352067738,0.14287970960140228
7748,-Metrics/Training(Step): loss,1610352071515,0.1632400006055832
7750,-Metrics/Training(Step): loss,1610352075319,0.1548842489719391
7752,-Metrics/Training(Step): loss,1610352079119,0.17348413169384003
7754,-Metrics/Training(Step): loss,1610352082820,0.1209101602435112
7756,-Metrics/Training(Step): loss,1610352086356,0.14263774454593658
7758,-Metrics/Training(Step): loss,1610352090420,0.1520421952009201
7760,-Metrics/Training(Step): loss,1610352094231,0.13858434557914734
7762,-Metrics/Training(Step): loss,1610352097528,0.1670619547367096
7764,-Metrics/Training(Step): loss,1610352101320,0.14091552793979645
7766,-Metrics/Training(Step): loss,1610352105000,0.10800179839134216
7768,-Metrics/Training(Step): loss,1610352108872,0.15226031839847565
7770,-Metrics/Training(Step): loss,1610352112384,0.15431734919548035
7772,-Metrics/Training(Step): loss,1610352115752,0.1593141257762909
7774,-Metrics/Training(Step): loss,1610352118635,0.1174209713935852
7776,-Metrics/Training(Step): loss,1610352121843,0.12876573204994202
7778,-Metrics/Training(Step): loss,1610352125521,0.10124341398477554
7780,-Metrics/Training(Step): loss,1610352129117,0.1405719816684723
7782,-Metrics/Training(Step): loss,1610352133048,0.15616478025913239
7784,-Metrics/Training(Step): loss,1610352136818,0.12160255014896393
7786,-Metrics/Training(Step): loss,1610352139810,0.14953000843524933
7788,-Metrics/Training(Step): loss,1610352141924,0.11347842961549759
7790,-Metrics/Training(Step): loss,1610352144308,0.16206246614456177
7792,-Metrics/Training(Step): loss,1610352146602,0.15319949388504028
7794,-Metrics/Training(Step): loss,1610352148633,0.13326811790466309
7796,-Metrics/Training(Step): loss,1610352150804,0.15551310777664185
7798,-Metrics/Training(Step): loss,1610352152923,0.1695522964000702
7800,-Metrics/Training(Step): loss,1610352154921,0.14270123839378357
7802,-Metrics/Training(Step): loss,1610352156866,0.15788935124874115
7804,-Metrics/Training(Step): loss,1610352158951,0.11237198859453201
7806,-Metrics/Training(Step): loss,1610352160838,0.10797242820262909
7808,-Metrics/Training(Step): loss,1610352162837,0.14813052117824554
7810,-Metrics/Training(Step): loss,1610352164903,0.1705237329006195
7812,-Metrics/Training(Step): loss,1610352166965,0.14723899960517883
7814,-Metrics/Training(Step): loss,1610352169005,0.12989267706871033
7816,-Metrics/Training(Step): loss,1610352171062,0.1683022379875183
7818,-Metrics/Training(Step): loss,1610352173106,0.14584365487098694
7820,-Metrics/Training(Step): loss,1610352175159,0.1343674212694168
7822,-Metrics/Training(Step): loss,1610352177211,0.15158432722091675
7824,-Metrics/Training(Step): loss,1610352179295,0.14241738617420197
7826,-Metrics/Training(Step): loss,1610352181363,0.13954012095928192
7828,-Metrics/Training(Step): loss,1610352194418,0.12311980873346329
7830,-Metrics/Training(Step): loss,1610352198319,0.14375559985637665
7832,-Metrics/Training(Step): loss,1610352202628,0.12494315952062607
7834,-Metrics/Training(Step): loss,1610352207020,0.1485683172941208
7836,-Metrics/Training(Step): loss,1610352210920,0.1452925205230713
7838,-Metrics/Training(Step): loss,1610352214820,0.11588858813047409
7840,-Metrics/Training(Step): loss,1610352218646,0.0942346379160881
7842,-Metrics/Training(Step): loss,1610352222220,0.19250454008579254
7844,-Metrics/Training(Step): loss,1610352225228,0.16500474512577057
7846,-Metrics/Training(Step): loss,1610352228619,0.1826964169740677
7848,-Metrics/Training(Step): loss,1610352232496,0.1835382580757141
7850,-Metrics/Training(Step): loss,1610352236320,0.13652117550373077
7852,-Metrics/Training(Step): loss,1610352240222,0.1609155535697937
7854,-Metrics/Training(Step): loss,1610352243544,0.12955646216869354
7856,-Metrics/Training(Step): loss,1610352246819,0.13209083676338196
7858,-Metrics/Training(Step): loss,1610352250571,0.14962035417556763
7860,-Metrics/Training(Step): loss,1610352254220,0.1516869068145752
7862,-Metrics/Training(Step): loss,1610352257720,0.1319061666727066
7864,-Metrics/Training(Step): loss,1610352261042,0.14808325469493866
7866,-Metrics/Training(Step): loss,1610352264617,0.17515818774700165
7868,-Metrics/Training(Step): loss,1610352268034,0.14347097277641296
7870,-Metrics/Training(Step): loss,1610352271253,0.15603336691856384
7872,-Metrics/Training(Step): loss,1610352274619,0.13203565776348114
7874,-Metrics/Training(Step): loss,1610352277533,0.1646512895822525
7876,-Metrics/Training(Step): loss,1610352280064,0.1690562218427658
7878,-Metrics/Training(Step): loss,1610352282122,0.1438017636537552
7880,-Metrics/Training(Step): loss,1610352284159,0.14635084569454193
7882,-Metrics/Training(Step): loss,1610352286255,0.16042301058769226
7884,-Metrics/Training(Step): loss,1610352288362,0.1395399570465088
7886,-Metrics/Training(Step): loss,1610352290474,0.14890463650226593
7888,-Metrics/Training(Step): loss,1610352292470,0.167680025100708
7890,-Metrics/Training(Step): loss,1610352294600,0.13680687546730042
7892,-Metrics/Training(Step): loss,1610352296580,0.18372762203216553
7894,-Metrics/Training(Step): loss,1610352298428,0.14536583423614502
7896,-Metrics/Training(Step): loss,1610352300503,0.17410306632518768
7898,-Metrics/Training(Step): loss,1610352302568,0.16738726198673248
7900,-Metrics/Training(Step): loss,1610352304702,0.13620948791503906
7902,-Metrics/Training(Step): loss,1610352306624,0.16165924072265625
7904,-Metrics/Training(Step): loss,1610352308729,0.15893250703811646
7906,-Metrics/Training(Step): loss,1610352310827,0.1470993608236313
7908,-Metrics/Training(Step): loss,1610352312729,0.14326633512973785
7910,-Metrics/Training(Step): loss,1610352314798,0.1278415471315384
7912,-Metrics/Training(Step): loss,1610352316858,0.17970339953899384
7914,-Metrics/Training(Step): loss,1610352329525,0.19736532866954803
7916,-Metrics/Training(Step): loss,1610352333520,0.1549990028142929
7918,-Metrics/Training(Step): loss,1610352337459,0.1380579024553299
7920,-Metrics/Training(Step): loss,1610352341649,0.13197702169418335
7922,-Metrics/Training(Step): loss,1610352345562,0.17822794616222382
7924,-Metrics/Training(Step): loss,1610352349419,0.17028862237930298
7926,-Metrics/Training(Step): loss,1610352353144,0.11682974547147751
7928,-Metrics/Training(Step): loss,1610352357247,0.15442609786987305
7930,-Metrics/Training(Step): loss,1610352360825,0.12678368389606476
7932,-Metrics/Training(Step): loss,1610352364221,0.0918281227350235
7934,-Metrics/Training(Step): loss,1610352367940,0.14179080724716187
7936,-Metrics/Training(Step): loss,1610352371427,0.16514165699481964
7938,-Metrics/Training(Step): loss,1610352375018,0.12157577276229858
7940,-Metrics/Training(Step): loss,1610352378719,0.16003327071666718
7942,-Metrics/Training(Step): loss,1610352382222,0.16389858722686768
7944,-Metrics/Training(Step): loss,1610352384941,0.1447167694568634
7946,-Metrics/Training(Step): loss,1610352388150,0.11522742360830307
7948,-Metrics/Training(Step): loss,1610352392020,0.16681836545467377
7950,-Metrics/Training(Step): loss,1610352395630,0.10988285392522812
7952,-Metrics/Training(Step): loss,1610352399025,0.1594351828098297
7954,-Metrics/Training(Step): loss,1610352402322,0.1559242308139801
7956,-Metrics/Training(Step): loss,1610352406119,0.142905130982399
7958,-Metrics/Training(Step): loss,1610352409470,0.1515432745218277
7960,-Metrics/Training(Step): loss,1610352411964,0.11414827406406403
7962,-Metrics/Training(Step): loss,1610352414096,0.14866097271442413
7964,-Metrics/Training(Step): loss,1610352416256,0.14498741924762726
7966,-Metrics/Training(Step): loss,1610352418499,0.13832320272922516
7968,-Metrics/Training(Step): loss,1610352420554,0.17934094369411469
7970,-Metrics/Training(Step): loss,1610352422694,0.1149195060133934
7972,-Metrics/Training(Step): loss,1610352424791,0.1281113177537918
7974,-Metrics/Training(Step): loss,1610352426797,0.14822028577327728
7976,-Metrics/Training(Step): loss,1610352428802,0.15489459037780762
7978,-Metrics/Training(Step): loss,1610352430660,0.12019911408424377
7980,-Metrics/Training(Step): loss,1610352432546,0.14188264310359955
7982,-Metrics/Training(Step): loss,1610352434465,0.1329779177904129
7984,-Metrics/Training(Step): loss,1610352436557,0.15419794619083405
7986,-Metrics/Training(Step): loss,1610352438451,0.14446906745433807
7988,-Metrics/Training(Step): loss,1610352440517,0.1235615462064743
7990,-Metrics/Training(Step): loss,1610352442580,0.13618984818458557
7992,-Metrics/Training(Step): loss,1610352444657,0.1428588479757309
7994,-Metrics/Training(Step): loss,1610352446748,0.17940565943717957
7996,-Metrics/Training(Step): loss,1610352448771,0.13172665238380432
7998,-Metrics/Training(Step): loss,1610352450819,0.1025112122297287
8000,-Metrics/Training(Step): loss,1610352463825,0.14768721163272858
8002,-Metrics/Training(Step): loss,1610352468020,0.1446630209684372
8004,-Metrics/Training(Step): loss,1610352472235,0.15895968675613403
8006,-Metrics/Training(Step): loss,1610352476446,0.15682663023471832
8008,-Metrics/Training(Step): loss,1610352480222,0.185555562376976
8010,-Metrics/Training(Step): loss,1610352483833,0.12049122154712677
8012,-Metrics/Training(Step): loss,1610352487728,0.13797254860401154
8014,-Metrics/Training(Step): loss,1610352491236,0.11669953167438507
8016,-Metrics/Training(Step): loss,1610352494714,0.10762198269367218
8018,-Metrics/Training(Step): loss,1610352498120,0.13789157569408417
8020,-Metrics/Training(Step): loss,1610352502120,0.11684410274028778
8022,-Metrics/Training(Step): loss,1610352505535,0.17921003699302673
8024,-Metrics/Training(Step): loss,1610352509120,0.13990668952465057
8026,-Metrics/Training(Step): loss,1610352512499,0.1409437209367752
8028,-Metrics/Training(Step): loss,1610352515969,0.12452686578035355
8030,-Metrics/Training(Step): loss,1610352519819,0.10400266200304031
8032,-Metrics/Training(Step): loss,1610352523319,0.14083358645439148
8034,-Metrics/Training(Step): loss,1610352526519,0.15321293473243713
8036,-Metrics/Training(Step): loss,1610352529815,0.1323685497045517
8038,-Metrics/Training(Step): loss,1610352533159,0.134809210896492
8040,-Metrics/Training(Step): loss,1610352537255,0.114004947245121
8042,-Metrics/Training(Step): loss,1610352540620,0.18139122426509857
8044,-Metrics/Training(Step): loss,1610352543830,0.1558719128370285
8046,-Metrics/Training(Step): loss,1610352547145,0.1483430713415146
8048,-Metrics/Training(Step): loss,1610352549872,0.13249237835407257
8050,-Metrics/Training(Step): loss,1610352551841,0.16905008256435394
8052,-Metrics/Training(Step): loss,1610352553848,0.19059941172599792
8054,-Metrics/Training(Step): loss,1610352555909,0.17540214955806732
8056,-Metrics/Training(Step): loss,1610352558000,0.13606810569763184
8058,-Metrics/Training(Step): loss,1610352560080,0.14596760272979736
8060,-Metrics/Training(Step): loss,1610352562208,0.12592075765132904
8062,-Metrics/Training(Step): loss,1610352564304,0.1373990923166275
8064,-Metrics/Training(Step): loss,1610352566232,0.15258586406707764
8066,-Metrics/Training(Step): loss,1610352568351,0.15661457180976868
8068,-Metrics/Training(Step): loss,1610352570409,0.14842146635055542
8070,-Metrics/Training(Step): loss,1610352572498,0.14118613302707672
8072,-Metrics/Training(Step): loss,1610352574582,0.15422429144382477
8074,-Metrics/Training(Step): loss,1610352576660,0.17135564982891083
8076,-Metrics/Training(Step): loss,1610352578693,0.12997429072856903
8078,-Metrics/Training(Step): loss,1610352580775,0.13301505148410797
8080,-Metrics/Training(Step): loss,1610352582840,0.12300962954759598
8082,-Metrics/Training(Step): loss,1610352584883,0.15390942990779877
8084,-Metrics/Training(Step): loss,1610352586925,0.15054257214069366
8086,-Metrics/Training(Step): loss,1610352598933,0.1269000917673111
8088,-Metrics/Training(Step): loss,1610352603117,0.13282832503318787
8090,-Metrics/Training(Step): loss,1610352606940,0.1396101713180542
8092,-Metrics/Training(Step): loss,1610352610819,0.14890040457248688
8094,-Metrics/Training(Step): loss,1610352614647,0.13446083664894104
8096,-Metrics/Training(Step): loss,1610352618619,0.13685934245586395
8098,-Metrics/Training(Step): loss,1610352622348,0.13200893998146057
8100,-Metrics/Training(Step): loss,1610352626819,0.14006637036800385
8102,-Metrics/Training(Step): loss,1610352630892,0.10759947448968887
8104,-Metrics/Training(Step): loss,1610352634588,0.139489084482193
8106,-Metrics/Training(Step): loss,1610352637417,0.14696715772151947
8108,-Metrics/Training(Step): loss,1610352640826,0.1331460326910019
8110,-Metrics/Training(Step): loss,1610352644126,0.14841260015964508
8112,-Metrics/Training(Step): loss,1610352647119,0.12823793292045593
8114,-Metrics/Training(Step): loss,1610352650422,0.129170224070549
8116,-Metrics/Training(Step): loss,1610352653919,0.12838973104953766
8118,-Metrics/Training(Step): loss,1610352657812,0.12207937985658646
8120,-Metrics/Training(Step): loss,1610352660843,0.09649311751127243
8122,-Metrics/Training(Step): loss,1610352664428,0.17429132759571075
8124,-Metrics/Training(Step): loss,1610352668249,0.13522230088710785
8126,-Metrics/Training(Step): loss,1610352671820,0.16156865656375885
8128,-Metrics/Training(Step): loss,1610352675319,0.16680580377578735
8130,-Metrics/Training(Step): loss,1610352678982,0.15486304461956024
8132,-Metrics/Training(Step): loss,1610352682336,0.25132352113723755
8134,-Metrics/Training(Step): loss,1610352684969,0.19279202818870544
8136,-Metrics/Training(Step): loss,1610352687080,0.11005248129367828
8138,-Metrics/Training(Step): loss,1610352689108,0.14305168390274048
8140,-Metrics/Training(Step): loss,1610352691189,0.1545582115650177
8142,-Metrics/Training(Step): loss,1610352693326,0.18834343552589417
8144,-Metrics/Training(Step): loss,1610352695344,0.14366702735424042
8146,-Metrics/Training(Step): loss,1610352697448,0.13896887004375458
8148,-Metrics/Training(Step): loss,1610352699532,0.0934792011976242
8150,-Metrics/Training(Step): loss,1610352701478,0.13346877694129944
8152,-Metrics/Training(Step): loss,1610352703551,0.16737334430217743
8154,-Metrics/Training(Step): loss,1610352705516,0.1176648885011673
8156,-Metrics/Training(Step): loss,1610352707648,0.1481943428516388
8158,-Metrics/Training(Step): loss,1610352709725,0.12059392035007477
8160,-Metrics/Training(Step): loss,1610352711797,0.16984641551971436
8162,-Metrics/Training(Step): loss,1610352713795,0.1294538974761963
8164,-Metrics/Training(Step): loss,1610352715714,0.11688458919525146
8166,-Metrics/Training(Step): loss,1610352717791,0.1332116574048996
8168,-Metrics/Training(Step): loss,1610352719785,0.15193745493888855
8170,-Metrics/Training(Step): loss,1610352721849,0.14072830975055695
8172,-Metrics/Training(Step): loss,1610352734728,0.1467999815940857
8174,-Metrics/Training(Step): loss,1610352739226,0.11305194348096848
8176,-Metrics/Training(Step): loss,1610352743016,0.13099230825901031
8178,-Metrics/Training(Step): loss,1610352747321,0.15097466111183167
8180,-Metrics/Training(Step): loss,1610352751358,0.18078017234802246
8182,-Metrics/Training(Step): loss,1610352755820,0.14985625445842743
8184,-Metrics/Training(Step): loss,1610352759920,0.17645567655563354
8186,-Metrics/Training(Step): loss,1610352763720,0.13212642073631287
8188,-Metrics/Training(Step): loss,1610352767224,0.1432584673166275
8190,-Metrics/Training(Step): loss,1610352770827,0.10501855611801147
8192,-Metrics/Training(Step): loss,1610352775121,0.13077431917190552
8194,-Metrics/Training(Step): loss,1610352778835,0.1359097808599472
8196,-Metrics/Training(Step): loss,1610352782519,0.16155630350112915
8198,-Metrics/Training(Step): loss,1610352786219,0.15015809237957
8200,-Metrics/Training(Step): loss,1610352789619,0.12092182785272598
8202,-Metrics/Training(Step): loss,1610352793415,0.09737910330295563
8204,-Metrics/Training(Step): loss,1610352796979,0.14919212460517883
8206,-Metrics/Training(Step): loss,1610352800436,0.14785683155059814
8208,-Metrics/Training(Step): loss,1610352804019,0.13489288091659546
8210,-Metrics/Training(Step): loss,1610352807220,0.12214946001768112
8212,-Metrics/Training(Step): loss,1610352810858,0.14080268144607544
8214,-Metrics/Training(Step): loss,1610352814119,0.1634073257446289
8216,-Metrics/Training(Step): loss,1610352816769,0.16562969982624054
8218,-Metrics/Training(Step): loss,1610352819020,0.1493847519159317
8220,-Metrics/Training(Step): loss,1610352821363,0.14356112480163574
8222,-Metrics/Training(Step): loss,1610352823520,0.13041187822818756
8224,-Metrics/Training(Step): loss,1610352825618,0.17315717041492462
8226,-Metrics/Training(Step): loss,1610352827706,0.15913817286491394
8228,-Metrics/Training(Step): loss,1610352829856,0.1192638948559761
8230,-Metrics/Training(Step): loss,1610352831916,0.16681711375713348
8232,-Metrics/Training(Step): loss,1610352833997,0.13506294786930084
8234,-Metrics/Training(Step): loss,1610352836053,0.11853351444005966
8236,-Metrics/Training(Step): loss,1610352838050,0.13566522300243378
8238,-Metrics/Training(Step): loss,1610352840175,0.17177598178386688
8240,-Metrics/Training(Step): loss,1610352842228,0.13427121937274933
8242,-Metrics/Training(Step): loss,1610352844279,0.1727910339832306
8244,-Metrics/Training(Step): loss,1610352846319,0.12390369921922684
8246,-Metrics/Training(Step): loss,1610352848350,0.14126735925674438
8248,-Metrics/Training(Step): loss,1610352850460,0.12779363989830017
8250,-Metrics/Training(Step): loss,1610352852560,0.16595374047756195
8252,-Metrics/Training(Step): loss,1610352854551,0.17969480156898499
8254,-Metrics/Training(Step): loss,1610352856617,0.1644524186849594
8256,-Metrics/Training(Step): loss,1610352858664,0.13924510776996613
8258,-Metrics/Training(Step): loss,1610352871056,0.1259019374847412
8260,-Metrics/Training(Step): loss,1610352875021,0.14402484893798828
8262,-Metrics/Training(Step): loss,1610352878721,0.16393184661865234
8264,-Metrics/Training(Step): loss,1610352882524,0.11580602079629898
8266,-Metrics/Training(Step): loss,1610352886426,0.13275787234306335
8268,-Metrics/Training(Step): loss,1610352890920,0.12368997931480408
8270,-Metrics/Training(Step): loss,1610352895029,0.13585039973258972
8272,-Metrics/Training(Step): loss,1610352898560,0.1919090300798416
8274,-Metrics/Training(Step): loss,1610352902150,0.13785792887210846
8276,-Metrics/Training(Step): loss,1610352906438,0.1465522050857544
8278,-Metrics/Training(Step): loss,1610352910076,0.13184840977191925
8280,-Metrics/Training(Step): loss,1610352913581,0.14049899578094482
8282,-Metrics/Training(Step): loss,1610352917736,0.12653212249279022
8284,-Metrics/Training(Step): loss,1610352921104,0.12509073317050934
8286,-Metrics/Training(Step): loss,1610352924335,0.14896424114704132
8288,-Metrics/Training(Step): loss,1610352927922,0.149641752243042
8290,-Metrics/Training(Step): loss,1610352931652,0.12579208612442017
8292,-Metrics/Training(Step): loss,1610352934941,0.15755204856395721
8294,-Metrics/Training(Step): loss,1610352938420,0.15439940989017487
8296,-Metrics/Training(Step): loss,1610352941842,0.10107891261577606
8298,-Metrics/Training(Step): loss,1610352945420,0.15208083391189575
8300,-Metrics/Training(Step): loss,1610352949253,0.16800269484519958
8302,-Metrics/Training(Step): loss,1610352952917,0.16367574036121368
8304,-Metrics/Training(Step): loss,1610352955296,0.1390318125486374
8306,-Metrics/Training(Step): loss,1610352957633,0.1575477123260498
8308,-Metrics/Training(Step): loss,1610352959690,0.14968419075012207
8310,-Metrics/Training(Step): loss,1610352961728,0.11697345227003098
8312,-Metrics/Training(Step): loss,1610352963790,0.13102050125598907
8314,-Metrics/Training(Step): loss,1610352965870,0.1598070114850998
8316,-Metrics/Training(Step): loss,1610352968000,0.12439071387052536
8318,-Metrics/Training(Step): loss,1610352970022,0.11106003075838089
8320,-Metrics/Training(Step): loss,1610352972096,0.1922217756509781
8322,-Metrics/Training(Step): loss,1610352974170,0.16251489520072937
8324,-Metrics/Training(Step): loss,1610352976150,0.11687418073415756
8326,-Metrics/Training(Step): loss,1610352978201,0.13004641234874725
8328,-Metrics/Training(Step): loss,1610352980309,0.12809084355831146
8330,-Metrics/Training(Step): loss,1610352982330,0.16537520289421082
8332,-Metrics/Training(Step): loss,1610352984328,0.13692569732666016
8334,-Metrics/Training(Step): loss,1610352986307,0.12772512435913086
8336,-Metrics/Training(Step): loss,1610352988353,0.1486869752407074
8338,-Metrics/Training(Step): loss,1610352990429,0.14078378677368164
8340,-Metrics/Training(Step): loss,1610352992549,0.14114491641521454
8342,-Metrics/Training(Step): loss,1610352994585,0.1434870809316635
8344,-Metrics/Training(Step): loss,1610353007621,0.08811123669147491
8346,-Metrics/Training(Step): loss,1610353011626,0.15340304374694824
8348,-Metrics/Training(Step): loss,1610353015420,0.15092633664608002
8350,-Metrics/Training(Step): loss,1610353019631,0.13642366230487823
8352,-Metrics/Training(Step): loss,1610353023731,0.14445704221725464
8354,-Metrics/Training(Step): loss,1610353027634,0.12561415135860443
8356,-Metrics/Training(Step): loss,1610353031328,0.1431729793548584
8358,-Metrics/Training(Step): loss,1610353035920,0.153445765376091
8360,-Metrics/Training(Step): loss,1610353039755,0.19460545480251312
8362,-Metrics/Training(Step): loss,1610353043445,0.12455984950065613
8364,-Metrics/Training(Step): loss,1610353047119,0.16528551280498505
8366,-Metrics/Training(Step): loss,1610353051107,0.17101560533046722
8368,-Metrics/Training(Step): loss,1610353055048,0.12026403844356537
8370,-Metrics/Training(Step): loss,1610353058455,0.11842082440853119
8372,-Metrics/Training(Step): loss,1610353062196,0.14285552501678467
8374,-Metrics/Training(Step): loss,1610353066393,0.15400853753089905
8376,-Metrics/Training(Step): loss,1610353069245,0.16368776559829712
8378,-Metrics/Training(Step): loss,1610353072719,0.16342279314994812
8380,-Metrics/Training(Step): loss,1610353076321,0.11904525756835938
8382,-Metrics/Training(Step): loss,1610353080034,0.15101024508476257
8384,-Metrics/Training(Step): loss,1610353083919,0.1390361189842224
8386,-Metrics/Training(Step): loss,1610353087015,0.14108766615390778
8388,-Metrics/Training(Step): loss,1610353090416,0.14684823155403137
8390,-Metrics/Training(Step): loss,1610353092978,0.10511047393083572
8392,-Metrics/Training(Step): loss,1610353095108,0.11103177815675735
8394,-Metrics/Training(Step): loss,1610353097462,0.12052066624164581
8396,-Metrics/Training(Step): loss,1610353099631,0.16716839373111725
8398,-Metrics/Training(Step): loss,1610353101749,0.14022238552570343
8400,-Metrics/Training(Step): loss,1610353103817,0.131678968667984
8402,-Metrics/Training(Step): loss,1610353105924,0.13391239941120148
8404,-Metrics/Training(Step): loss,1610353108082,0.12789160013198853
8406,-Metrics/Training(Step): loss,1610353110160,0.17777299880981445
8408,-Metrics/Training(Step): loss,1610353112232,0.1278696358203888
8410,-Metrics/Training(Step): loss,1610353114289,0.13574500381946564
8412,-Metrics/Training(Step): loss,1610353116425,0.17260220646858215
8414,-Metrics/Training(Step): loss,1610353118496,0.12163782119750977
8416,-Metrics/Training(Step): loss,1610353120573,0.1160130575299263
8418,-Metrics/Training(Step): loss,1610353122598,0.14785917103290558
8420,-Metrics/Training(Step): loss,1610353124670,0.14106586575508118
8422,-Metrics/Training(Step): loss,1610353126644,0.15398654341697693
8424,-Metrics/Training(Step): loss,1610353128576,0.15567974746227264
8426,-Metrics/Training(Step): loss,1610353130488,0.12372127920389175
8428,-Metrics/Training(Step): loss,1610353132556,0.10716205090284348
8430,-Metrics/Training(Step): loss,1610353145736,0.14108310639858246
8432,-Metrics/Training(Step): loss,1610353150019,0.1380656361579895
8434,-Metrics/Training(Step): loss,1610353153824,0.1092570573091507
8436,-Metrics/Training(Step): loss,1610353157726,0.14119186997413635
8438,-Metrics/Training(Step): loss,1610353161728,0.16183340549468994
8440,-Metrics/Training(Step): loss,1610353165640,0.14280793070793152
8442,-Metrics/Training(Step): loss,1610353169439,0.14614205062389374
8444,-Metrics/Training(Step): loss,1610353173234,0.1386042982339859
8446,-Metrics/Training(Step): loss,1610353176828,0.16122184693813324
8448,-Metrics/Training(Step): loss,1610353180144,0.12057103961706161
8450,-Metrics/Training(Step): loss,1610353184120,0.11859259009361267
8452,-Metrics/Training(Step): loss,1610353187640,0.13739006221294403
8454,-Metrics/Training(Step): loss,1610353190826,0.13305017352104187
8456,-Metrics/Training(Step): loss,1610353194219,0.1260986626148224
8458,-Metrics/Training(Step): loss,1610353198088,0.14240027964115143
8460,-Metrics/Training(Step): loss,1610353201620,0.12282376736402512
8462,-Metrics/Training(Step): loss,1610353205624,0.16201478242874146
8464,-Metrics/Training(Step): loss,1610353209034,0.14081962406635284
8466,-Metrics/Training(Step): loss,1610353212414,0.13979864120483398
8468,-Metrics/Training(Step): loss,1610353215713,0.12211809307336807
8470,-Metrics/Training(Step): loss,1610353219442,0.12106047570705414
8472,-Metrics/Training(Step): loss,1610353222983,0.13679105043411255
8474,-Metrics/Training(Step): loss,1610353226529,0.1271170824766159
8476,-Metrics/Training(Step): loss,1610353229470,0.1535254716873169
8478,-Metrics/Training(Step): loss,1610353231594,0.11430669575929642
8480,-Metrics/Training(Step): loss,1610353233704,0.12876038253307343
8482,-Metrics/Training(Step): loss,1610353235834,0.14548642933368683
8484,-Metrics/Training(Step): loss,1610353237929,0.1814873069524765
8486,-Metrics/Training(Step): loss,1610353239974,0.18260453641414642
8488,-Metrics/Training(Step): loss,1610353242118,0.11471866071224213
8490,-Metrics/Training(Step): loss,1610353244217,0.1198025718331337
8492,-Metrics/Training(Step): loss,1610353246297,0.11049981415271759
8494,-Metrics/Training(Step): loss,1610353248356,0.1404220312833786
8496,-Metrics/Training(Step): loss,1610353250398,0.15433374047279358
8498,-Metrics/Training(Step): loss,1610353252500,0.1544896513223648
8500,-Metrics/Training(Step): loss,1610353254361,0.12039352208375931
8502,-Metrics/Training(Step): loss,1610353256252,0.12947654724121094
8504,-Metrics/Training(Step): loss,1610353258381,0.14910662174224854
8506,-Metrics/Training(Step): loss,1610353260448,0.15340986847877502
8508,-Metrics/Training(Step): loss,1610353262530,0.14377480745315552
8510,-Metrics/Training(Step): loss,1610353264509,0.12607426941394806
8512,-Metrics/Training(Step): loss,1610353266541,0.143284872174263
8514,-Metrics/Training(Step): loss,1610353268597,0.1762280911207199
8516,-Metrics/Training(Step): loss,1610353281116,0.12922903895378113
8518,-Metrics/Training(Step): loss,1610353285021,0.16019779443740845
8520,-Metrics/Training(Step): loss,1610353289423,0.15105733275413513
8522,-Metrics/Training(Step): loss,1610353293650,0.11238770931959152
8524,-Metrics/Training(Step): loss,1610353297444,0.1279863566160202
8526,-Metrics/Training(Step): loss,1610353301722,0.1485481858253479
8528,-Metrics/Training(Step): loss,1610353305958,0.17176453769207
8530,-Metrics/Training(Step): loss,1610353309720,0.12326731532812119
8532,-Metrics/Training(Step): loss,1610353313015,0.11661531031131744
8534,-Metrics/Training(Step): loss,1610353317020,0.12583841383457184
8536,-Metrics/Training(Step): loss,1610353320520,0.12956544756889343
8538,-Metrics/Training(Step): loss,1610353324242,0.18530473113059998
8540,-Metrics/Training(Step): loss,1610353327219,0.13824979960918427
8542,-Metrics/Training(Step): loss,1610353330719,0.1624930053949356
8544,-Metrics/Training(Step): loss,1610353333827,0.13549360632896423
8546,-Metrics/Training(Step): loss,1610353337045,0.17369747161865234
8548,-Metrics/Training(Step): loss,1610353340719,0.16120591759681702
8550,-Metrics/Training(Step): loss,1610353344223,0.14137162268161774
8552,-Metrics/Training(Step): loss,1610353347117,0.13637758791446686
8554,-Metrics/Training(Step): loss,1610353350507,0.14330661296844482
8556,-Metrics/Training(Step): loss,1610353354520,0.1274760514497757
8558,-Metrics/Training(Step): loss,1610353358643,0.14747211337089539
8560,-Metrics/Training(Step): loss,1610353361878,0.10571877658367157
8562,-Metrics/Training(Step): loss,1610353364429,0.13823942840099335
8564,-Metrics/Training(Step): loss,1610353366841,0.15203191339969635
8566,-Metrics/Training(Step): loss,1610353369001,0.13559800386428833
8568,-Metrics/Training(Step): loss,1610353371101,0.13334643840789795
8570,-Metrics/Training(Step): loss,1610353373172,0.15898112952709198
8572,-Metrics/Training(Step): loss,1610353375268,0.09314802289009094
8574,-Metrics/Training(Step): loss,1610353377302,0.14859271049499512
8576,-Metrics/Training(Step): loss,1610353379375,0.16226443648338318
8578,-Metrics/Training(Step): loss,1610353381390,0.14909391105175018
8580,-Metrics/Training(Step): loss,1610353383444,0.15595149993896484
8582,-Metrics/Training(Step): loss,1610353385515,0.11967873573303223
8584,-Metrics/Training(Step): loss,1610353387568,0.13591884076595306
8586,-Metrics/Training(Step): loss,1610353389659,0.1366087943315506
8588,-Metrics/Training(Step): loss,1610353391734,0.12262973189353943
8590,-Metrics/Training(Step): loss,1610353393733,0.11076910048723221
8592,-Metrics/Training(Step): loss,1610353395779,0.1390022337436676
8594,-Metrics/Training(Step): loss,1610353397797,0.12819212675094604
8596,-Metrics/Training(Step): loss,1610353399839,0.16930262744426727
8598,-Metrics/Training(Step): loss,1610353401947,0.14065557718276978
8600,-Metrics/Training(Step): loss,1610353403884,0.14373531937599182
8602,-Metrics/Training(Step): loss,1610353416818,0.14741544425487518
8604,-Metrics/Training(Step): loss,1610353421037,0.11959897726774216
8606,-Metrics/Training(Step): loss,1610353426040,0.19958162307739258
8608,-Metrics/Training(Step): loss,1610353430019,0.13526907563209534
8610,-Metrics/Training(Step): loss,1610353433927,0.12838974595069885
8612,-Metrics/Training(Step): loss,1610353438146,0.1577298790216446
8614,-Metrics/Training(Step): loss,1610353441821,0.15360061824321747
8616,-Metrics/Training(Step): loss,1610353445521,0.16517828404903412
8618,-Metrics/Training(Step): loss,1610353449020,0.14563393592834473
8620,-Metrics/Training(Step): loss,1610353452922,0.12470249831676483
8622,-Metrics/Training(Step): loss,1610353456458,0.16502393782138824
8624,-Metrics/Training(Step): loss,1610353460063,0.10542653501033783
8626,-Metrics/Training(Step): loss,1610353463319,0.16288083791732788
8628,-Metrics/Training(Step): loss,1610353466420,0.08876553922891617
8630,-Metrics/Training(Step): loss,1610353470231,0.14040997624397278
8632,-Metrics/Training(Step): loss,1610353473637,0.13683773577213287
8634,-Metrics/Training(Step): loss,1610353476544,0.12111067026853561
8636,-Metrics/Training(Step): loss,1610353480220,0.11109025031328201
8638,-Metrics/Training(Step): loss,1610353483621,0.15252208709716797
8640,-Metrics/Training(Step): loss,1610353486820,0.10997138917446136
8642,-Metrics/Training(Step): loss,1610353490420,0.12638723850250244
8644,-Metrics/Training(Step): loss,1610353494808,0.144606351852417
8646,-Metrics/Training(Step): loss,1610353498020,0.12699085474014282
8648,-Metrics/Training(Step): loss,1610353500641,0.15464140474796295
8650,-Metrics/Training(Step): loss,1610353502900,0.1371636837720871
8652,-Metrics/Training(Step): loss,1610353505109,0.15486182272434235
8654,-Metrics/Training(Step): loss,1610353507302,0.19445785880088806
8656,-Metrics/Training(Step): loss,1610353509256,0.108183354139328
8658,-Metrics/Training(Step): loss,1610353511318,0.15623708069324493
8660,-Metrics/Training(Step): loss,1610353513408,0.17294645309448242
8662,-Metrics/Training(Step): loss,1610353515459,0.17064893245697021
8664,-Metrics/Training(Step): loss,1610353517538,0.1577906310558319
8666,-Metrics/Training(Step): loss,1610353519483,0.18147170543670654
8668,-Metrics/Training(Step): loss,1610353521493,0.14450830221176147
8670,-Metrics/Training(Step): loss,1610353523488,0.17823836207389832
8672,-Metrics/Training(Step): loss,1610353525529,0.1450057327747345
8674,-Metrics/Training(Step): loss,1610353527624,0.14167837798595428
8676,-Metrics/Training(Step): loss,1610353529657,0.1095525398850441
8678,-Metrics/Training(Step): loss,1610353531679,0.16899420320987701
8680,-Metrics/Training(Step): loss,1610353533757,0.18217124044895172
8682,-Metrics/Training(Step): loss,1610353535705,0.13329483568668365
8684,-Metrics/Training(Step): loss,1610353537733,0.12024571001529694
8686,-Metrics/Training(Step): loss,1610353539744,0.13884355127811432
8688,-Metrics/Training(Step): loss,1610353551821,0.1519848108291626
8690,-Metrics/Training(Step): loss,1610353555524,0.15272635221481323
8692,-Metrics/Training(Step): loss,1610353559619,0.13306932151317596
8694,-Metrics/Training(Step): loss,1610353563419,0.11503386497497559
8696,-Metrics/Training(Step): loss,1610353567226,0.1651884764432907
8698,-Metrics/Training(Step): loss,1610353570956,0.15531572699546814
8700,-Metrics/Training(Step): loss,1610353575627,0.1365898698568344
8702,-Metrics/Training(Step): loss,1610353579327,0.1553131639957428
8704,-Metrics/Training(Step): loss,1610353583124,0.19323045015335083
8706,-Metrics/Training(Step): loss,1610353586819,0.1368905007839203
8708,-Metrics/Training(Step): loss,1610353590356,0.1565481424331665
8710,-Metrics/Training(Step): loss,1610353593591,0.17177706956863403
8712,-Metrics/Training(Step): loss,1610353597320,0.17074495553970337
8714,-Metrics/Training(Step): loss,1610353600620,0.1339341402053833
8716,-Metrics/Training(Step): loss,1610353604310,0.13056138157844543
8718,-Metrics/Training(Step): loss,1610353607738,0.11307336390018463
8720,-Metrics/Training(Step): loss,1610353611100,0.1390470713376999
8722,-Metrics/Training(Step): loss,1610353614147,0.11906671524047852
8724,-Metrics/Training(Step): loss,1610353618132,0.1607835739850998
8726,-Metrics/Training(Step): loss,1610353621577,0.14041472971439362
8728,-Metrics/Training(Step): loss,1610353625224,0.15478578209877014
8730,-Metrics/Training(Step): loss,1610353629066,0.14300690591335297
8732,-Metrics/Training(Step): loss,1610353632750,0.13405680656433105
8734,-Metrics/Training(Step): loss,1610353635935,0.13382188975811005
8736,-Metrics/Training(Step): loss,1610353638471,0.15433143079280853
8738,-Metrics/Training(Step): loss,1610353640658,0.12392832338809967
8740,-Metrics/Training(Step): loss,1610353642709,0.12683279812335968
8742,-Metrics/Training(Step): loss,1610353644696,0.14163051545619965
8744,-Metrics/Training(Step): loss,1610353646813,0.11048826575279236
8746,-Metrics/Training(Step): loss,1610353648918,0.12846140563488007
8748,-Metrics/Training(Step): loss,1610353651016,0.15616042912006378
8750,-Metrics/Training(Step): loss,1610353653143,0.15001167356967926
8752,-Metrics/Training(Step): loss,1610353655208,0.14554589986801147
8754,-Metrics/Training(Step): loss,1610353657303,0.12542720139026642
8756,-Metrics/Training(Step): loss,1610353659253,0.12055046111345291
8758,-Metrics/Training(Step): loss,1610353661378,0.15502259135246277
8760,-Metrics/Training(Step): loss,1610353663478,0.1785202920436859
8762,-Metrics/Training(Step): loss,1610353665363,0.18173664808273315
8764,-Metrics/Training(Step): loss,1610353667344,0.1208091750741005
8766,-Metrics/Training(Step): loss,1610353669414,0.12503257393836975
8768,-Metrics/Training(Step): loss,1610353671512,0.14322660863399506
8770,-Metrics/Training(Step): loss,1610353673591,0.15371930599212646
8772,-Metrics/Training(Step): loss,1610353675581,0.12255488336086273
8774,-Metrics/Training(Step): loss,1610353687733,0.11188752204179764
8776,-Metrics/Training(Step): loss,1610353691820,0.14097611606121063
8778,-Metrics/Training(Step): loss,1610353695723,0.16726426780223846
8780,-Metrics/Training(Step): loss,1610353699919,0.15792369842529297
8782,-Metrics/Training(Step): loss,1610353703919,0.1788150519132614
8784,-Metrics/Training(Step): loss,1610353708019,0.13564839959144592
8786,-Metrics/Training(Step): loss,1610353711819,0.16601595282554626
8788,-Metrics/Training(Step): loss,1610353715920,0.18650321662425995
8790,-Metrics/Training(Step): loss,1610353719921,0.1372993141412735
8792,-Metrics/Training(Step): loss,1610353723634,0.14169302582740784
8794,-Metrics/Training(Step): loss,1610353727320,0.11671891808509827
8796,-Metrics/Training(Step): loss,1610353731081,0.11421875655651093
8798,-Metrics/Training(Step): loss,1610353734920,0.13979730010032654
8800,-Metrics/Training(Step): loss,1610353738330,0.15971751511096954
8802,-Metrics/Training(Step): loss,1610353741823,0.15916423499584198
8804,-Metrics/Training(Step): loss,1610353745220,0.12960772216320038
8806,-Metrics/Training(Step): loss,1610353748906,0.13357537984848022
8808,-Metrics/Training(Step): loss,1610353751687,0.08659833669662476
8810,-Metrics/Training(Step): loss,1610353755120,0.1528574824333191
8812,-Metrics/Training(Step): loss,1610353758718,0.15652403235435486
8814,-Metrics/Training(Step): loss,1610353761940,0.12614700198173523
8816,-Metrics/Training(Step): loss,1610353765419,0.18912707269191742
8818,-Metrics/Training(Step): loss,1610353768644,0.11373132467269897
8820,-Metrics/Training(Step): loss,1610353771898,0.13500170409679413
8822,-Metrics/Training(Step): loss,1610353774253,0.12345606833696365
8824,-Metrics/Training(Step): loss,1610353776527,0.1334846019744873
8826,-Metrics/Training(Step): loss,1610353778664,0.17182600498199463
8828,-Metrics/Training(Step): loss,1610353780762,0.15616925060749054
8830,-Metrics/Training(Step): loss,1610353782889,0.15187372267246246
8832,-Metrics/Training(Step): loss,1610353784877,0.14476419985294342
8834,-Metrics/Training(Step): loss,1610353786940,0.16520459949970245
8836,-Metrics/Training(Step): loss,1610353788857,0.13948507606983185
8838,-Metrics/Training(Step): loss,1610353790958,0.14643965661525726
8840,-Metrics/Training(Step): loss,1610353792942,0.14954747259616852
8842,-Metrics/Training(Step): loss,1610353794860,0.12879741191864014
8844,-Metrics/Training(Step): loss,1610353796965,0.1277821809053421
8846,-Metrics/Training(Step): loss,1610353799040,0.12838460505008698
8848,-Metrics/Training(Step): loss,1610353801103,0.14298853278160095
8850,-Metrics/Training(Step): loss,1610353803171,0.15646900236606598
8852,-Metrics/Training(Step): loss,1610353805245,0.14538739621639252
8854,-Metrics/Training(Step): loss,1610353807321,0.12575627863407135
8856,-Metrics/Training(Step): loss,1610353809404,0.12641474604606628
8858,-Metrics/Training(Step): loss,1610353811429,0.15777258574962616
8860,-Metrics/Training(Step): loss,1610353823920,0.15829893946647644
8862,-Metrics/Training(Step): loss,1610353827820,0.16985057294368744
8864,-Metrics/Training(Step): loss,1610353831919,0.13480207324028015
8866,-Metrics/Training(Step): loss,1610353835741,0.12452127784490585
8868,-Metrics/Training(Step): loss,1610353839444,0.13750427961349487
8870,-Metrics/Training(Step): loss,1610353843618,0.1407245248556137
8872,-Metrics/Training(Step): loss,1610353847321,0.1498214602470398
8874,-Metrics/Training(Step): loss,1610353850720,0.18402402102947235
8876,-Metrics/Training(Step): loss,1610353854410,0.10752886533737183
8878,-Metrics/Training(Step): loss,1610353857843,0.13698282837867737
8880,-Metrics/Training(Step): loss,1610353861519,0.1392926424741745
8882,-Metrics/Training(Step): loss,1610353865570,0.13025102019309998
8884,-Metrics/Training(Step): loss,1610353869020,0.11838017404079437
8886,-Metrics/Training(Step): loss,1610353872522,0.10637582093477249
8888,-Metrics/Training(Step): loss,1610353875919,0.13704250752925873
8890,-Metrics/Training(Step): loss,1610353879620,0.14109767973423004
8892,-Metrics/Training(Step): loss,1610353882919,0.11822964996099472
8894,-Metrics/Training(Step): loss,1610353886383,0.16443142294883728
8896,-Metrics/Training(Step): loss,1610353890219,0.11990894377231598
8898,-Metrics/Training(Step): loss,1610353893720,0.1292998045682907
8900,-Metrics/Training(Step): loss,1610353896867,0.12227310985326767
8902,-Metrics/Training(Step): loss,1610353900419,0.1244637668132782
8904,-Metrics/Training(Step): loss,1610353903776,0.11728092283010483
8906,-Metrics/Training(Step): loss,1610353906799,0.16232667863368988
8908,-Metrics/Training(Step): loss,1610353908900,0.12632104754447937
8910,-Metrics/Training(Step): loss,1610353911012,0.1453564167022705
8912,-Metrics/Training(Step): loss,1610353913097,0.10875780135393143
8914,-Metrics/Training(Step): loss,1610353915194,0.08260884881019592
8916,-Metrics/Training(Step): loss,1610353917249,0.11771983653306961
8918,-Metrics/Training(Step): loss,1610353919236,0.10183689743280411
8920,-Metrics/Training(Step): loss,1610353921295,0.1216663122177124
8922,-Metrics/Training(Step): loss,1610353923303,0.14237073063850403
8924,-Metrics/Training(Step): loss,1610353925235,0.11469165980815887
8926,-Metrics/Training(Step): loss,1610353927181,0.1343568116426468
8928,-Metrics/Training(Step): loss,1610353929152,0.130795419216156
8930,-Metrics/Training(Step): loss,1610353931148,0.17172396183013916
8932,-Metrics/Training(Step): loss,1610353933184,0.15459106862545013
8934,-Metrics/Training(Step): loss,1610353935088,0.1329735666513443
8936,-Metrics/Training(Step): loss,1610353937197,0.1598796397447586
8938,-Metrics/Training(Step): loss,1610353939248,0.18796104192733765
8940,-Metrics/Training(Step): loss,1610353941114,0.1337217092514038
8942,-Metrics/Training(Step): loss,1610353943167,0.13248005509376526
8944,-Metrics/Training(Step): loss,1610353945189,0.13563327491283417
8946,-Metrics/Training(Step): loss,1610353957423,0.1221018135547638
8948,-Metrics/Training(Step): loss,1610353961432,0.11927133798599243
8950,-Metrics/Training(Step): loss,1610353965521,0.11363860219717026
8952,-Metrics/Training(Step): loss,1610353969317,0.1360456943511963
8954,-Metrics/Training(Step): loss,1610353973554,0.12962433695793152
8956,-Metrics/Training(Step): loss,1610353977344,0.15375129878520966
8958,-Metrics/Training(Step): loss,1610353981535,0.10379175841808319
8960,-Metrics/Training(Step): loss,1610353985620,0.12088337540626526
8962,-Metrics/Training(Step): loss,1610353989614,0.14261986315250397
8964,-Metrics/Training(Step): loss,1610353993220,0.15825526416301727
8966,-Metrics/Training(Step): loss,1610353997140,0.13971562683582306
8968,-Metrics/Training(Step): loss,1610354000853,0.1563742458820343
8970,-Metrics/Training(Step): loss,1610354004520,0.12082934379577637
8972,-Metrics/Training(Step): loss,1610354008674,0.12696076929569244
8974,-Metrics/Training(Step): loss,1610354011916,0.13140153884887695
8976,-Metrics/Training(Step): loss,1610354015116,0.14131326973438263
8978,-Metrics/Training(Step): loss,1610354018516,0.10493302345275879
8980,-Metrics/Training(Step): loss,1610354022024,0.14737246930599213
8982,-Metrics/Training(Step): loss,1610354025519,0.12877175211906433
8984,-Metrics/Training(Step): loss,1610354028828,0.1735931783914566
8986,-Metrics/Training(Step): loss,1610354032420,0.10485777258872986
8988,-Metrics/Training(Step): loss,1610354036226,0.1312260925769806
8990,-Metrics/Training(Step): loss,1610354039551,0.11391923576593399
8992,-Metrics/Training(Step): loss,1610354042208,0.14160233736038208
8994,-Metrics/Training(Step): loss,1610354044393,0.1606082320213318
8996,-Metrics/Training(Step): loss,1610354046531,0.11618172377347946
8998,-Metrics/Training(Step): loss,1610354048641,0.13046413660049438
9000,-Metrics/Training(Step): loss,1610354050727,0.145377054810524
9002,-Metrics/Training(Step): loss,1610354052854,0.14505267143249512
9004,-Metrics/Training(Step): loss,1610354054978,0.14527900516986847
9006,-Metrics/Training(Step): loss,1610354057079,0.12417957931756973
9008,-Metrics/Training(Step): loss,1610354059164,0.1464291214942932
9010,-Metrics/Training(Step): loss,1610354061029,0.12861095368862152
9012,-Metrics/Training(Step): loss,1610354063081,0.14558807015419006
9014,-Metrics/Training(Step): loss,1610354065050,0.1557648777961731
9016,-Metrics/Training(Step): loss,1610354067068,0.10140620172023773
9018,-Metrics/Training(Step): loss,1610354069104,0.11407002806663513
9020,-Metrics/Training(Step): loss,1610354071037,0.13606368005275726
9022,-Metrics/Training(Step): loss,1610354073037,0.14430725574493408
9024,-Metrics/Training(Step): loss,1610354075012,0.13693173229694366
9026,-Metrics/Training(Step): loss,1610354077059,0.1328936219215393
9028,-Metrics/Training(Step): loss,1610354079101,0.14271633327007294
9030,-Metrics/Training(Step): loss,1610354081153,0.12947194278240204
9032,-Metrics/Training(Step): loss,1610354093828,0.11539983004331589
9034,-Metrics/Training(Step): loss,1610354098119,0.17096830904483795
9036,-Metrics/Training(Step): loss,1610354102215,0.12405905872583389
9038,-Metrics/Training(Step): loss,1610354105858,0.1693366914987564
9040,-Metrics/Training(Step): loss,1610354110015,0.13981468975543976
9042,-Metrics/Training(Step): loss,1610354114015,0.15861260890960693
9044,-Metrics/Training(Step): loss,1610354118058,0.1776205152273178
9046,-Metrics/Training(Step): loss,1610354121819,0.10987538844347
9048,-Metrics/Training(Step): loss,1610354125442,0.10416413098573685
9050,-Metrics/Training(Step): loss,1610354129019,0.11307307332754135
9052,-Metrics/Training(Step): loss,1610354132716,0.13738052546977997
9054,-Metrics/Training(Step): loss,1610354136445,0.14835737645626068
9056,-Metrics/Training(Step): loss,1610354139898,0.1602649837732315
9058,-Metrics/Training(Step): loss,1610354143428,0.11936582624912262
9060,-Metrics/Training(Step): loss,1610354147020,0.1079859659075737
9062,-Metrics/Training(Step): loss,1610354150809,0.125337153673172
9064,-Metrics/Training(Step): loss,1610354153946,0.13086092472076416
9066,-Metrics/Training(Step): loss,1610354157422,0.10067515820264816
9068,-Metrics/Training(Step): loss,1610354160820,0.12609730660915375
9070,-Metrics/Training(Step): loss,1610354164019,0.12176013737916946
9072,-Metrics/Training(Step): loss,1610354167464,0.11824093759059906
9074,-Metrics/Training(Step): loss,1610354171062,0.10620783269405365
9076,-Metrics/Training(Step): loss,1610354174024,0.13394668698310852
9078,-Metrics/Training(Step): loss,1610354177431,0.1215527281165123
9080,-Metrics/Training(Step): loss,1610354180039,0.12740856409072876
9082,-Metrics/Training(Step): loss,1610354182098,0.09088847786188126
9084,-Metrics/Training(Step): loss,1610354184196,0.1531408131122589
9086,-Metrics/Training(Step): loss,1610354186310,0.12990622222423553
9088,-Metrics/Training(Step): loss,1610354188422,0.11381402611732483
9090,-Metrics/Training(Step): loss,1610354190505,0.13564829528331757
9092,-Metrics/Training(Step): loss,1610354192564,0.11792369186878204
9094,-Metrics/Training(Step): loss,1610354194576,0.12232161313295364
9096,-Metrics/Training(Step): loss,1610354196564,0.14233766496181488
9098,-Metrics/Training(Step): loss,1610354198666,0.10083559155464172
9100,-Metrics/Training(Step): loss,1610354200692,0.11703714728355408
9102,-Metrics/Training(Step): loss,1610354202794,0.14528995752334595
9104,-Metrics/Training(Step): loss,1610354204819,0.1012156531214714
9106,-Metrics/Training(Step): loss,1610354206844,0.13285678625106812
9108,-Metrics/Training(Step): loss,1610354208700,0.11074569076299667
9110,-Metrics/Training(Step): loss,1610354210756,0.15587083995342255
9112,-Metrics/Training(Step): loss,1610354212763,0.12308266758918762
9114,-Metrics/Training(Step): loss,1610354214787,0.12999767065048218
9116,-Metrics/Training(Step): loss,1610354216833,0.13715094327926636
9118,-Metrics/Training(Step): loss,1610354229519,0.13927623629570007
9120,-Metrics/Training(Step): loss,1610354233621,0.12768612802028656
9122,-Metrics/Training(Step): loss,1610354237722,0.12697887420654297
9124,-Metrics/Training(Step): loss,1610354241515,0.11571252346038818
9126,-Metrics/Training(Step): loss,1610354245338,0.14716897904872894
9128,-Metrics/Training(Step): loss,1610354249228,0.16084937751293182
9130,-Metrics/Training(Step): loss,1610354253219,0.11898672580718994
9132,-Metrics/Training(Step): loss,1610354257025,0.14333288371562958
9134,-Metrics/Training(Step): loss,1610354260516,0.1303340345621109
9136,-Metrics/Training(Step): loss,1610354264063,0.11797375231981277
9138,-Metrics/Training(Step): loss,1610354267397,0.12827998399734497
9140,-Metrics/Training(Step): loss,1610354270619,0.13752572238445282
9142,-Metrics/Training(Step): loss,1610354274268,0.13547825813293457
9144,-Metrics/Training(Step): loss,1610354277726,0.10984750092029572
9146,-Metrics/Training(Step): loss,1610354281319,0.12763212621212006
9148,-Metrics/Training(Step): loss,1610354284769,0.1281726360321045
9150,-Metrics/Training(Step): loss,1610354288220,0.11123265326023102
9152,-Metrics/Training(Step): loss,1610354291541,0.12910372018814087
9154,-Metrics/Training(Step): loss,1610354294920,0.12619411945343018
9156,-Metrics/Training(Step): loss,1610354298919,0.142020583152771
9158,-Metrics/Training(Step): loss,1610354303228,0.14165370166301727
9160,-Metrics/Training(Step): loss,1610354307105,0.1562216579914093
9162,-Metrics/Training(Step): loss,1610354310826,0.1303991824388504
9164,-Metrics/Training(Step): loss,1610354314030,0.1638757884502411
9166,-Metrics/Training(Step): loss,1610354316458,0.14186446368694305
9168,-Metrics/Training(Step): loss,1610354318519,0.11478886008262634
9170,-Metrics/Training(Step): loss,1610354320518,0.12100358307361603
9172,-Metrics/Training(Step): loss,1610354322611,0.17039231956005096
9174,-Metrics/Training(Step): loss,1610354324715,0.1486060917377472
9176,-Metrics/Training(Step): loss,1610354326843,0.11700734496116638
9178,-Metrics/Training(Step): loss,1610354328947,0.11835021525621414
9180,-Metrics/Training(Step): loss,1610354331045,0.1647459715604782
9182,-Metrics/Training(Step): loss,1610354333174,0.14227961003780365
9184,-Metrics/Training(Step): loss,1610354335246,0.12544164061546326
9186,-Metrics/Training(Step): loss,1610354337312,0.14196348190307617
9188,-Metrics/Training(Step): loss,1610354339229,0.09799394011497498
9190,-Metrics/Training(Step): loss,1610354341233,0.1585482507944107
9192,-Metrics/Training(Step): loss,1610354343300,0.12196656316518784
9194,-Metrics/Training(Step): loss,1610354345283,0.11519259959459305
9196,-Metrics/Training(Step): loss,1610354347294,0.11243509501218796
9198,-Metrics/Training(Step): loss,1610354349305,0.12906944751739502
9200,-Metrics/Training(Step): loss,1610354351356,0.14402426779270172
9202,-Metrics/Training(Step): loss,1610354353377,0.16810455918312073
9204,-Metrics/Training(Step): loss,1610354367123,0.13669174909591675
9206,-Metrics/Training(Step): loss,1610354371428,0.11738289892673492
9208,-Metrics/Training(Step): loss,1610354375320,0.10556869953870773
9210,-Metrics/Training(Step): loss,1610354379319,0.15947137773036957
9212,-Metrics/Training(Step): loss,1610354383221,0.13799062371253967
9214,-Metrics/Training(Step): loss,1610354386827,0.10865960270166397
9216,-Metrics/Training(Step): loss,1610354390119,0.1516694873571396
9218,-Metrics/Training(Step): loss,1610354393521,0.13030560314655304
9220,-Metrics/Training(Step): loss,1610354397219,0.1211826279759407
9222,-Metrics/Training(Step): loss,1610354400729,0.11898429691791534
9224,-Metrics/Training(Step): loss,1610354404127,0.14196093380451202
9226,-Metrics/Training(Step): loss,1610354407801,0.15370087325572968
9228,-Metrics/Training(Step): loss,1610354411230,0.12390749901533127
9230,-Metrics/Training(Step): loss,1610354414720,0.0935554951429367
9232,-Metrics/Training(Step): loss,1610354418254,0.12074562162160873
9234,-Metrics/Training(Step): loss,1610354422021,0.13231149315834045
9236,-Metrics/Training(Step): loss,1610354425520,0.13545957207679749
9238,-Metrics/Training(Step): loss,1610354429315,0.11009041219949722
9240,-Metrics/Training(Step): loss,1610354432912,0.12060417234897614
9242,-Metrics/Training(Step): loss,1610354436033,0.07074714452028275
9244,-Metrics/Training(Step): loss,1610354439093,0.12742860615253448
9246,-Metrics/Training(Step): loss,1610354442620,0.1527664065361023
9248,-Metrics/Training(Step): loss,1610354445823,0.13554587960243225
9250,-Metrics/Training(Step): loss,1610354449219,0.16411390900611877
9252,-Metrics/Training(Step): loss,1610354451415,0.13827307522296906
9254,-Metrics/Training(Step): loss,1610354453753,0.12724187970161438
9256,-Metrics/Training(Step): loss,1610354455787,0.13777805864810944
9258,-Metrics/Training(Step): loss,1610354457831,0.14814545214176178
9260,-Metrics/Training(Step): loss,1610354459922,0.14693932235240936
9262,-Metrics/Training(Step): loss,1610354462032,0.13319720327854156
9264,-Metrics/Training(Step): loss,1610354464154,0.1302511841058731
9266,-Metrics/Training(Step): loss,1610354466230,0.1426142156124115
9268,-Metrics/Training(Step): loss,1610354468100,0.12887835502624512
9270,-Metrics/Training(Step): loss,1610354470099,0.11486827582120895
9272,-Metrics/Training(Step): loss,1610354472204,0.14217136800289154
9274,-Metrics/Training(Step): loss,1610354474298,0.16392455995082855
9276,-Metrics/Training(Step): loss,1610354476257,0.1356503814458847
9278,-Metrics/Training(Step): loss,1610354478267,0.1309860199689865
9280,-Metrics/Training(Step): loss,1610354480324,0.11597208678722382
9282,-Metrics/Training(Step): loss,1610354482356,0.14799217879772186
9284,-Metrics/Training(Step): loss,1610354484438,0.12651535868644714
9286,-Metrics/Training(Step): loss,1610354486411,0.14030157029628754
9288,-Metrics/Training(Step): loss,1610354488477,0.1399211287498474
9290,-Metrics/Training(Step): loss,1610354500720,0.13363207876682281
9292,-Metrics/Training(Step): loss,1610354505125,0.13352295756340027
9294,-Metrics/Training(Step): loss,1610354509029,0.10901481658220291
9296,-Metrics/Training(Step): loss,1610354513056,0.13959579169750214
9298,-Metrics/Training(Step): loss,1610354516619,0.12166940420866013
9300,-Metrics/Training(Step): loss,1610354520440,0.10986784100532532
9302,-Metrics/Training(Step): loss,1610354524758,0.13289892673492432
9304,-Metrics/Training(Step): loss,1610354528020,0.1310788094997406
9306,-Metrics/Training(Step): loss,1610354531264,0.14419999718666077
9308,-Metrics/Training(Step): loss,1610354534920,0.15272170305252075
9310,-Metrics/Training(Step): loss,1610354538160,0.1403510421514511
9312,-Metrics/Training(Step): loss,1610354541815,0.11223378777503967
9314,-Metrics/Training(Step): loss,1610354545518,0.12339581549167633
9316,-Metrics/Training(Step): loss,1610354548863,0.13586671650409698
9318,-Metrics/Training(Step): loss,1610354552206,0.12314024567604065
9320,-Metrics/Training(Step): loss,1610354555518,0.13761073350906372
9322,-Metrics/Training(Step): loss,1610354558643,0.12344023585319519
9324,-Metrics/Training(Step): loss,1610354562520,0.1558770090341568
9326,-Metrics/Training(Step): loss,1610354566232,0.14112162590026855
9328,-Metrics/Training(Step): loss,1610354570020,0.16431424021720886
9330,-Metrics/Training(Step): loss,1610354573713,0.10084334760904312
9332,-Metrics/Training(Step): loss,1610354577219,0.1339356154203415
9334,-Metrics/Training(Step): loss,1610354580319,0.1282905787229538
9336,-Metrics/Training(Step): loss,1610354583518,0.14398494362831116
9338,-Metrics/Training(Step): loss,1610354586099,0.17550787329673767
9340,-Metrics/Training(Step): loss,1610354588272,0.10712967067956924
9342,-Metrics/Training(Step): loss,1610354590452,0.15823903679847717
9344,-Metrics/Training(Step): loss,1610354592503,0.1521516889333725
9346,-Metrics/Training(Step): loss,1610354594521,0.12141599506139755
9348,-Metrics/Training(Step): loss,1610354596578,0.10738939046859741
9350,-Metrics/Training(Step): loss,1610354598684,0.13937242329120636
9352,-Metrics/Training(Step): loss,1610354600606,0.1372578889131546
9354,-Metrics/Training(Step): loss,1610354602622,0.12809261679649353
9356,-Metrics/Training(Step): loss,1610354604547,0.10021918267011642
9358,-Metrics/Training(Step): loss,1610354606594,0.14846408367156982
9360,-Metrics/Training(Step): loss,1610354608622,0.11874857544898987
9362,-Metrics/Training(Step): loss,1610354610454,0.14770002663135529
9364,-Metrics/Training(Step): loss,1610354612506,0.14013825356960297
9366,-Metrics/Training(Step): loss,1610354614576,0.11922083050012589
9368,-Metrics/Training(Step): loss,1610354616499,0.12809428572654724
9370,-Metrics/Training(Step): loss,1610354618558,0.141144797205925
9372,-Metrics/Training(Step): loss,1610354620641,0.12064680457115173
9374,-Metrics/Training(Step): loss,1610354622669,0.1289912462234497
9376,-Metrics/Training(Step): loss,1610354637028,0.1545707732439041
9378,-Metrics/Training(Step): loss,1610354641019,0.1378319263458252
9380,-Metrics/Training(Step): loss,1610354645137,0.1059781089425087
9382,-Metrics/Training(Step): loss,1610354649220,0.12009397149085999
9384,-Metrics/Training(Step): loss,1610354653319,0.1361757218837738
9386,-Metrics/Training(Step): loss,1610354656924,0.133053719997406
9388,-Metrics/Training(Step): loss,1610354661118,0.14029519259929657
9390,-Metrics/Training(Step): loss,1610354664938,0.12551116943359375
9392,-Metrics/Training(Step): loss,1610354668390,0.14260105788707733
9394,-Metrics/Training(Step): loss,1610354671824,0.13170483708381653
9396,-Metrics/Training(Step): loss,1610354675419,0.1709049493074417
9398,-Metrics/Training(Step): loss,1610354678551,0.11584557592868805
9400,-Metrics/Training(Step): loss,1610354682415,0.1280447244644165
9402,-Metrics/Training(Step): loss,1610354686113,0.1161498874425888
9404,-Metrics/Training(Step): loss,1610354689145,0.10276782512664795
9406,-Metrics/Training(Step): loss,1610354692424,0.12978576123714447
9408,-Metrics/Training(Step): loss,1610354695640,0.14990724623203278
9410,-Metrics/Training(Step): loss,1610354699496,0.12586210668087006
9412,-Metrics/Training(Step): loss,1610354703219,0.10383501648902893
9414,-Metrics/Training(Step): loss,1610354706947,0.1449204832315445
9416,-Metrics/Training(Step): loss,1610354710247,0.11866805702447891
9418,-Metrics/Training(Step): loss,1610354714231,0.12696392834186554
9420,-Metrics/Training(Step): loss,1610354717843,0.08876468986272812
9422,-Metrics/Training(Step): loss,1610354720597,0.14144425094127655
9424,-Metrics/Training(Step): loss,1610354722852,0.1199837401509285
9426,-Metrics/Training(Step): loss,1610354724848,0.1050650104880333
9428,-Metrics/Training(Step): loss,1610354726913,0.14632295072078705
9430,-Metrics/Training(Step): loss,1610354728969,0.10736209154129028
9432,-Metrics/Training(Step): loss,1610354731030,0.11557716876268387
9434,-Metrics/Training(Step): loss,1610354733150,0.1236829087138176
9436,-Metrics/Training(Step): loss,1610354735200,0.12669877707958221
9438,-Metrics/Training(Step): loss,1610354737214,0.1352134793996811
9440,-Metrics/Training(Step): loss,1610354739193,0.11828234791755676
9442,-Metrics/Training(Step): loss,1610354741112,0.11586539447307587
9444,-Metrics/Training(Step): loss,1610354743225,0.09647919237613678
9446,-Metrics/Training(Step): loss,1610354745254,0.10342094302177429
9448,-Metrics/Training(Step): loss,1610354747122,0.13494038581848145
9450,-Metrics/Training(Step): loss,1610354749130,0.1303788721561432
9452,-Metrics/Training(Step): loss,1610354751246,0.13935071229934692
9454,-Metrics/Training(Step): loss,1610354753335,0.14811573922634125
9456,-Metrics/Training(Step): loss,1610354755409,0.11152137070894241
9458,-Metrics/Training(Step): loss,1610354757403,0.12459281831979752
9460,-Metrics/Training(Step): loss,1610354759433,0.12368763238191605
9462,-Metrics/Training(Step): loss,1610354772726,0.1150708943605423
9464,-Metrics/Training(Step): loss,1610354776719,0.1220845878124237
9466,-Metrics/Training(Step): loss,1610354780420,0.12164623290300369
9468,-Metrics/Training(Step): loss,1610354784258,0.13972045481204987
9470,-Metrics/Training(Step): loss,1610354788143,0.12496201694011688
9472,-Metrics/Training(Step): loss,1610354792058,0.1328035145998001
9474,-Metrics/Training(Step): loss,1610354796120,0.11927849799394608
9476,-Metrics/Training(Step): loss,1610354800319,0.13383564352989197
9478,-Metrics/Training(Step): loss,1610354804119,0.15307340025901794
9480,-Metrics/Training(Step): loss,1610354807632,0.12516874074935913
9482,-Metrics/Training(Step): loss,1610354811444,0.15401633083820343
9484,-Metrics/Training(Step): loss,1610354815619,0.11710061132907867
9486,-Metrics/Training(Step): loss,1610354819021,0.13443459570407867
9488,-Metrics/Training(Step): loss,1610354823094,0.12381163984537125
9490,-Metrics/Training(Step): loss,1610354826439,0.1428045779466629
9492,-Metrics/Training(Step): loss,1610354830021,0.11152145266532898
9494,-Metrics/Training(Step): loss,1610354833520,0.1326303333044052
9496,-Metrics/Training(Step): loss,1610354837120,0.12432210147380829
9498,-Metrics/Training(Step): loss,1610354840267,0.09283513575792313
9500,-Metrics/Training(Step): loss,1610354843719,0.12758983671665192
9502,-Metrics/Training(Step): loss,1610354846898,0.17193861305713654
9504,-Metrics/Training(Step): loss,1610354850664,0.1247650608420372
9506,-Metrics/Training(Step): loss,1610354854459,0.11526292562484741
9508,-Metrics/Training(Step): loss,1610354857236,0.0938500463962555
9510,-Metrics/Training(Step): loss,1610354859239,0.11142461746931076
9512,-Metrics/Training(Step): loss,1610354861259,0.1396375149488449
9514,-Metrics/Training(Step): loss,1610354863343,0.1286497414112091
9516,-Metrics/Training(Step): loss,1610354865342,0.148991659283638
9518,-Metrics/Training(Step): loss,1610354867434,0.1379980593919754
9520,-Metrics/Training(Step): loss,1610354869534,0.1379823088645935
9522,-Metrics/Training(Step): loss,1610354871581,0.11405579000711441
9524,-Metrics/Training(Step): loss,1610354873591,0.11981767416000366
9526,-Metrics/Training(Step): loss,1610354875740,0.11877816170454025
9528,-Metrics/Training(Step): loss,1610354877699,0.1274912804365158
9530,-Metrics/Training(Step): loss,1610354879486,0.12669146060943604
9532,-Metrics/Training(Step): loss,1610354881406,0.13448263704776764
9534,-Metrics/Training(Step): loss,1610354883491,0.1417127251625061
9536,-Metrics/Training(Step): loss,1610354885571,0.12294739484786987
9538,-Metrics/Training(Step): loss,1610354887552,0.12248684465885162
9540,-Metrics/Training(Step): loss,1610354889571,0.08827042579650879
9542,-Metrics/Training(Step): loss,1610354891637,0.14697493612766266
9544,-Metrics/Training(Step): loss,1610354893719,0.14115966856479645
9546,-Metrics/Training(Step): loss,1610354895773,0.13683804869651794
9548,-Metrics/Training(Step): loss,1610354908726,0.13837888836860657
9550,-Metrics/Training(Step): loss,1610354912619,0.12953750789165497
9552,-Metrics/Training(Step): loss,1610354916736,0.12073441594839096
9554,-Metrics/Training(Step): loss,1610354920824,0.14284063875675201
9556,-Metrics/Training(Step): loss,1610354924552,0.1079874038696289
9558,-Metrics/Training(Step): loss,1610354928430,0.13753041625022888
9560,-Metrics/Training(Step): loss,1610354932725,0.11413127183914185
9562,-Metrics/Training(Step): loss,1610354936722,0.1009063869714737
9564,-Metrics/Training(Step): loss,1610354940220,0.11828294396400452
9566,-Metrics/Training(Step): loss,1610354943579,0.09715136885643005
9568,-Metrics/Training(Step): loss,1610354947042,0.13933439552783966
9570,-Metrics/Training(Step): loss,1610354950721,0.12595213949680328
9572,-Metrics/Training(Step): loss,1610354954220,0.10699135810136795
9574,-Metrics/Training(Step): loss,1610354957718,0.14266589283943176
9576,-Metrics/Training(Step): loss,1610354961132,0.1330796629190445
9578,-Metrics/Training(Step): loss,1610354964519,0.12899136543273926
9580,-Metrics/Training(Step): loss,1610354968130,0.15362311899662018
9582,-Metrics/Training(Step): loss,1610354972219,0.13276290893554688
9584,-Metrics/Training(Step): loss,1610354975460,0.112482450902462
9586,-Metrics/Training(Step): loss,1610354978913,0.1344306617975235
9588,-Metrics/Training(Step): loss,1610354982320,0.1643979400396347
9590,-Metrics/Training(Step): loss,1610354985509,0.12856799364089966
9592,-Metrics/Training(Step): loss,1610354988955,0.11760702729225159
9594,-Metrics/Training(Step): loss,1610354992252,0.11149343103170395
9596,-Metrics/Training(Step): loss,1610354994941,0.12877151370048523
9598,-Metrics/Training(Step): loss,1610354997021,0.12590394914150238
9600,-Metrics/Training(Step): loss,1610354999055,0.1336098164319992
9602,-Metrics/Training(Step): loss,1610355001163,0.1308530867099762
9604,-Metrics/Training(Step): loss,1610355003344,0.1351509690284729
9606,-Metrics/Training(Step): loss,1610355005383,0.14219006896018982
9608,-Metrics/Training(Step): loss,1610355007498,0.09332570433616638
9610,-Metrics/Training(Step): loss,1610355009317,0.10862530767917633
9612,-Metrics/Training(Step): loss,1610355011426,0.1473325788974762
9614,-Metrics/Training(Step): loss,1610355013286,0.1456155925989151
9616,-Metrics/Training(Step): loss,1610355015383,0.1247982606291771
9618,-Metrics/Training(Step): loss,1610355017392,0.12231288850307465
9620,-Metrics/Training(Step): loss,1610355019445,0.11647386103868484
9622,-Metrics/Training(Step): loss,1610355021424,0.12931780517101288
9624,-Metrics/Training(Step): loss,1610355023377,0.14005419611930847
9626,-Metrics/Training(Step): loss,1610355025445,0.14630113542079926
9628,-Metrics/Training(Step): loss,1610355027535,0.15188871324062347
9630,-Metrics/Training(Step): loss,1610355029654,0.11388643831014633
9632,-Metrics/Training(Step): loss,1610355031735,0.13677260279655457
9634,-Metrics/Training(Step): loss,1610355044719,0.14215171337127686
9636,-Metrics/Training(Step): loss,1610355048720,0.11926564574241638
9638,-Metrics/Training(Step): loss,1610355052455,0.11522388458251953
9640,-Metrics/Training(Step): loss,1610355056321,0.13471390306949615
9642,-Metrics/Training(Step): loss,1610355060347,0.11633874475955963
9644,-Metrics/Training(Step): loss,1610355064421,0.1506415158510208
9646,-Metrics/Training(Step): loss,1610355067822,0.12611939013004303
9648,-Metrics/Training(Step): loss,1610355071250,0.10716506838798523
9650,-Metrics/Training(Step): loss,1610355074820,0.10434959828853607
9652,-Metrics/Training(Step): loss,1610355078450,0.11946228891611099
9654,-Metrics/Training(Step): loss,1610355081656,0.13858582079410553
9656,-Metrics/Training(Step): loss,1610355085232,0.1152585968375206
9658,-Metrics/Training(Step): loss,1610355088727,0.13928347826004028
9660,-Metrics/Training(Step): loss,1610355092319,0.1315860003232956
9662,-Metrics/Training(Step): loss,1610355095960,0.10619189590215683
9664,-Metrics/Training(Step): loss,1610355099640,0.11306295543909073
9666,-Metrics/Training(Step): loss,1610355103016,0.15979179739952087
9668,-Metrics/Training(Step): loss,1610355106620,0.14464014768600464
9670,-Metrics/Training(Step): loss,1610355109639,0.10166700184345245
9672,-Metrics/Training(Step): loss,1610355112759,0.13653701543807983
9674,-Metrics/Training(Step): loss,1610355116220,0.15214803814888
9676,-Metrics/Training(Step): loss,1610355119740,0.17375360429286957
9678,-Metrics/Training(Step): loss,1610355123030,0.17104458808898926
9680,-Metrics/Training(Step): loss,1610355126585,0.17342080175876617
9682,-Metrics/Training(Step): loss,1610355129123,0.15379859507083893
9684,-Metrics/Training(Step): loss,1610355131464,0.08780568093061447
9686,-Metrics/Training(Step): loss,1610355133522,0.14504748582839966
9688,-Metrics/Training(Step): loss,1610355135698,0.10797243565320969
9690,-Metrics/Training(Step): loss,1610355137752,0.10836964845657349
9692,-Metrics/Training(Step): loss,1610355139854,0.13666929304599762
9694,-Metrics/Training(Step): loss,1610355141870,0.14342251420021057
9696,-Metrics/Training(Step): loss,1610355143869,0.1305292546749115
9698,-Metrics/Training(Step): loss,1610355145969,0.1415189802646637
9700,-Metrics/Training(Step): loss,1610355148106,0.09879522025585175
9702,-Metrics/Training(Step): loss,1610355150183,0.12401888519525528
9704,-Metrics/Training(Step): loss,1610355152241,0.13104507327079773
9706,-Metrics/Training(Step): loss,1610355154286,0.11718674749135971
9708,-Metrics/Training(Step): loss,1610355156301,0.11384177953004837
9710,-Metrics/Training(Step): loss,1610355158302,0.1692088544368744
9712,-Metrics/Training(Step): loss,1610355160337,0.11688381433486938
9714,-Metrics/Training(Step): loss,1610355162316,0.11579333245754242
9716,-Metrics/Training(Step): loss,1610355164376,0.13476912677288055
9718,-Metrics/Training(Step): loss,1610355166417,0.13350467383861542
9720,-Metrics/Training(Step): loss,1610355179329,0.1092090904712677
9722,-Metrics/Training(Step): loss,1610355183529,0.1653200387954712
9724,-Metrics/Training(Step): loss,1610355187420,0.1137305274605751
9726,-Metrics/Training(Step): loss,1610355191525,0.13346314430236816
9728,-Metrics/Training(Step): loss,1610355195719,0.13551117479801178
9730,-Metrics/Training(Step): loss,1610355199521,0.11952528357505798
9732,-Metrics/Training(Step): loss,1610355203519,0.12283352017402649
9734,-Metrics/Training(Step): loss,1610355207515,0.153639554977417
9736,-Metrics/Training(Step): loss,1610355211278,0.11908993870019913
9738,-Metrics/Training(Step): loss,1610355215021,0.129182368516922
9740,-Metrics/Training(Step): loss,1610355218619,0.14211681485176086
9742,-Metrics/Training(Step): loss,1610355222323,0.12917666137218475
9744,-Metrics/Training(Step): loss,1610355225921,0.11600431054830551
9746,-Metrics/Training(Step): loss,1610355229420,0.16342777013778687
9748,-Metrics/Training(Step): loss,1610355233321,0.14665795862674713
9750,-Metrics/Training(Step): loss,1610355236821,0.1284913569688797
9752,-Metrics/Training(Step): loss,1610355240152,0.13212786614894867
9754,-Metrics/Training(Step): loss,1610355243772,0.10385121405124664
9756,-Metrics/Training(Step): loss,1610355247317,0.12220604717731476
9758,-Metrics/Training(Step): loss,1610355250519,0.1193031370639801
9760,-Metrics/Training(Step): loss,1610355254019,0.1251857578754425
9762,-Metrics/Training(Step): loss,1610355257830,0.1308969408273697
9764,-Metrics/Training(Step): loss,1610355261082,0.13252897560596466
9766,-Metrics/Training(Step): loss,1610355264240,0.12265705317258835
9768,-Metrics/Training(Step): loss,1610355266513,0.09216705709695816
9770,-Metrics/Training(Step): loss,1610355268564,0.16807129979133606
9772,-Metrics/Training(Step): loss,1610355270605,0.10503129661083221
9774,-Metrics/Training(Step): loss,1610355272707,0.12509702146053314
9776,-Metrics/Training(Step): loss,1610355274805,0.10931730270385742
9778,-Metrics/Training(Step): loss,1610355276905,0.1350279301404953
9780,-Metrics/Training(Step): loss,1610355279048,0.1327725648880005
9782,-Metrics/Training(Step): loss,1610355281096,0.15696637332439423
9784,-Metrics/Training(Step): loss,1610355283152,0.14716193079948425
9786,-Metrics/Training(Step): loss,1610355285279,0.12628009915351868
9788,-Metrics/Training(Step): loss,1610355287328,0.11535593122243881
9790,-Metrics/Training(Step): loss,1610355289414,0.10599212348461151
9792,-Metrics/Training(Step): loss,1610355291387,0.14440152049064636
9794,-Metrics/Training(Step): loss,1610355293451,0.14660018682479858
9796,-Metrics/Training(Step): loss,1610355295347,0.11944784969091415
9798,-Metrics/Training(Step): loss,1610355297435,0.14820435643196106
9800,-Metrics/Training(Step): loss,1610355299506,0.14447073638439178
9802,-Metrics/Training(Step): loss,1610355301482,0.12418113648891449
9804,-Metrics/Training(Step): loss,1610355303557,0.11342059820890427
9806,-Metrics/Training(Step): loss,1610355315639,0.11403468251228333
9808,-Metrics/Training(Step): loss,1610355319819,0.1275450438261032
9810,-Metrics/Training(Step): loss,1610355323847,0.09114924818277359
9812,-Metrics/Training(Step): loss,1610355327718,0.11985993385314941
9814,-Metrics/Training(Step): loss,1610355331658,0.11777675896883011
9816,-Metrics/Training(Step): loss,1610355335621,0.12014905363321304
9818,-Metrics/Training(Step): loss,1610355339534,0.1290377378463745
9820,-Metrics/Training(Step): loss,1610355343121,0.12038347125053406
9822,-Metrics/Training(Step): loss,1610355346919,0.13057290017604828
9824,-Metrics/Training(Step): loss,1610355350220,0.1309613585472107
9826,-Metrics/Training(Step): loss,1610355353822,0.1055062934756279
9828,-Metrics/Training(Step): loss,1610355356934,0.11785057932138443
9830,-Metrics/Training(Step): loss,1610355361120,0.13438594341278076
9832,-Metrics/Training(Step): loss,1610355364616,0.13083790242671967
9834,-Metrics/Training(Step): loss,1610355368224,0.10264232754707336
9836,-Metrics/Training(Step): loss,1610355371994,0.09514710307121277
9838,-Metrics/Training(Step): loss,1610355374948,0.12098316848278046
9840,-Metrics/Training(Step): loss,1610355378121,0.14864309132099152
9842,-Metrics/Training(Step): loss,1610355381920,0.11280811578035355
9844,-Metrics/Training(Step): loss,1610355385038,0.12347273528575897
9846,-Metrics/Training(Step): loss,1610355388435,0.1149645522236824
9848,-Metrics/Training(Step): loss,1610355392219,0.107734814286232
9850,-Metrics/Training(Step): loss,1610355395450,0.13903780281543732
9852,-Metrics/Training(Step): loss,1610355398719,0.12186700850725174
9854,-Metrics/Training(Step): loss,1610355401163,0.1286359578371048
9856,-Metrics/Training(Step): loss,1610355403475,0.1466328501701355
9858,-Metrics/Training(Step): loss,1610355405554,0.09365642070770264
9860,-Metrics/Training(Step): loss,1610355407662,0.21673664450645447
9862,-Metrics/Training(Step): loss,1610355409832,0.11636210978031158
9864,-Metrics/Training(Step): loss,1610355411932,0.14019662141799927
9866,-Metrics/Training(Step): loss,1610355413946,0.14457738399505615
9868,-Metrics/Training(Step): loss,1610355415989,0.12762151658535004
9870,-Metrics/Training(Step): loss,1610355418022,0.12944909930229187
9872,-Metrics/Training(Step): loss,1610355420071,0.12472106516361237
9874,-Metrics/Training(Step): loss,1610355421941,0.11578322947025299
9876,-Metrics/Training(Step): loss,1610355423954,0.1441033035516739
9878,-Metrics/Training(Step): loss,1610355425907,0.17003560066223145
9880,-Metrics/Training(Step): loss,1610355427935,0.11736216396093369
9882,-Metrics/Training(Step): loss,1610355429910,0.15079542994499207
9884,-Metrics/Training(Step): loss,1610355431799,0.12925122678279877
9886,-Metrics/Training(Step): loss,1610355433870,0.13550691306591034
9888,-Metrics/Training(Step): loss,1610355435901,0.1327539086341858
9890,-Metrics/Training(Step): loss,1610355437973,0.09894926846027374
9892,-Metrics/Training(Step): loss,1610355451630,0.09570524096488953
9894,-Metrics/Training(Step): loss,1610355455523,0.11295992136001587
9896,-Metrics/Training(Step): loss,1610355459358,0.1324567347764969
9898,-Metrics/Training(Step): loss,1610355463444,0.11418841779232025
9900,-Metrics/Training(Step): loss,1610355467820,0.14745596051216125
9902,-Metrics/Training(Step): loss,1610355471719,0.12371768057346344
9904,-Metrics/Training(Step): loss,1610355474748,0.11171653121709824
9906,-Metrics/Training(Step): loss,1610355478321,0.13736292719841003
9908,-Metrics/Training(Step): loss,1610355482043,0.13842591643333435
9910,-Metrics/Training(Step): loss,1610355486071,0.15713095664978027
9912,-Metrics/Training(Step): loss,1610355490419,0.15398500859737396
9914,-Metrics/Training(Step): loss,1610355494350,0.12722352147102356
9916,-Metrics/Training(Step): loss,1610355497819,0.11896436661481857
9918,-Metrics/Training(Step): loss,1610355501723,0.11838170886039734
9920,-Metrics/Training(Step): loss,1610355505265,0.1490214765071869
9922,-Metrics/Training(Step): loss,1610355508724,0.09815093129873276
9924,-Metrics/Training(Step): loss,1610355512520,0.10545092821121216
9926,-Metrics/Training(Step): loss,1610355516222,0.12309159338474274
9928,-Metrics/Training(Step): loss,1610355519487,0.10585296899080276
9930,-Metrics/Training(Step): loss,1610355523609,0.16725169122219086
9932,-Metrics/Training(Step): loss,1610355526840,0.1167609691619873
9934,-Metrics/Training(Step): loss,1610355530766,0.10590662062168121
9936,-Metrics/Training(Step): loss,1610355533677,0.12068991363048553
9938,-Metrics/Training(Step): loss,1610355535928,0.10848205536603928
9940,-Metrics/Training(Step): loss,1610355538309,0.15027634799480438
9942,-Metrics/Training(Step): loss,1610355540254,0.08728765696287155
9944,-Metrics/Training(Step): loss,1610355542320,0.11154524981975555
9946,-Metrics/Training(Step): loss,1610355544435,0.1343182623386383
9948,-Metrics/Training(Step): loss,1610355546554,0.10908988863229752
9950,-Metrics/Training(Step): loss,1610355548609,0.11978006362915039
9952,-Metrics/Training(Step): loss,1610355550582,0.1489638239145279
9954,-Metrics/Training(Step): loss,1610355552651,0.10160727053880692
9956,-Metrics/Training(Step): loss,1610355554765,0.16126298904418945
9958,-Metrics/Training(Step): loss,1610355556843,0.1606716513633728
9960,-Metrics/Training(Step): loss,1610355558962,0.12228015065193176
9962,-Metrics/Training(Step): loss,1610355560851,0.12957194447517395
9964,-Metrics/Training(Step): loss,1610355562965,0.13747110962867737
9966,-Metrics/Training(Step): loss,1610355565091,0.11169251799583435
9968,-Metrics/Training(Step): loss,1610355567041,0.12188076972961426
9970,-Metrics/Training(Step): loss,1610355568989,0.1275922805070877
9972,-Metrics/Training(Step): loss,1610355570930,0.13029035925865173
9974,-Metrics/Training(Step): loss,1610355572985,0.15919780731201172
9976,-Metrics/Training(Step): loss,1610355575091,0.1102292388677597
9978,-Metrics/Training(Step): loss,1610355587626,0.11426014453172684
9980,-Metrics/Training(Step): loss,1610355591725,0.10857938975095749
9982,-Metrics/Training(Step): loss,1610355595719,0.12229537218809128
9984,-Metrics/Training(Step): loss,1610355599920,0.14537231624126434
9986,-Metrics/Training(Step): loss,1610355603920,0.13460837304592133
9988,-Metrics/Training(Step): loss,1610355607816,0.11395048350095749
9990,-Metrics/Training(Step): loss,1610355611922,0.09597592800855637
9992,-Metrics/Training(Step): loss,1610355615020,0.13482294976711273
9994,-Metrics/Training(Step): loss,1610355618416,0.11930982023477554
9996,-Metrics/Training(Step): loss,1610355621528,0.17239190638065338
9998,-Metrics/Training(Step): loss,1610355624519,0.10451019555330276
10000,-Metrics/Training(Step): loss,1610355628319,0.14789438247680664
10002,-Metrics/Training(Step): loss,1610355632052,0.1314592957496643
10004,-Metrics/Training(Step): loss,1610355635789,0.14148573577404022
10006,-Metrics/Training(Step): loss,1610355639351,0.14050211012363434
10008,-Metrics/Training(Step): loss,1610355642846,0.09989605844020844
10010,-Metrics/Training(Step): loss,1610355646725,0.14630070328712463
10012,-Metrics/Training(Step): loss,1610355650420,0.0883423388004303
10014,-Metrics/Training(Step): loss,1610355653822,0.12044627964496613
10016,-Metrics/Training(Step): loss,1610355656443,0.1004040539264679
10018,-Metrics/Training(Step): loss,1610355660003,0.12360118329524994
10020,-Metrics/Training(Step): loss,1610355663338,0.1462850570678711
10022,-Metrics/Training(Step): loss,1610355666675,0.10661505162715912
10024,-Metrics/Training(Step): loss,1610355670560,0.09597079455852509
10026,-Metrics/Training(Step): loss,1610355673696,0.13245554268360138
10028,-Metrics/Training(Step): loss,1610355675842,0.09006662666797638
10030,-Metrics/Training(Step): loss,1610355677892,0.11839275807142258
10032,-Metrics/Training(Step): loss,1610355679956,0.1338556557893753
10034,-Metrics/Training(Step): loss,1610355682076,0.16037209331989288
10036,-Metrics/Training(Step): loss,1610355684180,0.14810948073863983
10038,-Metrics/Training(Step): loss,1610355686315,0.15189507603645325
10040,-Metrics/Training(Step): loss,1610355688219,0.1432904601097107
10042,-Metrics/Training(Step): loss,1610355690392,0.09903059899806976
10044,-Metrics/Training(Step): loss,1610355692444,0.08987586200237274
10046,-Metrics/Training(Step): loss,1610355694383,0.10088524222373962
10048,-Metrics/Training(Step): loss,1610355696439,0.11122822761535645
10050,-Metrics/Training(Step): loss,1610355698492,0.14817318320274353
10052,-Metrics/Training(Step): loss,1610355700593,0.13388299942016602
10054,-Metrics/Training(Step): loss,1610355702658,0.16407975554466248
10056,-Metrics/Training(Step): loss,1610355704675,0.11263187974691391
10058,-Metrics/Training(Step): loss,1610355706649,0.1312311440706253
10060,-Metrics/Training(Step): loss,1610355708697,0.11959116160869598
10062,-Metrics/Training(Step): loss,1610355710744,0.13261233270168304
10064,-Metrics/Training(Step): loss,1610355723640,0.11704202741384506
10066,-Metrics/Training(Step): loss,1610355727731,0.10277342051267624
10068,-Metrics/Training(Step): loss,1610355731720,0.136622816324234
10070,-Metrics/Training(Step): loss,1610355735720,0.14160722494125366
10072,-Metrics/Training(Step): loss,1610355739719,0.11414911597967148
10074,-Metrics/Training(Step): loss,1610355743820,0.10251174122095108
10076,-Metrics/Training(Step): loss,1610355747432,0.16257494688034058
10078,-Metrics/Training(Step): loss,1610355751035,0.14654085040092468
10080,-Metrics/Training(Step): loss,1610355754822,0.11390798538923264
10082,-Metrics/Training(Step): loss,1610355758323,0.145558163523674
10084,-Metrics/Training(Step): loss,1610355761920,0.1228698343038559
10086,-Metrics/Training(Step): loss,1610355766222,0.12647968530654907
10088,-Metrics/Training(Step): loss,1610355769720,0.13416044414043427
10090,-Metrics/Training(Step): loss,1610355773650,0.12791158258914948
10092,-Metrics/Training(Step): loss,1610355777360,0.15088047087192535
10094,-Metrics/Training(Step): loss,1610355781136,0.11257036030292511
10096,-Metrics/Training(Step): loss,1610355784509,0.11802094429731369
10098,-Metrics/Training(Step): loss,1610355787820,0.08830210566520691
10100,-Metrics/Training(Step): loss,1610355791480,0.11331384629011154
10102,-Metrics/Training(Step): loss,1610355795120,0.14659300446510315
10104,-Metrics/Training(Step): loss,1610355798380,0.1078614741563797
10106,-Metrics/Training(Step): loss,1610355801819,0.13992996513843536
10108,-Metrics/Training(Step): loss,1610355805387,0.11130381375551224
10110,-Metrics/Training(Step): loss,1610355808537,0.15118491649627686
10112,-Metrics/Training(Step): loss,1610355810676,0.08964639157056808
10114,-Metrics/Training(Step): loss,1610355812684,0.10014517605304718
10116,-Metrics/Training(Step): loss,1610355814789,0.13601911067962646
10118,-Metrics/Training(Step): loss,1610355816861,0.11510046571493149
10120,-Metrics/Training(Step): loss,1610355818944,0.12453324347734451
10122,-Metrics/Training(Step): loss,1610355820996,0.13502323627471924
10124,-Metrics/Training(Step): loss,1610355823095,0.12321895360946655
10126,-Metrics/Training(Step): loss,1610355825213,0.0957372859120369
10128,-Metrics/Training(Step): loss,1610355827317,0.12481790035963058
10130,-Metrics/Training(Step): loss,1610355829299,0.1084059402346611
10132,-Metrics/Training(Step): loss,1610355831245,0.13205258548259735
10134,-Metrics/Training(Step): loss,1610355833357,0.15102894604206085
10136,-Metrics/Training(Step): loss,1610355835338,0.07986728847026825
10138,-Metrics/Training(Step): loss,1610355837207,0.13492725789546967
10140,-Metrics/Training(Step): loss,1610355839204,0.08274121582508087
10142,-Metrics/Training(Step): loss,1610355841258,0.1405048668384552
10144,-Metrics/Training(Step): loss,1610355843353,0.12172296643257141
10146,-Metrics/Training(Step): loss,1610355845436,0.11790619045495987
10148,-Metrics/Training(Step): loss,1610355847473,0.11166457831859589
10150,-Metrics/Training(Step): loss,1610355859330,0.08686632663011551
10152,-Metrics/Training(Step): loss,1610355863427,0.13374696671962738
10154,-Metrics/Training(Step): loss,1610355867520,0.1243644505739212
10156,-Metrics/Training(Step): loss,1610355871415,0.1273263841867447
10158,-Metrics/Training(Step): loss,1610355875326,0.1352858543395996
10160,-Metrics/Training(Step): loss,1610355879156,0.1515113115310669
10162,-Metrics/Training(Step): loss,1610355883158,0.1166168749332428
10164,-Metrics/Training(Step): loss,1610355887050,0.15017299354076385
10166,-Metrics/Training(Step): loss,1610355890742,0.13275311887264252
10168,-Metrics/Training(Step): loss,1610355894636,0.15900684893131256
10170,-Metrics/Training(Step): loss,1610355898673,0.14978253841400146
10172,-Metrics/Training(Step): loss,1610355902104,0.1652160882949829
10174,-Metrics/Training(Step): loss,1610355905242,0.1499250829219818
10176,-Metrics/Training(Step): loss,1610355908927,0.13742494583129883
10178,-Metrics/Training(Step): loss,1610355912836,0.12286801636219025
10180,-Metrics/Training(Step): loss,1610355916716,0.1056126058101654
10182,-Metrics/Training(Step): loss,1610355920519,0.11920301616191864
10184,-Metrics/Training(Step): loss,1610355924020,0.12101326137781143
10186,-Metrics/Training(Step): loss,1610355927757,0.07093330472707748
10188,-Metrics/Training(Step): loss,1610355931263,0.13808245956897736
10190,-Metrics/Training(Step): loss,1610355934539,0.12429887801408768
10192,-Metrics/Training(Step): loss,1610355938022,0.14369480311870575
10194,-Metrics/Training(Step): loss,1610355941721,0.1263299137353897
10196,-Metrics/Training(Step): loss,1610355944529,0.1543172150850296
10198,-Metrics/Training(Step): loss,1610355946719,0.10378605127334595
10200,-Metrics/Training(Step): loss,1610355948886,0.11953077465295792
10202,-Metrics/Training(Step): loss,1610355951007,0.10427561402320862
10204,-Metrics/Training(Step): loss,1610355953052,0.14132905006408691
10206,-Metrics/Training(Step): loss,1610355955169,0.11687362194061279
10208,-Metrics/Training(Step): loss,1610355957315,0.1300804764032364
10210,-Metrics/Training(Step): loss,1610355959421,0.12389706075191498
10212,-Metrics/Training(Step): loss,1610355961526,0.10188578814268112
10214,-Metrics/Training(Step): loss,1610355963577,0.11757966876029968
10216,-Metrics/Training(Step): loss,1610355965718,0.1201116144657135
10218,-Metrics/Training(Step): loss,1610355967770,0.1332237422466278
10220,-Metrics/Training(Step): loss,1610355969774,0.14294792711734772
10222,-Metrics/Training(Step): loss,1610355971820,0.09024742990732193
10224,-Metrics/Training(Step): loss,1610355973888,0.09764301776885986
10226,-Metrics/Training(Step): loss,1610355975878,0.1198539212346077
10228,-Metrics/Training(Step): loss,1610355977905,0.1294242888689041
10230,-Metrics/Training(Step): loss,1610355979975,0.0958842858672142
10232,-Metrics/Training(Step): loss,1610355982036,0.12121468782424927
10234,-Metrics/Training(Step): loss,1610355984061,0.12000640481710434
10236,-Metrics/Training(Step): loss,1610355997748,0.1202269196510315
10238,-Metrics/Training(Step): loss,1610356001758,0.09604844450950623
10240,-Metrics/Training(Step): loss,1610356006020,0.11507190763950348
10242,-Metrics/Training(Step): loss,1610356010031,0.11422808468341827
10244,-Metrics/Training(Step): loss,1610356013719,0.1378287672996521
10246,-Metrics/Training(Step): loss,1610356017720,0.14528244733810425
10248,-Metrics/Training(Step): loss,1610356021549,0.14107628166675568
10250,-Metrics/Training(Step): loss,1610356025356,0.131643608212471
10252,-Metrics/Training(Step): loss,1610356029220,0.14238305389881134
10254,-Metrics/Training(Step): loss,1610356032615,0.12558704614639282
10256,-Metrics/Training(Step): loss,1610356036319,0.10629720240831375
10258,-Metrics/Training(Step): loss,1610356039920,0.13240553438663483
10260,-Metrics/Training(Step): loss,1610356043519,0.14290829002857208
10262,-Metrics/Training(Step): loss,1610356046431,0.12732616066932678
10264,-Metrics/Training(Step): loss,1610356050320,0.1188177615404129
10266,-Metrics/Training(Step): loss,1610356053942,0.11003592610359192
10268,-Metrics/Training(Step): loss,1610356056886,0.12347964942455292
10270,-Metrics/Training(Step): loss,1610356061060,0.13738197088241577
10272,-Metrics/Training(Step): loss,1610356064819,0.09134320169687271
10274,-Metrics/Training(Step): loss,1610356067865,0.11222490668296814
10276,-Metrics/Training(Step): loss,1610356071738,0.12691748142242432
10278,-Metrics/Training(Step): loss,1610356075296,0.10998239368200302
10280,-Metrics/Training(Step): loss,1610356078645,0.10147851705551147
10282,-Metrics/Training(Step): loss,1610356081926,0.14123955368995667
10284,-Metrics/Training(Step): loss,1610356084720,0.11071185022592545
10286,-Metrics/Training(Step): loss,1610356086791,0.11614863574504852
10288,-Metrics/Training(Step): loss,1610356088872,0.12155550718307495
10290,-Metrics/Training(Step): loss,1610356091034,0.10849542170763016
10292,-Metrics/Training(Step): loss,1610356093175,0.16240258514881134
10294,-Metrics/Training(Step): loss,1610356095071,0.11504780501127243
10296,-Metrics/Training(Step): loss,1610356097064,0.14670878648757935
10298,-Metrics/Training(Step): loss,1610356099106,0.11048576235771179
10300,-Metrics/Training(Step): loss,1610356101188,0.140082448720932
10302,-Metrics/Training(Step): loss,1610356103119,0.09728657454252243
10304,-Metrics/Training(Step): loss,1610356105246,0.12656934559345245
10306,-Metrics/Training(Step): loss,1610356107362,0.10730300843715668
10308,-Metrics/Training(Step): loss,1610356109474,0.11454993486404419
10310,-Metrics/Training(Step): loss,1610356111490,0.10655104368925095
10312,-Metrics/Training(Step): loss,1610356113491,0.11902669817209244
10314,-Metrics/Training(Step): loss,1610356115318,0.128948375582695
10316,-Metrics/Training(Step): loss,1610356117205,0.11699163168668747
10318,-Metrics/Training(Step): loss,1610356119249,0.1205105409026146
10320,-Metrics/Training(Step): loss,1610356121291,0.11031284183263779
10322,-Metrics/Training(Step): loss,1610356144129,0.12930531799793243
10324,-Metrics/Training(Step): loss,1610356148121,0.12473967671394348
10326,-Metrics/Training(Step): loss,1610356152220,0.10130950063467026
10328,-Metrics/Training(Step): loss,1610356157323,0.1532842367887497
10330,-Metrics/Training(Step): loss,1610356161621,0.087080217897892
10332,-Metrics/Training(Step): loss,1610356165720,0.14414305984973907
10334,-Metrics/Training(Step): loss,1610356169648,0.1407356560230255
10336,-Metrics/Training(Step): loss,1610356173819,0.12738323211669922
10338,-Metrics/Training(Step): loss,1610356177821,0.09599591791629791
10340,-Metrics/Training(Step): loss,1610356181620,0.1295284628868103
10342,-Metrics/Training(Step): loss,1610356185895,0.12598799169063568
10344,-Metrics/Training(Step): loss,1610356189313,0.1652766317129135
10346,-Metrics/Training(Step): loss,1610356193119,0.16810844838619232
10348,-Metrics/Training(Step): loss,1610356196622,0.14083772897720337
10350,-Metrics/Training(Step): loss,1610356200031,0.12107916176319122
10352,-Metrics/Training(Step): loss,1610356203342,0.1690441071987152
10354,-Metrics/Training(Step): loss,1610356207258,0.1464887410402298
10356,-Metrics/Training(Step): loss,1610356210886,0.1715974062681198
10358,-Metrics/Training(Step): loss,1610356214241,0.23267491161823273
10360,-Metrics/Training(Step): loss,1610356217831,0.1739436537027359
10362,-Metrics/Training(Step): loss,1610356221120,0.15978683531284332
10364,-Metrics/Training(Step): loss,1610356224340,0.15789461135864258
10366,-Metrics/Training(Step): loss,1610356227460,0.15729016065597534
10368,-Metrics/Training(Step): loss,1610356230022,0.13774292171001434
10370,-Metrics/Training(Step): loss,1610356232177,0.11796251684427261
10372,-Metrics/Training(Step): loss,1610356234472,0.18065062165260315
10374,-Metrics/Training(Step): loss,1610356236620,0.13074138760566711
10376,-Metrics/Training(Step): loss,1610356238731,0.12118227034807205
10378,-Metrics/Training(Step): loss,1610356240823,0.15167605876922607
10380,-Metrics/Training(Step): loss,1610356242904,0.12980924546718597
10382,-Metrics/Training(Step): loss,1610356244809,0.1627901792526245
10384,-Metrics/Training(Step): loss,1610356246852,0.14981408417224884
10386,-Metrics/Training(Step): loss,1610356248852,0.1358334869146347
10388,-Metrics/Training(Step): loss,1610356250876,0.13353683054447174
10390,-Metrics/Training(Step): loss,1610356253032,0.12852376699447632
10392,-Metrics/Training(Step): loss,1610356255154,0.09903452545404434
10394,-Metrics/Training(Step): loss,1610356257074,0.10862747579813004
10396,-Metrics/Training(Step): loss,1610356259129,0.13186170160770416
10398,-Metrics/Training(Step): loss,1610356261098,0.11257638782262802
10400,-Metrics/Training(Step): loss,1610356263118,0.1404372602701187
10402,-Metrics/Training(Step): loss,1610356265180,0.14669756591320038
10404,-Metrics/Training(Step): loss,1610356267101,0.11792537569999695
10406,-Metrics/Training(Step): loss,1610356269084,0.11659888178110123
10408,-Metrics/Training(Step): loss,1610356281744,0.12029320746660233
10410,-Metrics/Training(Step): loss,1610356285615,0.12891320884227753
10412,-Metrics/Training(Step): loss,1610356289327,0.15693965554237366
10414,-Metrics/Training(Step): loss,1610356293320,0.13723979890346527
10416,-Metrics/Training(Step): loss,1610356297316,0.11126621067523956
10418,-Metrics/Training(Step): loss,1610356301220,0.14440928399562836
10420,-Metrics/Training(Step): loss,1610356305121,0.13467901945114136
10422,-Metrics/Training(Step): loss,1610356309120,0.11665783822536469
10424,-Metrics/Training(Step): loss,1610356312820,0.12674804031848907
10426,-Metrics/Training(Step): loss,1610356316220,0.11550074070692062
10428,-Metrics/Training(Step): loss,1610356320304,0.12660470604896545
10430,-Metrics/Training(Step): loss,1610356324330,0.15760239958763123
10432,-Metrics/Training(Step): loss,1610356328627,0.12170729041099548
10434,-Metrics/Training(Step): loss,1610356332229,0.10121031850576401
10436,-Metrics/Training(Step): loss,1610356336020,0.14104880392551422
10438,-Metrics/Training(Step): loss,1610356339022,0.11129792779684067
10440,-Metrics/Training(Step): loss,1610356342541,0.12437737733125687
10442,-Metrics/Training(Step): loss,1610356346021,0.1377495527267456
10444,-Metrics/Training(Step): loss,1610356349520,0.09611320495605469
10446,-Metrics/Training(Step): loss,1610356353195,0.11702660471200943
10448,-Metrics/Training(Step): loss,1610356356819,0.12057767063379288
10450,-Metrics/Training(Step): loss,1610356360519,0.13564416766166687
10452,-Metrics/Training(Step): loss,1610356363221,0.1408117711544037
10454,-Metrics/Training(Step): loss,1610356366191,0.13907326757907867
10456,-Metrics/Training(Step): loss,1610356368219,0.11004043370485306
10458,-Metrics/Training(Step): loss,1610356370315,0.10496917366981506
10460,-Metrics/Training(Step): loss,1610356372389,0.1324687898159027
10462,-Metrics/Training(Step): loss,1610356374499,0.1192198097705841
10464,-Metrics/Training(Step): loss,1610356376587,0.1654181331396103
10466,-Metrics/Training(Step): loss,1610356378623,0.10958622395992279
10468,-Metrics/Training(Step): loss,1610356380689,0.14527645707130432
10470,-Metrics/Training(Step): loss,1610356382724,0.20003986358642578
10472,-Metrics/Training(Step): loss,1610356384654,0.12950186431407928
10474,-Metrics/Training(Step): loss,1610356386785,0.12107754498720169
10476,-Metrics/Training(Step): loss,1610356388838,0.1117156371474266
10478,-Metrics/Training(Step): loss,1610356390802,0.09964809566736221
10480,-Metrics/Training(Step): loss,1610356392826,0.12296027690172195
10482,-Metrics/Training(Step): loss,1610356394910,0.14867714047431946
10484,-Metrics/Training(Step): loss,1610356396889,0.15550756454467773
10486,-Metrics/Training(Step): loss,1610356398962,0.1348949819803238
10488,-Metrics/Training(Step): loss,1610356400937,0.12619474530220032
10490,-Metrics/Training(Step): loss,1610356403039,0.11641284823417664
10492,-Metrics/Training(Step): loss,1610356405077,0.1346028596162796
10494,-Metrics/Training(Step): loss,1610356417533,0.12854048609733582
10496,-Metrics/Training(Step): loss,1610356421720,0.12739388644695282
10498,-Metrics/Training(Step): loss,1610356425624,0.1357014924287796
10500,-Metrics/Training(Step): loss,1610356429520,0.1252218633890152
10502,-Metrics/Training(Step): loss,1610356433220,0.11833091080188751
10504,-Metrics/Training(Step): loss,1610356437121,0.15451717376708984
10506,-Metrics/Training(Step): loss,1610356441020,0.147414430975914
10508,-Metrics/Training(Step): loss,1610356445420,0.15758347511291504
10510,-Metrics/Training(Step): loss,1610356449423,0.10583622753620148
10512,-Metrics/Training(Step): loss,1610356452706,0.1303190290927887
10514,-Metrics/Training(Step): loss,1610356456021,0.13493379950523376
10516,-Metrics/Training(Step): loss,1610356459052,0.1349639743566513
10518,-Metrics/Training(Step): loss,1610356462823,0.11400392651557922
10520,-Metrics/Training(Step): loss,1610356466864,0.153244748711586
10522,-Metrics/Training(Step): loss,1610356470639,0.12651591002941132
10524,-Metrics/Training(Step): loss,1610356474042,0.12580910325050354
10526,-Metrics/Training(Step): loss,1610356477120,0.11705557256937027
10528,-Metrics/Training(Step): loss,1610356480251,0.12676382064819336
10530,-Metrics/Training(Step): loss,1610356484297,0.09941547363996506
10532,-Metrics/Training(Step): loss,1610356487616,0.129564568400383
10534,-Metrics/Training(Step): loss,1610356491321,0.11369503289461136
10536,-Metrics/Training(Step): loss,1610356494519,0.14594107866287231
10538,-Metrics/Training(Step): loss,1610356497920,0.10459323972463608
10540,-Metrics/Training(Step): loss,1610356501366,0.09489953517913818
10542,-Metrics/Training(Step): loss,1610356503872,0.1293032169342041
10544,-Metrics/Training(Step): loss,1610356506015,0.16253727674484253
10546,-Metrics/Training(Step): loss,1610356508125,0.12241808325052261
10548,-Metrics/Training(Step): loss,1610356510225,0.10972940921783447
10550,-Metrics/Training(Step): loss,1610356512344,0.10558884590864182
10552,-Metrics/Training(Step): loss,1610356514474,0.10719765722751617
10554,-Metrics/Training(Step): loss,1610356516542,0.13878187537193298
10556,-Metrics/Training(Step): loss,1610356518651,0.12547019124031067
10558,-Metrics/Training(Step): loss,1610356520791,0.11971955001354218
10560,-Metrics/Training(Step): loss,1610356522806,0.14089441299438477
10562,-Metrics/Training(Step): loss,1610356524650,0.13425804674625397
10564,-Metrics/Training(Step): loss,1610356526547,0.11624553054571152
10566,-Metrics/Training(Step): loss,1610356528589,0.10857190191745758
10568,-Metrics/Training(Step): loss,1610356530680,0.1215284988284111
10570,-Metrics/Training(Step): loss,1610356532612,0.09514300525188446
10572,-Metrics/Training(Step): loss,1610356534686,0.11738903820514679
10574,-Metrics/Training(Step): loss,1610356536793,0.1169954314827919
10576,-Metrics/Training(Step): loss,1610356538858,0.13717076182365417
10578,-Metrics/Training(Step): loss,1610356540932,0.13769163191318512
10580,-Metrics/Training(Step): loss,1610356553630,0.1262357383966446
10582,-Metrics/Training(Step): loss,1610356557524,0.14376673102378845
10584,-Metrics/Training(Step): loss,1610356561320,0.13342995941638947
10586,-Metrics/Training(Step): loss,1610356565239,0.15468730032444
10588,-Metrics/Training(Step): loss,1610356569026,0.11289215832948685
10590,-Metrics/Training(Step): loss,1610356573029,0.12100044637918472
10592,-Metrics/Training(Step): loss,1610356577435,0.13134677708148956
10594,-Metrics/Training(Step): loss,1610356582156,0.10984709113836288
10596,-Metrics/Training(Step): loss,1610356585705,0.1330822855234146
10598,-Metrics/Training(Step): loss,1610356589199,0.11308925598859787
10600,-Metrics/Training(Step): loss,1610356592816,0.10506056249141693
10602,-Metrics/Training(Step): loss,1610356596420,0.12574638426303864
10604,-Metrics/Training(Step): loss,1610356600317,0.14509503543376923
10606,-Metrics/Training(Step): loss,1610356604060,0.11525627970695496
10608,-Metrics/Training(Step): loss,1610356607552,0.09538964182138443
10610,-Metrics/Training(Step): loss,1610356610929,0.1449960470199585
10612,-Metrics/Training(Step): loss,1610356614570,0.1264273077249527
10614,-Metrics/Training(Step): loss,1610356617854,0.1352260708808899
10616,-Metrics/Training(Step): loss,1610356621246,0.10143086314201355
10618,-Metrics/Training(Step): loss,1610356625020,0.15596163272857666
10620,-Metrics/Training(Step): loss,1610356628624,0.14319156110286713
10622,-Metrics/Training(Step): loss,1610356631620,0.12238194048404694
10624,-Metrics/Training(Step): loss,1610356634989,0.11305096000432968
10626,-Metrics/Training(Step): loss,1610356638330,0.1058751717209816
10628,-Metrics/Training(Step): loss,1610356640859,0.15206483006477356
10630,-Metrics/Training(Step): loss,1610356643035,0.14402124285697937
10632,-Metrics/Training(Step): loss,1610356645104,0.12499471008777618
10634,-Metrics/Training(Step): loss,1610356647156,0.08993293344974518
10636,-Metrics/Training(Step): loss,1610356649269,0.13756416738033295
10638,-Metrics/Training(Step): loss,1610356651365,0.12034183740615845
10640,-Metrics/Training(Step): loss,1610356653438,0.1219927966594696
10642,-Metrics/Training(Step): loss,1610356655420,0.12093650549650192
10644,-Metrics/Training(Step): loss,1610356657512,0.11661658436059952
10646,-Metrics/Training(Step): loss,1610356659422,0.13816751539707184
10648,-Metrics/Training(Step): loss,1610356661502,0.12230639159679413
10650,-Metrics/Training(Step): loss,1610356663530,0.12690429389476776
10652,-Metrics/Training(Step): loss,1610356665556,0.1188318133354187
10654,-Metrics/Training(Step): loss,1610356667671,0.1453210711479187
10656,-Metrics/Training(Step): loss,1610356669757,0.148932546377182
10658,-Metrics/Training(Step): loss,1610356671830,0.13558687269687653
10660,-Metrics/Training(Step): loss,1610356673891,0.11096946150064468
10662,-Metrics/Training(Step): loss,1610356675965,0.12330035120248795
10664,-Metrics/Training(Step): loss,1610356677988,0.12948767840862274
10666,-Metrics/Training(Step): loss,1610356691632,0.13506096601486206
10668,-Metrics/Training(Step): loss,1610356695729,0.12006724625825882
10670,-Metrics/Training(Step): loss,1610356699520,0.11957738548517227
10672,-Metrics/Training(Step): loss,1610356703532,0.1326160728931427
10674,-Metrics/Training(Step): loss,1610356707720,0.14983771741390228
10676,-Metrics/Training(Step): loss,1610356711677,0.08829573541879654
10678,-Metrics/Training(Step): loss,1610356715420,0.11407114565372467
10680,-Metrics/Training(Step): loss,1610356718847,0.13266919553279877
10682,-Metrics/Training(Step): loss,1610356722982,0.08809740096330643
10684,-Metrics/Training(Step): loss,1610356726358,0.12964871525764465
10686,-Metrics/Training(Step): loss,1610356729422,0.1275824010372162
10688,-Metrics/Training(Step): loss,1610356733117,0.14841659367084503
10690,-Metrics/Training(Step): loss,1610356736864,0.09921950101852417
10692,-Metrics/Training(Step): loss,1610356740567,0.11062594503164291
10694,-Metrics/Training(Step): loss,1610356744043,0.1160752922296524
10696,-Metrics/Training(Step): loss,1610356747671,0.14145320653915405
10698,-Metrics/Training(Step): loss,1610356750919,0.07010850310325623
10700,-Metrics/Training(Step): loss,1610356754531,0.1290399134159088
10702,-Metrics/Training(Step): loss,1610356757619,0.10957074910402298
10704,-Metrics/Training(Step): loss,1610356760820,0.13651584088802338
10706,-Metrics/Training(Step): loss,1610356764614,0.11059726774692535
10708,-Metrics/Training(Step): loss,1610356768548,0.11353502422571182
10710,-Metrics/Training(Step): loss,1610356771968,0.12373944371938705
10712,-Metrics/Training(Step): loss,1610356775219,0.13486133515834808
10714,-Metrics/Training(Step): loss,1610356777766,0.12974081933498383
10716,-Metrics/Training(Step): loss,1610356779846,0.10678209364414215
10718,-Metrics/Training(Step): loss,1610356781930,0.10961806029081345
10720,-Metrics/Training(Step): loss,1610356784054,0.1272856444120407
10722,-Metrics/Training(Step): loss,1610356786148,0.11870981007814407
10724,-Metrics/Training(Step): loss,1610356788191,0.13939093053340912
10726,-Metrics/Training(Step): loss,1610356790162,0.11342452466487885
10728,-Metrics/Training(Step): loss,1610356792196,0.10289173573255539
10730,-Metrics/Training(Step): loss,1610356794170,0.1363150179386139
10732,-Metrics/Training(Step): loss,1610356796165,0.11630097776651382
10734,-Metrics/Training(Step): loss,1610356798268,0.15499760210514069
10736,-Metrics/Training(Step): loss,1610356800313,0.13250212371349335
10738,-Metrics/Training(Step): loss,1610356802340,0.15462806820869446
10740,-Metrics/Training(Step): loss,1610356804445,0.11221376806497574
10742,-Metrics/Training(Step): loss,1610356806330,0.14255110919475555
10744,-Metrics/Training(Step): loss,1610356808408,0.137899711728096
10746,-Metrics/Training(Step): loss,1610356810461,0.11644062399864197
10748,-Metrics/Training(Step): loss,1610356812263,0.11773000657558441
10750,-Metrics/Training(Step): loss,1610356814331,0.12464059144258499
10752,-Metrics/Training(Step): loss,1610356827744,0.10864128917455673
10754,-Metrics/Training(Step): loss,1610356831917,0.12691006064414978
10756,-Metrics/Training(Step): loss,1610356835621,0.14796842634677887
10758,-Metrics/Training(Step): loss,1610356839822,0.1370743364095688
10760,-Metrics/Training(Step): loss,1610356843915,0.1171489804983139
10762,-Metrics/Training(Step): loss,1610356847948,0.09510962665081024
10764,-Metrics/Training(Step): loss,1610356852231,0.13166506588459015
10766,-Metrics/Training(Step): loss,1610356855920,0.13695374131202698
10768,-Metrics/Training(Step): loss,1610356859821,0.12569037079811096
10770,-Metrics/Training(Step): loss,1610356863528,0.12801840901374817
10772,-Metrics/Training(Step): loss,1610356867415,0.11977013200521469
10774,-Metrics/Training(Step): loss,1610356870720,0.12282858043909073
10776,-Metrics/Training(Step): loss,1610356874120,0.13210545480251312
10778,-Metrics/Training(Step): loss,1610356877540,0.10962401330471039
10780,-Metrics/Training(Step): loss,1610356880839,0.11536771059036255
10782,-Metrics/Training(Step): loss,1610356884041,0.1088116243481636
10784,-Metrics/Training(Step): loss,1610356887563,0.08838189393281937
10786,-Metrics/Training(Step): loss,1610356890622,0.1281779557466507
10788,-Metrics/Training(Step): loss,1610356894315,0.11920817941427231
10790,-Metrics/Training(Step): loss,1610356897615,0.12276818603277206
10792,-Metrics/Training(Step): loss,1610356901034,0.09737849980592728
10794,-Metrics/Training(Step): loss,1610356905120,0.11780325323343277
10796,-Metrics/Training(Step): loss,1610356908540,0.12758372724056244
10798,-Metrics/Training(Step): loss,1610356911900,0.11801476776599884
10800,-Metrics/Training(Step): loss,1610356914767,0.13611812889575958
10802,-Metrics/Training(Step): loss,1610356917102,0.11744602769613266
10804,-Metrics/Training(Step): loss,1610356919099,0.1391804814338684
10806,-Metrics/Training(Step): loss,1610356921188,0.12593655288219452
10808,-Metrics/Training(Step): loss,1610356923305,0.11074648797512054
10810,-Metrics/Training(Step): loss,1610356925417,0.10872340947389603
10812,-Metrics/Training(Step): loss,1610356927580,0.09410426765680313
10814,-Metrics/Training(Step): loss,1610356929503,0.11005298048257828
10816,-Metrics/Training(Step): loss,1610356931613,0.10009841620922089
10818,-Metrics/Training(Step): loss,1610356933711,0.11837277561426163
10820,-Metrics/Training(Step): loss,1610356935794,0.1194157749414444
10822,-Metrics/Training(Step): loss,1610356937889,0.08727461099624634
10824,-Metrics/Training(Step): loss,1610356939954,0.10312328487634659
10826,-Metrics/Training(Step): loss,1610356942057,0.13402819633483887
10828,-Metrics/Training(Step): loss,1610356944133,0.11626430600881577
10830,-Metrics/Training(Step): loss,1610356946212,0.13386575877666473
10832,-Metrics/Training(Step): loss,1610356948190,0.11098531633615494
10834,-Metrics/Training(Step): loss,1610356950244,0.1546124964952469
10836,-Metrics/Training(Step): loss,1610356952306,0.10773686319589615
10838,-Metrics/Training(Step): loss,1610356965223,0.11641892045736313
10840,-Metrics/Training(Step): loss,1610356969631,0.15792541205883026
10842,-Metrics/Training(Step): loss,1610356973929,0.14820101857185364
10844,-Metrics/Training(Step): loss,1610356978031,0.10981844365596771
10846,-Metrics/Training(Step): loss,1610356982019,0.11187830567359924
10848,-Metrics/Training(Step): loss,1610356986119,0.11815685033798218
10850,-Metrics/Training(Step): loss,1610356990015,0.12659846246242523
10852,-Metrics/Training(Step): loss,1610356993119,0.1296176314353943
10854,-Metrics/Training(Step): loss,1610356996506,0.14387789368629456
10856,-Metrics/Training(Step): loss,1610356999834,0.13023383915424347
10858,-Metrics/Training(Step): loss,1610357003620,0.11215130239725113
10860,-Metrics/Training(Step): loss,1610357007120,0.08014276623725891
10862,-Metrics/Training(Step): loss,1610357010420,0.09380088746547699
10864,-Metrics/Training(Step): loss,1610357014370,0.11917927116155624
10866,-Metrics/Training(Step): loss,1610357017865,0.1056840717792511
10868,-Metrics/Training(Step): loss,1610357021820,0.12064216285943985
10870,-Metrics/Training(Step): loss,1610357025648,0.1157606691122055
10872,-Metrics/Training(Step): loss,1610357029141,0.14158788323402405
10874,-Metrics/Training(Step): loss,1610357032326,0.13070835173130035
10876,-Metrics/Training(Step): loss,1610357036255,0.1374521553516388
10878,-Metrics/Training(Step): loss,1610357039120,0.13938768208026886
10880,-Metrics/Training(Step): loss,1610357042947,0.11182095110416412
10882,-Metrics/Training(Step): loss,1610357046601,0.12672972679138184
10884,-Metrics/Training(Step): loss,1610357050226,0.13150694966316223
10886,-Metrics/Training(Step): loss,1610357052731,0.10868480056524277
10888,-Metrics/Training(Step): loss,1610357054960,0.11926823109388351
10890,-Metrics/Training(Step): loss,1610357057088,0.12038566917181015
10892,-Metrics/Training(Step): loss,1610357059187,0.11556106805801392
10894,-Metrics/Training(Step): loss,1610357061313,0.11210336536169052
10896,-Metrics/Training(Step): loss,1610357063346,0.12312086671590805
10898,-Metrics/Training(Step): loss,1610357065454,0.14200569689273834
10900,-Metrics/Training(Step): loss,1610357067554,0.1212432011961937
10902,-Metrics/Training(Step): loss,1610357069619,0.11149733513593674
10904,-Metrics/Training(Step): loss,1610357071720,0.10501054674386978
10906,-Metrics/Training(Step): loss,1610357073807,0.1040528193116188
10908,-Metrics/Training(Step): loss,1610357075803,0.15214531123638153
10910,-Metrics/Training(Step): loss,1610357077900,0.11270993202924728
10912,-Metrics/Training(Step): loss,1610357080002,0.11866152286529541
10914,-Metrics/Training(Step): loss,1610357082089,0.15112555027008057
10916,-Metrics/Training(Step): loss,1610357084167,0.09392485022544861
10918,-Metrics/Training(Step): loss,1610357086240,0.08299202471971512
10920,-Metrics/Training(Step): loss,1610357088313,0.11899542063474655
10922,-Metrics/Training(Step): loss,1610357090416,0.1182665228843689
10924,-Metrics/Training(Step): loss,1610357103115,0.09591633081436157
10926,-Metrics/Training(Step): loss,1610357107220,0.1099793091416359
10928,-Metrics/Training(Step): loss,1610357111220,0.14277465641498566
10930,-Metrics/Training(Step): loss,1610357115126,0.11466801166534424
10932,-Metrics/Training(Step): loss,1610357119220,0.1325543373823166
10934,-Metrics/Training(Step): loss,1610357122920,0.09213285893201828
10936,-Metrics/Training(Step): loss,1610357126459,0.1367545872926712
10938,-Metrics/Training(Step): loss,1610357129232,0.12502196431159973
10940,-Metrics/Training(Step): loss,1610357132969,0.15544600784778595
10942,-Metrics/Training(Step): loss,1610357136917,0.10715890675783157
10944,-Metrics/Training(Step): loss,1610357140726,0.13719944655895233
10946,-Metrics/Training(Step): loss,1610357144520,0.1259230524301529
10948,-Metrics/Training(Step): loss,1610357148559,0.13792432844638824
10950,-Metrics/Training(Step): loss,1610357152126,0.12102532386779785
10952,-Metrics/Training(Step): loss,1610357156219,0.13475999236106873
10954,-Metrics/Training(Step): loss,1610357159916,0.09308307617902756
10956,-Metrics/Training(Step): loss,1610357162692,0.10781274735927582
10958,-Metrics/Training(Step): loss,1610357166721,0.11232589930295944
10960,-Metrics/Training(Step): loss,1610357170251,0.12442109733819962
10962,-Metrics/Training(Step): loss,1610357173338,0.13802047073841095
10964,-Metrics/Training(Step): loss,1610357176722,0.14354722201824188
10966,-Metrics/Training(Step): loss,1610357179920,0.1350298672914505
10968,-Metrics/Training(Step): loss,1610357183373,0.09149283170700073
10970,-Metrics/Training(Step): loss,1610357187349,0.15080034732818604
10972,-Metrics/Training(Step): loss,1610357190069,0.13555721938610077
10974,-Metrics/Training(Step): loss,1610357192325,0.12492681294679642
10976,-Metrics/Training(Step): loss,1610357194393,0.1257372349500656
10978,-Metrics/Training(Step): loss,1610357196471,0.10668157786130905
10980,-Metrics/Training(Step): loss,1610357198572,0.12291537970304489
10982,-Metrics/Training(Step): loss,1610357200699,0.1157761886715889
10984,-Metrics/Training(Step): loss,1610357202818,0.13903746008872986
10986,-Metrics/Training(Step): loss,1610357204815,0.12401357293128967
10988,-Metrics/Training(Step): loss,1610357206860,0.10316921770572662
10990,-Metrics/Training(Step): loss,1610357208959,0.11361616849899292
10992,-Metrics/Training(Step): loss,1610357211054,0.13415279984474182
10994,-Metrics/Training(Step): loss,1610357213047,0.14331747591495514
10996,-Metrics/Training(Step): loss,1610357215165,0.14690759778022766
10998,-Metrics/Training(Step): loss,1610357217150,0.11603610962629318
11000,-Metrics/Training(Step): loss,1610357219245,0.12305013090372086
11002,-Metrics/Training(Step): loss,1610357221338,0.10189387947320938
11004,-Metrics/Training(Step): loss,1610357223335,0.14379194378852844
11006,-Metrics/Training(Step): loss,1610357225379,0.11074134707450867
11008,-Metrics/Training(Step): loss,1610357227359,0.13145166635513306
11010,-Metrics/Training(Step): loss,1610357240142,0.09952732920646667
11012,-Metrics/Training(Step): loss,1610357244628,0.1304583102464676
11014,-Metrics/Training(Step): loss,1610357248291,0.13713271915912628
11016,-Metrics/Training(Step): loss,1610357252323,0.13811707496643066
11018,-Metrics/Training(Step): loss,1610357256337,0.1360699087381363
11020,-Metrics/Training(Step): loss,1610357260246,0.11359720677137375
11022,-Metrics/Training(Step): loss,1610357265015,0.12424177676439285
11024,-Metrics/Training(Step): loss,1610357267919,0.08813407272100449
11026,-Metrics/Training(Step): loss,1610357271720,0.11884637922048569
11028,-Metrics/Training(Step): loss,1610357275441,0.11014225333929062
11030,-Metrics/Training(Step): loss,1610357279247,0.10784912109375
11032,-Metrics/Training(Step): loss,1610357283006,0.1403125524520874
11034,-Metrics/Training(Step): loss,1610357286820,0.15770787000656128
11036,-Metrics/Training(Step): loss,1610357290699,0.09230634570121765
11038,-Metrics/Training(Step): loss,1610357294164,0.11273551732301712
11040,-Metrics/Training(Step): loss,1610357297614,0.1084655299782753
11042,-Metrics/Training(Step): loss,1610357300856,0.1414746344089508
11044,-Metrics/Training(Step): loss,1610357304250,0.11791767179965973
11046,-Metrics/Training(Step): loss,1610357307543,0.14225433766841888
11048,-Metrics/Training(Step): loss,1610357311520,0.13790000975131989
11050,-Metrics/Training(Step): loss,1610357315221,0.13452406227588654
11052,-Metrics/Training(Step): loss,1610357318821,0.140468031167984
11054,-Metrics/Training(Step): loss,1610357322574,0.12215437740087509
11056,-Metrics/Training(Step): loss,1610357324969,0.10353021323680878
11058,-Metrics/Training(Step): loss,1610357327140,0.11039964854717255
11060,-Metrics/Training(Step): loss,1610357329226,0.11801497638225555
11062,-Metrics/Training(Step): loss,1610357331309,0.10567720234394073
11064,-Metrics/Training(Step): loss,1610357333417,0.14346188306808472
11066,-Metrics/Training(Step): loss,1610357335551,0.10766730457544327
11068,-Metrics/Training(Step): loss,1610357337633,0.11925923824310303
11070,-Metrics/Training(Step): loss,1610357339790,0.12164711952209473
11072,-Metrics/Training(Step): loss,1610357341895,0.14690856635570526
11074,-Metrics/Training(Step): loss,1610357343853,0.11496785283088684
11076,-Metrics/Training(Step): loss,1610357345930,0.11515536904335022
11078,-Metrics/Training(Step): loss,1610357347784,0.11777213960886002
11080,-Metrics/Training(Step): loss,1610357349765,0.145593523979187
11082,-Metrics/Training(Step): loss,1610357351729,0.09729703515768051
11084,-Metrics/Training(Step): loss,1610357353788,0.11969998478889465
11086,-Metrics/Training(Step): loss,1610357355714,0.10145706683397293
11088,-Metrics/Training(Step): loss,1610357357640,0.12014707177877426
11090,-Metrics/Training(Step): loss,1610357359626,0.09204252809286118
11092,-Metrics/Training(Step): loss,1610357361676,0.13678105175495148
11094,-Metrics/Training(Step): loss,1610357363771,0.10375698655843735
11096,-Metrics/Training(Step): loss,1610357376746,0.1374654918909073
11098,-Metrics/Training(Step): loss,1610357380820,0.13278275728225708
11100,-Metrics/Training(Step): loss,1610357385058,0.10810603946447372
11102,-Metrics/Training(Step): loss,1610357388819,0.13411541283130646
11104,-Metrics/Training(Step): loss,1610357393016,0.15992742776870728
11106,-Metrics/Training(Step): loss,1610357396930,0.13872472941875458
11108,-Metrics/Training(Step): loss,1610357400848,0.1146119013428688
11110,-Metrics/Training(Step): loss,1610357404449,0.12820428609848022
11112,-Metrics/Training(Step): loss,1610357408021,0.14457093179225922
11114,-Metrics/Training(Step): loss,1610357411121,0.14153258502483368
11116,-Metrics/Training(Step): loss,1610357414442,0.12347710877656937
11118,-Metrics/Training(Step): loss,1610357418320,0.11301788687705994
11120,-Metrics/Training(Step): loss,1610357421239,0.11721841990947723
11122,-Metrics/Training(Step): loss,1610357424623,0.08920974284410477
11124,-Metrics/Training(Step): loss,1610357427649,0.15504178404808044
11126,-Metrics/Training(Step): loss,1610357430650,0.11399594694375992
11128,-Metrics/Training(Step): loss,1610357434242,0.2122579663991928
11130,-Metrics/Training(Step): loss,1610357438057,0.28695914149284363
11132,-Metrics/Training(Step): loss,1610357441770,0.20520944893360138
11134,-Metrics/Training(Step): loss,1610357445610,0.2055024802684784
11136,-Metrics/Training(Step): loss,1610357449124,0.18974152207374573
11138,-Metrics/Training(Step): loss,1610357452719,0.1582619696855545
11140,-Metrics/Training(Step): loss,1610357455918,0.15291811525821686
11142,-Metrics/Training(Step): loss,1610357459814,0.1785602867603302
11144,-Metrics/Training(Step): loss,1610357462847,0.16515061259269714
11146,-Metrics/Training(Step): loss,1610357464840,0.17093439400196075
11148,-Metrics/Training(Step): loss,1610357466887,0.16592426598072052
11150,-Metrics/Training(Step): loss,1610357468992,0.18817675113677979
11152,-Metrics/Training(Step): loss,1610357471103,0.198568657040596
11154,-Metrics/Training(Step): loss,1610357473220,0.1711885780096054
11156,-Metrics/Training(Step): loss,1610357475355,0.22058595716953278
11158,-Metrics/Training(Step): loss,1610357477399,0.16627414524555206
11160,-Metrics/Training(Step): loss,1610357479391,0.14583516120910645
11162,-Metrics/Training(Step): loss,1610357481466,0.13975800573825836
11164,-Metrics/Training(Step): loss,1610357483530,0.15619920194149017
11166,-Metrics/Training(Step): loss,1610357485397,0.1477399468421936
11168,-Metrics/Training(Step): loss,1610357487546,0.1306968778371811
11170,-Metrics/Training(Step): loss,1610357489547,0.11252006888389587
11172,-Metrics/Training(Step): loss,1610357491693,0.12857921421527863
11174,-Metrics/Training(Step): loss,1610357493593,0.10851879417896271
11176,-Metrics/Training(Step): loss,1610357495579,0.16096165776252747
11178,-Metrics/Training(Step): loss,1610357497704,0.1333632916212082
11180,-Metrics/Training(Step): loss,1610357499722,0.12553025782108307
11182,-Metrics/Training(Step): loss,1610357511840,0.13035959005355835
11184,-Metrics/Training(Step): loss,1610357516231,0.12409777194261551
11186,-Metrics/Training(Step): loss,1610357520420,0.11077550798654556
11188,-Metrics/Training(Step): loss,1610357524318,0.1717233508825302
11190,-Metrics/Training(Step): loss,1610357528820,0.14141011238098145
11192,-Metrics/Training(Step): loss,1610357532815,0.12844017148017883
11194,-Metrics/Training(Step): loss,1610357536620,0.11834505945444107
11196,-Metrics/Training(Step): loss,1610357540419,0.15392830967903137
11198,-Metrics/Training(Step): loss,1610357544246,0.15690909326076508
11200,-Metrics/Training(Step): loss,1610357547544,0.1250341385602951
11202,-Metrics/Training(Step): loss,1610357550851,0.10253755003213882
11204,-Metrics/Training(Step): loss,1610357554426,0.13043293356895447
11206,-Metrics/Training(Step): loss,1610357557719,0.1536625474691391
11208,-Metrics/Training(Step): loss,1610357561420,0.12570305168628693
11210,-Metrics/Training(Step): loss,1610357565122,0.139094740152359
11212,-Metrics/Training(Step): loss,1610357568822,0.11898606270551682
11214,-Metrics/Training(Step): loss,1610357571844,0.15418483316898346
11216,-Metrics/Training(Step): loss,1610357575720,0.17496927082538605
11218,-Metrics/Training(Step): loss,1610357579793,0.12622790038585663
11220,-Metrics/Training(Step): loss,1610357583318,0.15023083984851837
11222,-Metrics/Training(Step): loss,1610357587320,0.09891784936189651
11224,-Metrics/Training(Step): loss,1610357590420,0.10141738504171371
11226,-Metrics/Training(Step): loss,1610357594153,0.14159393310546875
11228,-Metrics/Training(Step): loss,1610357597261,0.11515931040048599
11230,-Metrics/Training(Step): loss,1610357599505,0.11845749616622925
11232,-Metrics/Training(Step): loss,1610357601752,0.12422003597021103
11234,-Metrics/Training(Step): loss,1610357603862,0.14407280087471008
11236,-Metrics/Training(Step): loss,1610357605947,0.14784428477287292
11238,-Metrics/Training(Step): loss,1610357608007,0.14593641459941864
11240,-Metrics/Training(Step): loss,1610357610042,0.1403045654296875
11242,-Metrics/Training(Step): loss,1610357612043,0.1371973156929016
11244,-Metrics/Training(Step): loss,1610357614132,0.12451447546482086
11246,-Metrics/Training(Step): loss,1610357616167,0.10607534646987915
11248,-Metrics/Training(Step): loss,1610357618221,0.13810469210147858
11250,-Metrics/Training(Step): loss,1610357620168,0.1267046481370926
11252,-Metrics/Training(Step): loss,1610357622302,0.12104315310716629
11254,-Metrics/Training(Step): loss,1610357624378,0.15172339975833893
11256,-Metrics/Training(Step): loss,1610357626460,0.13684619963169098
11258,-Metrics/Training(Step): loss,1610357628398,0.12425757199525833
11260,-Metrics/Training(Step): loss,1610357630453,0.13134975731372833
11262,-Metrics/Training(Step): loss,1610357632560,0.13972735404968262
11264,-Metrics/Training(Step): loss,1610357634489,0.11934711039066315
11266,-Metrics/Training(Step): loss,1610357636556,0.09870121628046036
11268,-Metrics/Training(Step): loss,1610357649534,0.1345537155866623
11270,-Metrics/Training(Step): loss,1610357653919,0.11922837793827057
11272,-Metrics/Training(Step): loss,1610357657623,0.1599629670381546
11274,-Metrics/Training(Step): loss,1610357661520,0.09449142962694168
11276,-Metrics/Training(Step): loss,1610357665423,0.12142674624919891
11278,-Metrics/Training(Step): loss,1610357669323,0.17110034823417664
11280,-Metrics/Training(Step): loss,1610357673424,0.14667223393917084
11282,-Metrics/Training(Step): loss,1610357676820,0.134469673037529
11284,-Metrics/Training(Step): loss,1610357680650,0.1134440079331398
11286,-Metrics/Training(Step): loss,1610357684115,0.13296037912368774
11288,-Metrics/Training(Step): loss,1610357687158,0.1340411752462387
11290,-Metrics/Training(Step): loss,1610357690443,0.11814781278371811
11292,-Metrics/Training(Step): loss,1610357694320,0.14465972781181335
11294,-Metrics/Training(Step): loss,1610357697620,0.14078904688358307
11296,-Metrics/Training(Step): loss,1610357701496,0.15085668861865997
11298,-Metrics/Training(Step): loss,1610357704828,0.11297427862882614
11300,-Metrics/Training(Step): loss,1610357708620,0.13575902581214905
11302,-Metrics/Training(Step): loss,1610357712379,0.10871370136737823
11304,-Metrics/Training(Step): loss,1610357715720,0.11604516953229904
11306,-Metrics/Training(Step): loss,1610357719468,0.11968913674354553
11308,-Metrics/Training(Step): loss,1610357722927,0.12116021662950516
11310,-Metrics/Training(Step): loss,1610357726120,0.1577794998884201
11312,-Metrics/Training(Step): loss,1610357729337,0.16820351779460907
11314,-Metrics/Training(Step): loss,1610357732128,0.13100144267082214
11316,-Metrics/Training(Step): loss,1610357734897,0.12245640158653259
11318,-Metrics/Training(Step): loss,1610357737158,0.11163263767957687
11320,-Metrics/Training(Step): loss,1610357739443,0.11108793318271637
11322,-Metrics/Training(Step): loss,1610357741548,0.11030631512403488
11324,-Metrics/Training(Step): loss,1610357743682,0.12203699350357056
11326,-Metrics/Training(Step): loss,1610357745635,0.09719143062829971
11328,-Metrics/Training(Step): loss,1610357747756,0.14078642427921295
11330,-Metrics/Training(Step): loss,1610357749842,0.11992406845092773
11332,-Metrics/Training(Step): loss,1610357751778,0.12104550004005432
11334,-Metrics/Training(Step): loss,1610357753858,0.13438311219215393
11336,-Metrics/Training(Step): loss,1610357755954,0.11598216742277145
11338,-Metrics/Training(Step): loss,1610357758049,0.1308632642030716
11340,-Metrics/Training(Step): loss,1610357760146,0.12187308073043823
11342,-Metrics/Training(Step): loss,1610357762265,0.1303943544626236
11344,-Metrics/Training(Step): loss,1610357764373,0.1181865781545639
11346,-Metrics/Training(Step): loss,1610357766429,0.11426736414432526
11348,-Metrics/Training(Step): loss,1610357768501,0.1254660189151764
11350,-Metrics/Training(Step): loss,1610357770584,0.10424104332923889
11352,-Metrics/Training(Step): loss,1610357772590,0.09044048190116882
11354,-Metrics/Training(Step): loss,1610357786937,0.07924176752567291
11356,-Metrics/Training(Step): loss,1610357791119,0.10332322865724564
11358,-Metrics/Training(Step): loss,1610357795020,0.13009555637836456
11360,-Metrics/Training(Step): loss,1610357799130,0.16613756120204926
11362,-Metrics/Training(Step): loss,1610357803318,0.1334119588136673
11364,-Metrics/Training(Step): loss,1610357807223,0.11891002953052521
11366,-Metrics/Training(Step): loss,1610357811345,0.09788954257965088
11368,-Metrics/Training(Step): loss,1610357815415,0.15869636833667755
11370,-Metrics/Training(Step): loss,1610357819020,0.2047610878944397
11372,-Metrics/Training(Step): loss,1610357822920,0.11636392027139664
11374,-Metrics/Training(Step): loss,1610357826553,0.1078006699681282
11376,-Metrics/Training(Step): loss,1610357829821,0.1316043883562088
11378,-Metrics/Training(Step): loss,1610357833420,0.11347735673189163
11380,-Metrics/Training(Step): loss,1610357836919,0.12523774802684784
11382,-Metrics/Training(Step): loss,1610357840242,0.11420688778162003
11384,-Metrics/Training(Step): loss,1610357843743,0.16186030209064484
11386,-Metrics/Training(Step): loss,1610357847422,0.16039925813674927
11388,-Metrics/Training(Step): loss,1610357850741,0.12824130058288574
11390,-Metrics/Training(Step): loss,1610357854119,0.14875881373882294
11392,-Metrics/Training(Step): loss,1610357857349,0.11376775056123734
11394,-Metrics/Training(Step): loss,1610357860720,0.10982392728328705
11396,-Metrics/Training(Step): loss,1610357864241,0.13679856061935425
11398,-Metrics/Training(Step): loss,1610357867641,0.12405207008123398
11400,-Metrics/Training(Step): loss,1610357870209,0.11301153898239136
11402,-Metrics/Training(Step): loss,1610357872426,0.10268745571374893
11404,-Metrics/Training(Step): loss,1610357874693,0.1196771115064621
11406,-Metrics/Training(Step): loss,1610357876754,0.10484115034341812
11408,-Metrics/Training(Step): loss,1610357878837,0.13461896777153015
11410,-Metrics/Training(Step): loss,1610357880908,0.09992745518684387
11412,-Metrics/Training(Step): loss,1610357882993,0.0938180536031723
11414,-Metrics/Training(Step): loss,1610357885122,0.11662276834249496
11416,-Metrics/Training(Step): loss,1610357887216,0.11103086173534393
11418,-Metrics/Training(Step): loss,1610357889299,0.12033252418041229
11420,-Metrics/Training(Step): loss,1610357891389,0.12306937575340271
11422,-Metrics/Training(Step): loss,1610357893347,0.1302197277545929
11424,-Metrics/Training(Step): loss,1610357895178,0.11943875998258591
11426,-Metrics/Training(Step): loss,1610357897110,0.12938036024570465
11428,-Metrics/Training(Step): loss,1610357899135,0.09786558151245117
11430,-Metrics/Training(Step): loss,1610357901205,0.08744041621685028
11432,-Metrics/Training(Step): loss,1610357903291,0.13636291027069092
11434,-Metrics/Training(Step): loss,1610357905369,0.10923273861408234
11436,-Metrics/Training(Step): loss,1610357907447,0.13442778587341309
11438,-Metrics/Training(Step): loss,1610357909538,0.12297870218753815
11440,-Metrics/Training(Step): loss,1610357923333,0.11480912566184998
11442,-Metrics/Training(Step): loss,1610357927321,0.10553254932165146
11444,-Metrics/Training(Step): loss,1610357931019,0.12772484123706818
11446,-Metrics/Training(Step): loss,1610357935220,0.12798836827278137
11448,-Metrics/Training(Step): loss,1610357939258,0.13891847431659698
11450,-Metrics/Training(Step): loss,1610357943224,0.08937575668096542
11452,-Metrics/Training(Step): loss,1610357947219,0.14493586122989655
11454,-Metrics/Training(Step): loss,1610357950222,0.11547719687223434
11456,-Metrics/Training(Step): loss,1610357954515,0.14902669191360474
11458,-Metrics/Training(Step): loss,1610357958024,0.12843108177185059
11460,-Metrics/Training(Step): loss,1610357961421,0.11103241890668869
11462,-Metrics/Training(Step): loss,1610357964658,0.11763899028301239
11464,-Metrics/Training(Step): loss,1610357968221,0.12470722198486328
11466,-Metrics/Training(Step): loss,1610357971821,0.12169115990400314
11468,-Metrics/Training(Step): loss,1610357975420,0.13888753950595856
11470,-Metrics/Training(Step): loss,1610357978942,0.13070964813232422
11472,-Metrics/Training(Step): loss,1610357982615,0.14162704348564148
11474,-Metrics/Training(Step): loss,1610357985841,0.10192728787660599
11476,-Metrics/Training(Step): loss,1610357989619,0.12961669266223907
11478,-Metrics/Training(Step): loss,1610357993163,0.14889104664325714
11480,-Metrics/Training(Step): loss,1610357996371,0.08692194521427155
11482,-Metrics/Training(Step): loss,1610357999990,0.1180436834692955
11484,-Metrics/Training(Step): loss,1610358003068,0.13233205676078796
11486,-Metrics/Training(Step): loss,1610358006116,0.1108681783080101
11488,-Metrics/Training(Step): loss,1610358008658,0.1481115072965622
11490,-Metrics/Training(Step): loss,1610358011009,0.09962304681539536
11492,-Metrics/Training(Step): loss,1610358013176,0.14742565155029297
11494,-Metrics/Training(Step): loss,1610358015257,0.14307811856269836
11496,-Metrics/Training(Step): loss,1610358017404,0.11967688798904419
11498,-Metrics/Training(Step): loss,1610358019494,0.14558568596839905
11500,-Metrics/Training(Step): loss,1610358021563,0.11107755452394485
11502,-Metrics/Training(Step): loss,1610358023713,0.1388874650001526
11504,-Metrics/Training(Step): loss,1610358025821,0.1509522944688797
11506,-Metrics/Training(Step): loss,1610358027752,0.12178703397512436
11508,-Metrics/Training(Step): loss,1610358029825,0.12938165664672852
11510,-Metrics/Training(Step): loss,1610358031834,0.12893012166023254
11512,-Metrics/Training(Step): loss,1610358033849,0.14102387428283691
11514,-Metrics/Training(Step): loss,1610358035748,0.10382448136806488
11516,-Metrics/Training(Step): loss,1610358037839,0.0989483967423439
11518,-Metrics/Training(Step): loss,1610358039880,0.11870115995407104
11520,-Metrics/Training(Step): loss,1610358042005,0.11338422447443008
11522,-Metrics/Training(Step): loss,1610358044124,0.12038427591323853
11524,-Metrics/Training(Step): loss,1610358046155,0.11047794669866562
11526,-Metrics/Training(Step): loss,1610358058315,0.10574071854352951
11528,-Metrics/Training(Step): loss,1610358062620,0.14111824333667755
11530,-Metrics/Training(Step): loss,1610358066415,0.11786588281393051
11532,-Metrics/Training(Step): loss,1610358070341,0.11862891912460327
11534,-Metrics/Training(Step): loss,1610358074120,0.11071658879518509
11536,-Metrics/Training(Step): loss,1610358078227,0.09301736950874329
11538,-Metrics/Training(Step): loss,1610358082520,0.12353496253490448
11540,-Metrics/Training(Step): loss,1610358086520,0.10415378957986832
11542,-Metrics/Training(Step): loss,1610358091023,0.10942655056715012
11544,-Metrics/Training(Step): loss,1610358094922,0.14539046585559845
11546,-Metrics/Training(Step): loss,1610358098573,0.07219596952199936
11548,-Metrics/Training(Step): loss,1610358102861,0.11892511695623398
11550,-Metrics/Training(Step): loss,1610358106761,0.1057397648692131
11552,-Metrics/Training(Step): loss,1610358110115,0.14829881489276886
11554,-Metrics/Training(Step): loss,1610358113630,0.11727400869131088
11556,-Metrics/Training(Step): loss,1610358117520,0.13427956402301788
11558,-Metrics/Training(Step): loss,1610358121020,0.12075356394052505
11560,-Metrics/Training(Step): loss,1610358124287,0.1277734786272049
11562,-Metrics/Training(Step): loss,1610358127820,0.10104548186063766
11564,-Metrics/Training(Step): loss,1610358131220,0.13622905313968658
11566,-Metrics/Training(Step): loss,1610358134525,0.14117032289505005
11568,-Metrics/Training(Step): loss,1610358138065,0.10362278670072556
11570,-Metrics/Training(Step): loss,1610358141754,0.09862867742776871
11572,-Metrics/Training(Step): loss,1610358144585,0.1161101907491684
11574,-Metrics/Training(Step): loss,1610358146786,0.11653822660446167
11576,-Metrics/Training(Step): loss,1610358148910,0.11792224645614624
11578,-Metrics/Training(Step): loss,1610358150952,0.12140803784132004
11580,-Metrics/Training(Step): loss,1610358153042,0.14500854909420013
11582,-Metrics/Training(Step): loss,1610358155150,0.10795079171657562
11584,-Metrics/Training(Step): loss,1610358157071,0.10069125145673752
11586,-Metrics/Training(Step): loss,1610358159162,0.11121594160795212
11588,-Metrics/Training(Step): loss,1610358161262,0.1123250424861908
11590,-Metrics/Training(Step): loss,1610358163396,0.113979272544384
11592,-Metrics/Training(Step): loss,1610358165455,0.1373189240694046
11594,-Metrics/Training(Step): loss,1610358167503,0.09845118969678879
11596,-Metrics/Training(Step): loss,1610358169518,0.16116054356098175
11598,-Metrics/Training(Step): loss,1610358171513,0.11447764933109283
11600,-Metrics/Training(Step): loss,1610358173438,0.11249265819787979
11602,-Metrics/Training(Step): loss,1610358175426,0.0955038070678711
11604,-Metrics/Training(Step): loss,1610358177432,0.09657494723796844
11606,-Metrics/Training(Step): loss,1610358179506,0.14348816871643066
11608,-Metrics/Training(Step): loss,1610358181569,0.10331573337316513
11610,-Metrics/Training(Step): loss,1610358183649,0.13995453715324402
11612,-Metrics/Training(Step): loss,1610358196139,0.13584382832050323
11614,-Metrics/Training(Step): loss,1610358200143,0.1220851019024849
11616,-Metrics/Training(Step): loss,1610358204119,0.13219061493873596
11618,-Metrics/Training(Step): loss,1610358208138,0.12539222836494446
11620,-Metrics/Training(Step): loss,1610358212040,0.10480786859989166
11622,-Metrics/Training(Step): loss,1610358216646,0.08858530223369598
11624,-Metrics/Training(Step): loss,1610358220196,0.14271074533462524
11626,-Metrics/Training(Step): loss,1610358224120,0.10939030349254608
11628,-Metrics/Training(Step): loss,1610358227366,0.1161150336265564
11630,-Metrics/Training(Step): loss,1610358231472,0.10784262418746948
11632,-Metrics/Training(Step): loss,1610358234995,0.13376519083976746
11634,-Metrics/Training(Step): loss,1610358238420,0.1117783859372139
11636,-Metrics/Training(Step): loss,1610358242472,0.08891040831804276
11638,-Metrics/Training(Step): loss,1610358246742,0.13841038942337036
11640,-Metrics/Training(Step): loss,1610358250477,0.12896297872066498
11642,-Metrics/Training(Step): loss,1610358253657,0.13195078074932098
11644,-Metrics/Training(Step): loss,1610358256972,0.09515245258808136
11646,-Metrics/Training(Step): loss,1610358260418,0.1589238941669464
11648,-Metrics/Training(Step): loss,1610358263620,0.13443025946617126
11650,-Metrics/Training(Step): loss,1610358267220,0.11431287974119186
11652,-Metrics/Training(Step): loss,1610358270778,0.0780038982629776
11654,-Metrics/Training(Step): loss,1610358274555,0.1292479932308197
11656,-Metrics/Training(Step): loss,1610358277753,0.10110128670930862
11658,-Metrics/Training(Step): loss,1610358280855,0.12667874991893768
11660,-Metrics/Training(Step): loss,1610358282901,0.11094948649406433
11662,-Metrics/Training(Step): loss,1610358285070,0.13966286182403564
11664,-Metrics/Training(Step): loss,1610358287145,0.1367957890033722
11666,-Metrics/Training(Step): loss,1610358289253,0.14037483930587769
11668,-Metrics/Training(Step): loss,1610358291360,0.12666155397891998
11670,-Metrics/Training(Step): loss,1610358293493,0.10069051384925842
11672,-Metrics/Training(Step): loss,1610358295486,0.11414852738380432
11674,-Metrics/Training(Step): loss,1610358297583,0.11125114560127258
11676,-Metrics/Training(Step): loss,1610358299700,0.13863271474838257
11678,-Metrics/Training(Step): loss,1610358301819,0.11687865108251572
11680,-Metrics/Training(Step): loss,1610358303780,0.12686529755592346
11682,-Metrics/Training(Step): loss,1610358305578,0.11314865201711655
11684,-Metrics/Training(Step): loss,1610358307471,0.11750520765781403
11686,-Metrics/Training(Step): loss,1610358309566,0.1305471807718277
11688,-Metrics/Training(Step): loss,1610358311649,0.13664165139198303
11690,-Metrics/Training(Step): loss,1610358313709,0.11275391280651093
11692,-Metrics/Training(Step): loss,1610358315667,0.13419747352600098
11694,-Metrics/Training(Step): loss,1610358317707,0.11604585498571396
11696,-Metrics/Training(Step): loss,1610358319804,0.11755393445491791
11698,-Metrics/Training(Step): loss,1610358332331,0.116344153881073
11700,-Metrics/Training(Step): loss,1610358336220,0.14333520829677582
11702,-Metrics/Training(Step): loss,1610358340245,0.09722008556127548
11704,-Metrics/Training(Step): loss,1610358344524,0.12495793402194977
11706,-Metrics/Training(Step): loss,1610358348445,0.12063027918338776
11708,-Metrics/Training(Step): loss,1610358352621,0.1389603614807129
11710,-Metrics/Training(Step): loss,1610358356446,0.12491080909967422
11712,-Metrics/Training(Step): loss,1610358359820,0.11485496908426285
11714,-Metrics/Training(Step): loss,1610358363616,0.15168873965740204
11716,-Metrics/Training(Step): loss,1610358367020,0.11958635598421097
11718,-Metrics/Training(Step): loss,1610358370821,0.10596335679292679
11720,-Metrics/Training(Step): loss,1610358374035,0.10620872676372528
11722,-Metrics/Training(Step): loss,1610358378021,0.10875155031681061
11724,-Metrics/Training(Step): loss,1610358381718,0.1723640412092209
11726,-Metrics/Training(Step): loss,1610358385503,0.11342767626047134
11728,-Metrics/Training(Step): loss,1610358388797,0.12394461780786514
11730,-Metrics/Training(Step): loss,1610358391742,0.11879987269639969
11732,-Metrics/Training(Step): loss,1610358394921,0.10865768045186996
11734,-Metrics/Training(Step): loss,1610358398566,0.1013147234916687
11736,-Metrics/Training(Step): loss,1610358401620,0.10059943050146103
11738,-Metrics/Training(Step): loss,1610358404666,0.10532127320766449
11740,-Metrics/Training(Step): loss,1610358408220,0.11118560284376144
11742,-Metrics/Training(Step): loss,1610358411320,0.10731251537799835
11744,-Metrics/Training(Step): loss,1610358414768,0.11394771933555603
11746,-Metrics/Training(Step): loss,1610358417598,0.12520882487297058
11748,-Metrics/Training(Step): loss,1610358419722,0.1170886978507042
11750,-Metrics/Training(Step): loss,1610358421955,0.10462197661399841
11752,-Metrics/Training(Step): loss,1610358424066,0.12144462764263153
11754,-Metrics/Training(Step): loss,1610358426159,0.1316206157207489
11756,-Metrics/Training(Step): loss,1610358428285,0.14034925401210785
11758,-Metrics/Training(Step): loss,1610358430385,0.11237781494855881
11760,-Metrics/Training(Step): loss,1610358432488,0.12650153040885925
11762,-Metrics/Training(Step): loss,1610358434563,0.11423531174659729
11764,-Metrics/Training(Step): loss,1610358436446,0.11127776652574539
11766,-Metrics/Training(Step): loss,1610358438387,0.106310173869133
11768,-Metrics/Training(Step): loss,1610358440489,0.11994247138500214
11770,-Metrics/Training(Step): loss,1610358442268,0.11922178417444229
11772,-Metrics/Training(Step): loss,1610358444286,0.12811081111431122
11774,-Metrics/Training(Step): loss,1610358446317,0.14196816086769104
11776,-Metrics/Training(Step): loss,1610358448408,0.12124623358249664
11778,-Metrics/Training(Step): loss,1610358450377,0.10213650017976761
11780,-Metrics/Training(Step): loss,1610358452421,0.11574956774711609
11782,-Metrics/Training(Step): loss,1610358454504,0.10209513455629349
11784,-Metrics/Training(Step): loss,1610358467041,0.12442176043987274
11786,-Metrics/Training(Step): loss,1610358471224,0.11186560988426208
11788,-Metrics/Training(Step): loss,1610358475220,0.10513089597225189
11790,-Metrics/Training(Step): loss,1610358479740,0.11028987169265747
11792,-Metrics/Training(Step): loss,1610358484138,0.10754689574241638
11794,-Metrics/Training(Step): loss,1610358487847,0.13292114436626434
11796,-Metrics/Training(Step): loss,1610358491969,0.11160599440336227
11798,-Metrics/Training(Step): loss,1610358495243,0.12437587231397629
11800,-Metrics/Training(Step): loss,1610358498639,0.11821261793375015
11802,-Metrics/Training(Step): loss,1610358502123,0.09039419889450073
11804,-Metrics/Training(Step): loss,1610358505457,0.15738941729068756
11806,-Metrics/Training(Step): loss,1610358509120,0.13239715993404388
11808,-Metrics/Training(Step): loss,1610358512478,0.11138907819986343
11810,-Metrics/Training(Step): loss,1610358516220,0.09740400314331055
11812,-Metrics/Training(Step): loss,1610358519925,0.1143740862607956
11814,-Metrics/Training(Step): loss,1610358523424,0.11318846046924591
11816,-Metrics/Training(Step): loss,1610358527016,0.13747815787792206
11818,-Metrics/Training(Step): loss,1610358530119,0.12389052659273148
11820,-Metrics/Training(Step): loss,1610358533957,0.09789489954710007
11822,-Metrics/Training(Step): loss,1610358537517,0.11167005449533463
11824,-Metrics/Training(Step): loss,1610358540920,0.10232724994421005
11826,-Metrics/Training(Step): loss,1610358544348,0.13019004464149475
11828,-Metrics/Training(Step): loss,1610358547965,0.13987906277179718
11830,-Metrics/Training(Step): loss,1610358551044,0.10376670956611633
11832,-Metrics/Training(Step): loss,1610358554019,0.11895806342363358
11834,-Metrics/Training(Step): loss,1610358556044,0.14215275645256042
11836,-Metrics/Training(Step): loss,1610358558086,0.11041367053985596
11838,-Metrics/Training(Step): loss,1610358560163,0.13212117552757263
11840,-Metrics/Training(Step): loss,1610358562218,0.0998241975903511
11842,-Metrics/Training(Step): loss,1610358564355,0.11642151325941086
11844,-Metrics/Training(Step): loss,1610358566449,0.12130657583475113
11846,-Metrics/Training(Step): loss,1610358568484,0.11285068094730377
11848,-Metrics/Training(Step): loss,1610358570526,0.10855691879987717
11850,-Metrics/Training(Step): loss,1610358572404,0.12412731349468231
11852,-Metrics/Training(Step): loss,1610358574537,0.12613430619239807
11854,-Metrics/Training(Step): loss,1610358576648,0.09498555213212967
11856,-Metrics/Training(Step): loss,1610358578732,0.11814428865909576
11858,-Metrics/Training(Step): loss,1610358580854,0.12792474031448364
11860,-Metrics/Training(Step): loss,1610358582871,0.13859130442142487
11862,-Metrics/Training(Step): loss,1610358584849,0.09857774525880814
11864,-Metrics/Training(Step): loss,1610358586917,0.11179744452238083
11866,-Metrics/Training(Step): loss,1610358588998,0.10991545766592026
11868,-Metrics/Training(Step): loss,1610358591051,0.12232653051614761
11870,-Metrics/Training(Step): loss,1610358603840,0.13624700903892517
11872,-Metrics/Training(Step): loss,1610358608028,0.1117088720202446
11874,-Metrics/Training(Step): loss,1610358611720,0.10619127750396729
11876,-Metrics/Training(Step): loss,1610358615725,0.10569250583648682
11878,-Metrics/Training(Step): loss,1610358619920,0.11621105670928955
11880,-Metrics/Training(Step): loss,1610358623826,0.1213325709104538
11882,-Metrics/Training(Step): loss,1610358627524,0.11114498972892761
11884,-Metrics/Training(Step): loss,1610358631288,0.11775442212820053
11886,-Metrics/Training(Step): loss,1610358635199,0.1267230361700058
11888,-Metrics/Training(Step): loss,1610358639249,0.11334957927465439
11890,-Metrics/Training(Step): loss,1610358643064,0.10123449563980103
11892,-Metrics/Training(Step): loss,1610358646822,0.12960708141326904
11894,-Metrics/Training(Step): loss,1610358650320,0.10762309283018112
11896,-Metrics/Training(Step): loss,1610358654100,0.13158723711967468
11898,-Metrics/Training(Step): loss,1610358657479,0.09098131209611893
11900,-Metrics/Training(Step): loss,1610358661320,0.1077347844839096
11902,-Metrics/Training(Step): loss,1610358665220,0.09475473314523697
11904,-Metrics/Training(Step): loss,1610358668519,0.07781483978033066
11906,-Metrics/Training(Step): loss,1610358672120,0.11861549317836761
11908,-Metrics/Training(Step): loss,1610358675737,0.10920682549476624
11910,-Metrics/Training(Step): loss,1610358679898,0.10487398505210876
11912,-Metrics/Training(Step): loss,1610358683423,0.08967258036136627
11914,-Metrics/Training(Step): loss,1610358686929,0.1223960593342781
11916,-Metrics/Training(Step): loss,1610358689473,0.10942540317773819
11918,-Metrics/Training(Step): loss,1610358691790,0.1284741461277008
11920,-Metrics/Training(Step): loss,1610358693957,0.11601125448942184
11922,-Metrics/Training(Step): loss,1610358696175,0.09647062420845032
11924,-Metrics/Training(Step): loss,1610358698252,0.11710576713085175
11926,-Metrics/Training(Step): loss,1610358700346,0.11931271851062775
11928,-Metrics/Training(Step): loss,1610358702453,0.10213429480791092
11930,-Metrics/Training(Step): loss,1610358704389,0.11338450759649277
11932,-Metrics/Training(Step): loss,1610358706287,0.09947282820940018
11934,-Metrics/Training(Step): loss,1610358708277,0.12634876370429993
11936,-Metrics/Training(Step): loss,1610358710370,0.13277195394039154
11938,-Metrics/Training(Step): loss,1610358712287,0.12437557429075241
11940,-Metrics/Training(Step): loss,1610358714267,0.10464300960302353
11942,-Metrics/Training(Step): loss,1610358716394,0.10194376111030579
11944,-Metrics/Training(Step): loss,1610358718457,0.10278195887804031
11946,-Metrics/Training(Step): loss,1610358720552,0.1320236325263977
11948,-Metrics/Training(Step): loss,1610358722621,0.10258835554122925
11950,-Metrics/Training(Step): loss,1610358724669,0.10047861188650131
11952,-Metrics/Training(Step): loss,1610358726718,0.10945877432823181
11954,-Metrics/Training(Step): loss,1610358728742,0.12494740635156631
11956,-Metrics/Training(Step): loss,1610358741632,0.12183643877506256
11958,-Metrics/Training(Step): loss,1610358745720,0.10932714492082596
11960,-Metrics/Training(Step): loss,1610358749722,0.11914999037981033
11962,-Metrics/Training(Step): loss,1610358753921,0.13031335175037384
11964,-Metrics/Training(Step): loss,1610358757752,0.10265854746103287
11966,-Metrics/Training(Step): loss,1610358762226,0.07686448097229004
11968,-Metrics/Training(Step): loss,1610358766121,0.09240598231554031
11970,-Metrics/Training(Step): loss,1610358769820,0.13467994332313538
11972,-Metrics/Training(Step): loss,1610358773106,0.08581043034791946
11974,-Metrics/Training(Step): loss,1610358776621,0.10983011871576309
11976,-Metrics/Training(Step): loss,1610358780620,0.1174771711230278
11978,-Metrics/Training(Step): loss,1610358784320,0.11659476906061172
11980,-Metrics/Training(Step): loss,1610358788251,0.1311837136745453
11982,-Metrics/Training(Step): loss,1610358792130,0.11767546832561493
11984,-Metrics/Training(Step): loss,1610358795920,0.12476909905672073
11986,-Metrics/Training(Step): loss,1610358799420,0.12353403121232986
11988,-Metrics/Training(Step): loss,1610358803519,0.1121721938252449
11990,-Metrics/Training(Step): loss,1610358806620,0.12267206609249115
11992,-Metrics/Training(Step): loss,1610358810041,0.11880159378051758
11994,-Metrics/Training(Step): loss,1610358813466,0.13288867473602295
11996,-Metrics/Training(Step): loss,1610358817550,0.13229355216026306
11998,-Metrics/Training(Step): loss,1610358820516,0.09495795518159866
12000,-Metrics/Training(Step): loss,1610358823781,0.09283705055713654
12002,-Metrics/Training(Step): loss,1610358826082,0.06925811618566513
12004,-Metrics/Training(Step): loss,1610358828250,0.1119580790400505
12006,-Metrics/Training(Step): loss,1610358830277,0.1065177321434021
12008,-Metrics/Training(Step): loss,1610358832318,0.12915363907814026
12010,-Metrics/Training(Step): loss,1610358834384,0.1339932233095169
12012,-Metrics/Training(Step): loss,1610358836503,0.1112561896443367
12014,-Metrics/Training(Step): loss,1610358838512,0.12389041483402252
12016,-Metrics/Training(Step): loss,1610358840589,0.14154823124408722
12018,-Metrics/Training(Step): loss,1610358842697,0.127788707613945
12020,-Metrics/Training(Step): loss,1610358844663,0.12857212126255035
12022,-Metrics/Training(Step): loss,1610358846618,0.08293652534484863
12024,-Metrics/Training(Step): loss,1610358848589,0.11010441929101944
12026,-Metrics/Training(Step): loss,1610358850447,0.11069580167531967
12028,-Metrics/Training(Step): loss,1610358852461,0.11773735284805298
12030,-Metrics/Training(Step): loss,1610358854452,0.1083233430981636
12032,-Metrics/Training(Step): loss,1610358856531,0.10844463109970093
12034,-Metrics/Training(Step): loss,1610358858592,0.10930217802524567
12036,-Metrics/Training(Step): loss,1610358860681,0.09767789393663406
12038,-Metrics/Training(Step): loss,1610358862694,0.11444561928510666
12040,-Metrics/Training(Step): loss,1610358864788,0.10536446422338486
12042,-Metrics/Training(Step): loss,1610358878026,0.12724338471889496
12044,-Metrics/Training(Step): loss,1610358882223,0.12182960659265518
12046,-Metrics/Training(Step): loss,1610358886020,0.11310426145792007
12048,-Metrics/Training(Step): loss,1610358889739,0.10645820945501328
12050,-Metrics/Training(Step): loss,1610358894234,0.16110391914844513
12052,-Metrics/Training(Step): loss,1610358898720,0.09213344752788544
12054,-Metrics/Training(Step): loss,1610358902657,0.13130207359790802
12056,-Metrics/Training(Step): loss,1610358906520,0.09196148812770844
12058,-Metrics/Training(Step): loss,1610358910228,0.11067430675029755
12060,-Metrics/Training(Step): loss,1610358913838,0.12484154105186462
12062,-Metrics/Training(Step): loss,1610358917123,0.1279507726430893
12064,-Metrics/Training(Step): loss,1610358920442,0.11507930606603622
12066,-Metrics/Training(Step): loss,1610358923920,0.12026149779558182
12068,-Metrics/Training(Step): loss,1610358927520,0.11347397416830063
12070,-Metrics/Training(Step): loss,1610358931046,0.1186317652463913
12072,-Metrics/Training(Step): loss,1610358934161,0.1282903254032135
12074,-Metrics/Training(Step): loss,1610358938395,0.13343599438667297
12076,-Metrics/Training(Step): loss,1610358942090,0.10917970538139343
12078,-Metrics/Training(Step): loss,1610358945924,0.11236505210399628
12080,-Metrics/Training(Step): loss,1610358949720,0.11151343584060669
12082,-Metrics/Training(Step): loss,1610358952815,0.1250624656677246
12084,-Metrics/Training(Step): loss,1610358956519,0.11722030490636826
12086,-Metrics/Training(Step): loss,1610358960036,0.10942854732275009
12088,-Metrics/Training(Step): loss,1610358962835,0.12114979326725006
12090,-Metrics/Training(Step): loss,1610358965549,0.10280507802963257
12092,-Metrics/Training(Step): loss,1610358967708,0.11908099055290222
12094,-Metrics/Training(Step): loss,1610358969852,0.11729513108730316
12096,-Metrics/Training(Step): loss,1610358971948,0.11771559715270996
12098,-Metrics/Training(Step): loss,1610358974059,0.1029713824391365
12100,-Metrics/Training(Step): loss,1610358976149,0.13277499377727509
12102,-Metrics/Training(Step): loss,1610358978163,0.09310702979564667
12104,-Metrics/Training(Step): loss,1610358980257,0.11447937786579132
12106,-Metrics/Training(Step): loss,1610358982185,0.1046338602900505
12108,-Metrics/Training(Step): loss,1610358984291,0.124454565346241
12110,-Metrics/Training(Step): loss,1610358986363,0.14842034876346588
12112,-Metrics/Training(Step): loss,1610358988446,0.09908261150121689
12114,-Metrics/Training(Step): loss,1610358990547,0.11788111925125122
12116,-Metrics/Training(Step): loss,1610358992600,0.10465818643569946
12118,-Metrics/Training(Step): loss,1610358994629,0.10490687191486359
12120,-Metrics/Training(Step): loss,1610358996736,0.09885608404874802
12122,-Metrics/Training(Step): loss,1610358998810,0.13126063346862793
12124,-Metrics/Training(Step): loss,1610359000835,0.12729424238204956
12126,-Metrics/Training(Step): loss,1610359002888,0.08693084120750427
12128,-Metrics/Training(Step): loss,1610359015639,0.10784106701612473
12130,-Metrics/Training(Step): loss,1610359019620,0.12729935348033905
12132,-Metrics/Training(Step): loss,1610359023620,0.10503178089857101
12134,-Metrics/Training(Step): loss,1610359027547,0.0991668552160263
12136,-Metrics/Training(Step): loss,1610359032320,0.09038073569536209
12138,-Metrics/Training(Step): loss,1610359036219,0.12513062357902527
12140,-Metrics/Training(Step): loss,1610359040429,0.0824495255947113
12142,-Metrics/Training(Step): loss,1610359044725,0.12288383394479752
12144,-Metrics/Training(Step): loss,1610359048734,0.13580238819122314
12146,-Metrics/Training(Step): loss,1610359052820,0.11901765316724777
12148,-Metrics/Training(Step): loss,1610359055958,0.12956029176712036
12150,-Metrics/Training(Step): loss,1610359060153,0.10863383114337921
12152,-Metrics/Training(Step): loss,1610359063825,0.12609164416790009
12154,-Metrics/Training(Step): loss,1610359067521,0.12410881370306015
12156,-Metrics/Training(Step): loss,1610359071019,0.08560959249734879
12158,-Metrics/Training(Step): loss,1610359074537,0.09966578334569931
12160,-Metrics/Training(Step): loss,1610359078178,0.09542693942785263
12162,-Metrics/Training(Step): loss,1610359081827,0.12628109753131866
12164,-Metrics/Training(Step): loss,1610359085421,0.11139585077762604
12166,-Metrics/Training(Step): loss,1610359088620,0.09040310978889465
12168,-Metrics/Training(Step): loss,1610359092253,0.12205267697572708
12170,-Metrics/Training(Step): loss,1610359095948,0.12171664834022522
12172,-Metrics/Training(Step): loss,1610359099544,0.11004773527383804
12174,-Metrics/Training(Step): loss,1610359101990,0.13497714698314667
12176,-Metrics/Training(Step): loss,1610359104046,0.10818470269441605
12178,-Metrics/Training(Step): loss,1610359106144,0.0961601585149765
12180,-Metrics/Training(Step): loss,1610359108248,0.10305867344141006
12182,-Metrics/Training(Step): loss,1610359110358,0.11786694079637527
12184,-Metrics/Training(Step): loss,1610359112508,0.11251646280288696
12186,-Metrics/Training(Step): loss,1610359114570,0.1081979051232338
12188,-Metrics/Training(Step): loss,1610359116699,0.0909636840224266
12190,-Metrics/Training(Step): loss,1610359118776,0.09112923592329025
12192,-Metrics/Training(Step): loss,1610359120656,0.09502049535512924
12194,-Metrics/Training(Step): loss,1610359122738,0.13771623373031616
12196,-Metrics/Training(Step): loss,1610359124815,0.12207503616809845
12198,-Metrics/Training(Step): loss,1610359126922,0.0930366963148117
12200,-Metrics/Training(Step): loss,1610359129005,0.12034960836172104
12202,-Metrics/Training(Step): loss,1610359130999,0.10232844948768616
12204,-Metrics/Training(Step): loss,1610359133068,0.12286922335624695
12206,-Metrics/Training(Step): loss,1610359135102,0.1057552769780159
12208,-Metrics/Training(Step): loss,1610359137156,0.11781922727823257
12210,-Metrics/Training(Step): loss,1610359139232,0.08079423755407333
12212,-Metrics/Training(Step): loss,1610359141274,0.10483840852975845
12214,-Metrics/Training(Step): loss,1610359154032,0.10827020555734634
12216,-Metrics/Training(Step): loss,1610359158120,0.11275368183851242
12218,-Metrics/Training(Step): loss,1610359162416,0.08609500527381897
12220,-Metrics/Training(Step): loss,1610359166358,0.128386989235878
12222,-Metrics/Training(Step): loss,1610359170423,0.11569265276193619
12224,-Metrics/Training(Step): loss,1610359174422,0.12094656378030777
12226,-Metrics/Training(Step): loss,1610359178247,0.12563279271125793
12228,-Metrics/Training(Step): loss,1610359182032,0.09478796273469925
12230,-Metrics/Training(Step): loss,1610359185625,0.11588650196790695
12232,-Metrics/Training(Step): loss,1610359190083,0.14603324234485626
12234,-Metrics/Training(Step): loss,1610359193520,0.10081825405359268
12236,-Metrics/Training(Step): loss,1610359197220,0.10797473043203354
12238,-Metrics/Training(Step): loss,1610359200863,0.07677700370550156
12240,-Metrics/Training(Step): loss,1610359204438,0.09843651205301285
12242,-Metrics/Training(Step): loss,1610359208258,0.12184632569551468
12244,-Metrics/Training(Step): loss,1610359212259,0.10145457834005356
12246,-Metrics/Training(Step): loss,1610359215938,0.08539783954620361
12248,-Metrics/Training(Step): loss,1610359219417,0.1202702447772026
12250,-Metrics/Training(Step): loss,1610359222747,0.10965833812952042
12252,-Metrics/Training(Step): loss,1610359226761,0.08857659995555878
12254,-Metrics/Training(Step): loss,1610359230388,0.0816718190908432
12256,-Metrics/Training(Step): loss,1610359233889,0.12687036395072937
12258,-Metrics/Training(Step): loss,1610359236925,0.10782807320356369
12260,-Metrics/Training(Step): loss,1610359239354,0.11250058561563492
12262,-Metrics/Training(Step): loss,1610359241550,0.12937268614768982
12264,-Metrics/Training(Step): loss,1610359243529,0.11295858025550842
12266,-Metrics/Training(Step): loss,1610359245594,0.1144694909453392
12268,-Metrics/Training(Step): loss,1610359247665,0.1168820932507515
12270,-Metrics/Training(Step): loss,1610359249796,0.14043007791042328
12272,-Metrics/Training(Step): loss,1610359251912,0.11282304674386978
12274,-Metrics/Training(Step): loss,1610359254051,0.11018279939889908
12276,-Metrics/Training(Step): loss,1610359256016,0.1166795939207077
12278,-Metrics/Training(Step): loss,1610359257929,0.10649065673351288
12280,-Metrics/Training(Step): loss,1610359259986,0.0873580276966095
12282,-Metrics/Training(Step): loss,1610359262067,0.11117704212665558
12284,-Metrics/Training(Step): loss,1610359264013,0.10544725507497787
12286,-Metrics/Training(Step): loss,1610359266148,0.10097341239452362
12288,-Metrics/Training(Step): loss,1610359268201,0.09975768625736237
12290,-Metrics/Training(Step): loss,1610359270346,0.1285209208726883
12292,-Metrics/Training(Step): loss,1610359272396,0.11666993796825409
12294,-Metrics/Training(Step): loss,1610359274447,0.11163977533578873
12296,-Metrics/Training(Step): loss,1610359276491,0.12302970141172409
12298,-Metrics/Training(Step): loss,1610359278536,0.1450689435005188
12300,-Metrics/Training(Step): loss,1610359291834,0.11292481422424316
12302,-Metrics/Training(Step): loss,1610359295841,0.10394661128520966
12304,-Metrics/Training(Step): loss,1610359300121,0.10337506234645844
12306,-Metrics/Training(Step): loss,1610359304135,0.10825999081134796
12308,-Metrics/Training(Step): loss,1610359308127,0.11494752019643784
12310,-Metrics/Training(Step): loss,1610359312221,0.12672147154808044
12312,-Metrics/Training(Step): loss,1610359316020,0.11594021320343018
12314,-Metrics/Training(Step): loss,1610359319147,0.13515931367874146
12316,-Metrics/Training(Step): loss,1610359322720,0.11914654076099396
12318,-Metrics/Training(Step): loss,1610359326658,0.10200808942317963
12320,-Metrics/Training(Step): loss,1610359330720,0.12910382449626923
12322,-Metrics/Training(Step): loss,1610359334325,0.11660566180944443
12324,-Metrics/Training(Step): loss,1610359337814,0.11627386510372162
12326,-Metrics/Training(Step): loss,1610359341220,0.11907859891653061
12328,-Metrics/Training(Step): loss,1610359344469,0.11878117173910141
12330,-Metrics/Training(Step): loss,1610359348316,0.07728958874940872
12332,-Metrics/Training(Step): loss,1610359351987,0.11863736808300018
12334,-Metrics/Training(Step): loss,1610359356020,0.09580609947443008
12336,-Metrics/Training(Step): loss,1610359359908,0.1053006574511528
12338,-Metrics/Training(Step): loss,1610359363426,0.10048850625753403
12340,-Metrics/Training(Step): loss,1610359366519,0.0932612344622612
12342,-Metrics/Training(Step): loss,1610359370157,0.07932136952877045
12344,-Metrics/Training(Step): loss,1610359373742,0.1299581378698349
12346,-Metrics/Training(Step): loss,1610359376341,0.1049748882651329
12348,-Metrics/Training(Step): loss,1610359378498,0.10996734350919724
12350,-Metrics/Training(Step): loss,1610359380587,0.11900611966848373
12352,-Metrics/Training(Step): loss,1610359382639,0.11353355646133423
12354,-Metrics/Training(Step): loss,1610359384694,0.12005513906478882
12356,-Metrics/Training(Step): loss,1610359386723,0.10833632946014404
12358,-Metrics/Training(Step): loss,1610359388830,0.0990523248910904
12360,-Metrics/Training(Step): loss,1610359390952,0.11796882003545761
12362,-Metrics/Training(Step): loss,1610359393094,0.09487336874008179
12364,-Metrics/Training(Step): loss,1610359395097,0.10135150700807571
12366,-Metrics/Training(Step): loss,1610359397205,0.08625024557113647
12368,-Metrics/Training(Step): loss,1610359399234,0.11832630634307861
12370,-Metrics/Training(Step): loss,1610359401184,0.12639068067073822
12372,-Metrics/Training(Step): loss,1610359403030,0.08218403905630112
12374,-Metrics/Training(Step): loss,1610359405036,0.10147874057292938
12376,-Metrics/Training(Step): loss,1610359407126,0.08987351506948471
12378,-Metrics/Training(Step): loss,1610359409226,0.11431945115327835
12380,-Metrics/Training(Step): loss,1610359411291,0.1035507321357727
12382,-Metrics/Training(Step): loss,1610359413315,0.09471277892589569
12384,-Metrics/Training(Step): loss,1610359415416,0.1182161495089531
12386,-Metrics/Training(Step): loss,1610359430346,0.09841032326221466
12388,-Metrics/Training(Step): loss,1610359434623,0.13051752746105194
12390,-Metrics/Training(Step): loss,1610359438921,0.09985579550266266
12392,-Metrics/Training(Step): loss,1610359442753,0.09640196710824966
12394,-Metrics/Training(Step): loss,1610359446645,0.10672209411859512
12396,-Metrics/Training(Step): loss,1610359450719,0.13322633504867554
12398,-Metrics/Training(Step): loss,1610359454621,0.13349229097366333
12400,-Metrics/Training(Step): loss,1610359458420,0.10495878756046295
12402,-Metrics/Training(Step): loss,1610359461646,0.12300727516412735
12404,-Metrics/Training(Step): loss,1610359465520,0.08521303534507751
12406,-Metrics/Training(Step): loss,1610359468819,0.11985263228416443
12408,-Metrics/Training(Step): loss,1610359472621,0.10208005458116531
12410,-Metrics/Training(Step): loss,1610359476322,0.0890272855758667
12412,-Metrics/Training(Step): loss,1610359479620,0.10085639357566833
12414,-Metrics/Training(Step): loss,1610359483471,0.11756173521280289
12416,-Metrics/Training(Step): loss,1610359486320,0.1115766316652298
12418,-Metrics/Training(Step): loss,1610359490327,0.1540037989616394
12420,-Metrics/Training(Step): loss,1610359493515,0.0984337106347084
12422,-Metrics/Training(Step): loss,1610359496994,0.11509160697460175
12424,-Metrics/Training(Step): loss,1610359500528,0.11154554784297943
12426,-Metrics/Training(Step): loss,1610359504228,0.09665614366531372
12428,-Metrics/Training(Step): loss,1610359508046,0.09296753257513046
12430,-Metrics/Training(Step): loss,1610359511823,0.1059143915772438
12432,-Metrics/Training(Step): loss,1610359514765,0.1602870523929596
12434,-Metrics/Training(Step): loss,1610359517162,0.09442564100027084
12436,-Metrics/Training(Step): loss,1610359519375,0.12249135226011276
12438,-Metrics/Training(Step): loss,1610359521588,0.13219040632247925
12440,-Metrics/Training(Step): loss,1610359523623,0.1249687522649765
12442,-Metrics/Training(Step): loss,1610359525705,0.12912999093532562
12444,-Metrics/Training(Step): loss,1610359527817,0.11876519024372101
12446,-Metrics/Training(Step): loss,1610359529873,0.10362832248210907
12448,-Metrics/Training(Step): loss,1610359531976,0.11748385429382324
12450,-Metrics/Training(Step): loss,1610359534019,0.09445679932832718
12452,-Metrics/Training(Step): loss,1610359536022,0.07879394292831421
12454,-Metrics/Training(Step): loss,1610359537928,0.1234583705663681
12456,-Metrics/Training(Step): loss,1610359539746,0.09360203146934509
12458,-Metrics/Training(Step): loss,1610359541850,0.11326030641794205
12460,-Metrics/Training(Step): loss,1610359543917,0.09482027590274811
12462,-Metrics/Training(Step): loss,1610359545948,0.10353972762823105
12464,-Metrics/Training(Step): loss,1610359547910,0.11186741292476654
12466,-Metrics/Training(Step): loss,1610359549951,0.10247329622507095
12468,-Metrics/Training(Step): loss,1610359552005,0.13855279982089996
12470,-Metrics/Training(Step): loss,1610359554044,0.11031242460012436
12472,-Metrics/Training(Step): loss,1610359566330,0.14260363578796387
12474,-Metrics/Training(Step): loss,1610359570329,0.13436122238636017
12476,-Metrics/Training(Step): loss,1610359574320,0.09919428825378418
12478,-Metrics/Training(Step): loss,1610359578215,0.10676535218954086
12480,-Metrics/Training(Step): loss,1610359582330,0.12758903205394745
12482,-Metrics/Training(Step): loss,1610359586819,0.13487207889556885
12484,-Metrics/Training(Step): loss,1610359591121,0.15221555531024933
12486,-Metrics/Training(Step): loss,1610359594725,0.14207446575164795
12488,-Metrics/Training(Step): loss,1610359598594,0.13774095475673676
12490,-Metrics/Training(Step): loss,1610359602629,0.222549170255661
12492,-Metrics/Training(Step): loss,1610359606164,0.15234550833702087
12494,-Metrics/Training(Step): loss,1610359610004,0.16738808155059814
12496,-Metrics/Training(Step): loss,1610359613525,0.18978720903396606
12498,-Metrics/Training(Step): loss,1610359617220,0.16814304888248444
12500,-Metrics/Training(Step): loss,1610359620649,0.19773882627487183
12502,-Metrics/Training(Step): loss,1610359624489,0.37180137634277344
12504,-Metrics/Training(Step): loss,1610359628320,0.207084521651268
12506,-Metrics/Training(Step): loss,1610359631946,0.15387675166130066
12508,-Metrics/Training(Step): loss,1610359635574,0.42072731256484985
12510,-Metrics/Training(Step): loss,1610359639237,0.17346912622451782
12512,-Metrics/Training(Step): loss,1610359642591,0.29557520151138306
12514,-Metrics/Training(Step): loss,1610359646040,0.23844465613365173
12516,-Metrics/Training(Step): loss,1610359649658,0.21703532338142395
12518,-Metrics/Training(Step): loss,1610359652387,0.17323502898216248
12520,-Metrics/Training(Step): loss,1610359654430,0.2849740982055664
12522,-Metrics/Training(Step): loss,1610359656472,0.2564298212528229
12524,-Metrics/Training(Step): loss,1610359658419,0.2155003696680069
12526,-Metrics/Training(Step): loss,1610359660488,0.21418063342571259
12528,-Metrics/Training(Step): loss,1610359662588,0.3201233446598053
12530,-Metrics/Training(Step): loss,1610359664581,0.2926456034183502
12532,-Metrics/Training(Step): loss,1610359666687,0.2545149624347687
12534,-Metrics/Training(Step): loss,1610359668704,0.23630230128765106
12536,-Metrics/Training(Step): loss,1610359670728,0.19622373580932617
12538,-Metrics/Training(Step): loss,1610359672709,0.2854927182197571
12540,-Metrics/Training(Step): loss,1610359674827,0.18544621765613556
12542,-Metrics/Training(Step): loss,1610359676862,0.18473680317401886
12544,-Metrics/Training(Step): loss,1610359678924,0.27037283778190613
12546,-Metrics/Training(Step): loss,1610359680878,0.1740555614233017
12548,-Metrics/Training(Step): loss,1610359682922,0.16981975734233856
12550,-Metrics/Training(Step): loss,1610359685044,0.23035666346549988
12552,-Metrics/Training(Step): loss,1610359687146,0.2719145715236664
12554,-Metrics/Training(Step): loss,1610359689228,0.17312058806419373
12556,-Metrics/Training(Step): loss,1610359691326,0.17198976874351501
12558,-Metrics/Training(Step): loss,1610359704735,0.21326182782649994
12560,-Metrics/Training(Step): loss,1610359709017,0.2040862739086151
12562,-Metrics/Training(Step): loss,1610359713145,0.21941883862018585
12564,-Metrics/Training(Step): loss,1610359717122,0.2225940227508545
12566,-Metrics/Training(Step): loss,1610359721121,0.2088264673948288
12568,-Metrics/Training(Step): loss,1610359725216,0.21679890155792236
12570,-Metrics/Training(Step): loss,1610359728920,0.1556708961725235
12572,-Metrics/Training(Step): loss,1610359732923,0.16222232580184937
12574,-Metrics/Training(Step): loss,1610359736622,0.21631339192390442
12576,-Metrics/Training(Step): loss,1610359739928,0.19044265151023865
12578,-Metrics/Training(Step): loss,1610359743378,0.16364739835262299
12580,-Metrics/Training(Step): loss,1610359746847,0.1848771572113037
12582,-Metrics/Training(Step): loss,1610359750329,0.18480989336967468
12584,-Metrics/Training(Step): loss,1610359754020,0.18015609681606293
12586,-Metrics/Training(Step): loss,1610359757320,0.12447866052389145
12588,-Metrics/Training(Step): loss,1610359760916,0.18514709174633026
12590,-Metrics/Training(Step): loss,1610359764502,0.14699219167232513
12592,-Metrics/Training(Step): loss,1610359768133,0.16601867973804474
12594,-Metrics/Training(Step): loss,1610359771937,0.19679726660251617
12596,-Metrics/Training(Step): loss,1610359775321,0.14487776160240173
12598,-Metrics/Training(Step): loss,1610359778441,0.12592260539531708
12600,-Metrics/Training(Step): loss,1610359781728,0.2082420140504837
12602,-Metrics/Training(Step): loss,1610359784421,0.16128912568092346
12604,-Metrics/Training(Step): loss,1610359787755,0.140298530459404
12606,-Metrics/Training(Step): loss,1610359790388,0.15779520571231842
12608,-Metrics/Training(Step): loss,1610359792552,0.18347595632076263
12610,-Metrics/Training(Step): loss,1610359794696,0.19550856947898865
12612,-Metrics/Training(Step): loss,1610359796785,0.14716173708438873
12614,-Metrics/Training(Step): loss,1610359798842,0.12185563892126083
12616,-Metrics/Training(Step): loss,1610359800891,0.15836068987846375
12618,-Metrics/Training(Step): loss,1610359802996,0.1535773128271103
12620,-Metrics/Training(Step): loss,1610359805161,0.174673393368721
12622,-Metrics/Training(Step): loss,1610359807126,0.11736341565847397
12624,-Metrics/Training(Step): loss,1610359809262,0.12874342501163483
12626,-Metrics/Training(Step): loss,1610359811262,0.17626221477985382
12628,-Metrics/Training(Step): loss,1610359813292,0.12087182700634003
12630,-Metrics/Training(Step): loss,1610359815254,0.1417088806629181
12632,-Metrics/Training(Step): loss,1610359817304,0.11104890704154968
12634,-Metrics/Training(Step): loss,1610359819325,0.1377393901348114
12636,-Metrics/Training(Step): loss,1610359821415,0.14491045475006104
12638,-Metrics/Training(Step): loss,1610359823387,0.20317316055297852
12640,-Metrics/Training(Step): loss,1610359825392,0.1284264326095581
12642,-Metrics/Training(Step): loss,1610359827478,0.1418170928955078
12644,-Metrics/Training(Step): loss,1610359841243,0.11807312071323395
12646,-Metrics/Training(Step): loss,1610359845120,0.13661068677902222
12648,-Metrics/Training(Step): loss,1610359849015,0.1286090463399887
12650,-Metrics/Training(Step): loss,1610359852658,0.16972209513187408
12652,-Metrics/Training(Step): loss,1610359856631,0.13225366175174713
12654,-Metrics/Training(Step): loss,1610359860520,0.16634796559810638
12656,-Metrics/Training(Step): loss,1610359864420,0.1124802976846695
12658,-Metrics/Training(Step): loss,1610359868384,0.1488351970911026
12660,-Metrics/Training(Step): loss,1610359871720,0.16123612225055695
12662,-Metrics/Training(Step): loss,1610359875322,0.132671520113945
12664,-Metrics/Training(Step): loss,1610359878720,0.13690340518951416
12666,-Metrics/Training(Step): loss,1610359882315,0.1424037516117096
12668,-Metrics/Training(Step): loss,1610359886019,0.11749289184808731
12670,-Metrics/Training(Step): loss,1610359889720,0.12616565823554993
12672,-Metrics/Training(Step): loss,1610359893419,0.1321103274822235
12674,-Metrics/Training(Step): loss,1610359896042,0.12778492271900177
12676,-Metrics/Training(Step): loss,1610359899716,0.13949428498744965
12678,-Metrics/Training(Step): loss,1610359903620,0.1018306165933609
12680,-Metrics/Training(Step): loss,1610359907015,0.13910886645317078
12682,-Metrics/Training(Step): loss,1610359910420,0.15673696994781494
12684,-Metrics/Training(Step): loss,1610359913887,0.1372268795967102
12686,-Metrics/Training(Step): loss,1610359917247,0.12119414657354355
12688,-Metrics/Training(Step): loss,1610359920567,0.10806652158498764
12690,-Metrics/Training(Step): loss,1610359923852,0.13867373764514923
12692,-Metrics/Training(Step): loss,1610359926128,0.1488211750984192
12694,-Metrics/Training(Step): loss,1610359928417,0.14046581089496613
12696,-Metrics/Training(Step): loss,1610359930596,0.12846045196056366
12698,-Metrics/Training(Step): loss,1610359932703,0.13429318368434906
12700,-Metrics/Training(Step): loss,1610359934731,0.16683344542980194
12702,-Metrics/Training(Step): loss,1610359936738,0.18132077157497406
12704,-Metrics/Training(Step): loss,1610359938677,0.1338645964860916
12706,-Metrics/Training(Step): loss,1610359940728,0.13439969718456268
12708,-Metrics/Training(Step): loss,1610359942716,0.12395448982715607
12710,-Metrics/Training(Step): loss,1610359944709,0.12824183702468872
12712,-Metrics/Training(Step): loss,1610359946789,0.12823417782783508
12714,-Metrics/Training(Step): loss,1610359948896,0.11515913903713226
12716,-Metrics/Training(Step): loss,1610359950894,0.11012593656778336
12718,-Metrics/Training(Step): loss,1610359952959,0.14474575221538544
12720,-Metrics/Training(Step): loss,1610359955057,0.11869071424007416
12722,-Metrics/Training(Step): loss,1610359957096,0.1344960629940033
12724,-Metrics/Training(Step): loss,1610359959193,0.1377234160900116
12726,-Metrics/Training(Step): loss,1610359961207,0.0923791378736496
12728,-Metrics/Training(Step): loss,1610359963227,0.11725735664367676
12730,-Metrics/Training(Step): loss,1610359976848,0.137007474899292
12732,-Metrics/Training(Step): loss,1610359981021,0.11223147064447403
12734,-Metrics/Training(Step): loss,1610359985120,0.13912037014961243
12736,-Metrics/Training(Step): loss,1610359988822,0.13074100017547607
12738,-Metrics/Training(Step): loss,1610359992931,0.11473032087087631
12740,-Metrics/Training(Step): loss,1610359996647,0.14294297993183136
12742,-Metrics/Training(Step): loss,1610359999820,0.12468483299016953
12744,-Metrics/Training(Step): loss,1610360003720,0.12063759565353394
12746,-Metrics/Training(Step): loss,1610360006920,0.12634915113449097
12748,-Metrics/Training(Step): loss,1610360010485,0.12023240327835083
12750,-Metrics/Training(Step): loss,1610360014051,0.1326064169406891
12752,-Metrics/Training(Step): loss,1610360018163,0.11233820766210556
12754,-Metrics/Training(Step): loss,1610360022320,0.1110033392906189
12756,-Metrics/Training(Step): loss,1610360026254,0.12187250703573227
12758,-Metrics/Training(Step): loss,1610360030125,0.13785482943058014
12760,-Metrics/Training(Step): loss,1610360033976,0.1319323629140854
12762,-Metrics/Training(Step): loss,1610360037879,0.12090082466602325
12764,-Metrics/Training(Step): loss,1610360041551,0.10791376978158951
12766,-Metrics/Training(Step): loss,1610360044763,0.10549942404031754
12768,-Metrics/Training(Step): loss,1610360047988,0.10845120996236801
12770,-Metrics/Training(Step): loss,1610360051419,0.11776946485042572
12772,-Metrics/Training(Step): loss,1610360054926,0.14611420035362244
12774,-Metrics/Training(Step): loss,1610360059021,0.12306423485279083
12776,-Metrics/Training(Step): loss,1610360061914,0.102877177298069
12778,-Metrics/Training(Step): loss,1610360064009,0.11902023106813431
12780,-Metrics/Training(Step): loss,1610360066305,0.13175594806671143
12782,-Metrics/Training(Step): loss,1610360068370,0.12502555549144745
12784,-Metrics/Training(Step): loss,1610360070453,0.11953557282686234
12786,-Metrics/Training(Step): loss,1610360072573,0.12134256213903427
12788,-Metrics/Training(Step): loss,1610360074687,0.12282376736402512
12790,-Metrics/Training(Step): loss,1610360076776,0.11563149839639664
12792,-Metrics/Training(Step): loss,1610360078706,0.1265960931777954
12794,-Metrics/Training(Step): loss,1610360080730,0.1279611885547638
12796,-Metrics/Training(Step): loss,1610360082809,0.11642280966043472
12798,-Metrics/Training(Step): loss,1610360084853,0.11338068544864655
12800,-Metrics/Training(Step): loss,1610360086796,0.11481434106826782
12802,-Metrics/Training(Step): loss,1610360088898,0.11208733916282654
12804,-Metrics/Training(Step): loss,1610360091006,0.10568251460790634
12806,-Metrics/Training(Step): loss,1610360092994,0.11563660204410553
12808,-Metrics/Training(Step): loss,1610360095102,0.1256188601255417
12810,-Metrics/Training(Step): loss,1610360097237,0.09667284786701202
12812,-Metrics/Training(Step): loss,1610360099241,0.10526970028877258
12814,-Metrics/Training(Step): loss,1610360101300,0.10288584232330322
12816,-Metrics/Training(Step): loss,1610360114127,0.10677842795848846
12818,-Metrics/Training(Step): loss,1610360118415,0.10993238538503647
12820,-Metrics/Training(Step): loss,1610360122518,0.12666724622249603
12822,-Metrics/Training(Step): loss,1610360126815,0.1048951968550682
12824,-Metrics/Training(Step): loss,1610360130916,0.150396466255188
12826,-Metrics/Training(Step): loss,1610360134623,0.12793096899986267
12828,-Metrics/Training(Step): loss,1610360138528,0.08281049132347107
12830,-Metrics/Training(Step): loss,1610360141931,0.14243516325950623
12832,-Metrics/Training(Step): loss,1610360145285,0.16610419750213623
12834,-Metrics/Training(Step): loss,1610360148850,0.14497338235378265
12836,-Metrics/Training(Step): loss,1610360152336,0.09899634122848511
12838,-Metrics/Training(Step): loss,1610360156423,0.11037176102399826
12840,-Metrics/Training(Step): loss,1610360159810,0.13245154917240143
12842,-Metrics/Training(Step): loss,1610360163415,0.0948539450764656
12844,-Metrics/Training(Step): loss,1610360167348,0.11053439974784851
12846,-Metrics/Training(Step): loss,1610360170720,0.11394386738538742
12848,-Metrics/Training(Step): loss,1610360174421,0.1127389445900917
12850,-Metrics/Training(Step): loss,1610360178327,0.11280283331871033
12852,-Metrics/Training(Step): loss,1610360181598,0.08013628423213959
12854,-Metrics/Training(Step): loss,1610360184615,0.11825300008058548
12856,-Metrics/Training(Step): loss,1610360188525,0.12814226746559143
12858,-Metrics/Training(Step): loss,1610360192025,0.14243876934051514
12860,-Metrics/Training(Step): loss,1610360196342,0.13703955709934235
12862,-Metrics/Training(Step): loss,1610360199445,0.11420170962810516
12864,-Metrics/Training(Step): loss,1610360201948,0.101251021027565
12866,-Metrics/Training(Step): loss,1610360204007,0.1233285665512085
12868,-Metrics/Training(Step): loss,1610360206101,0.12128734588623047
12870,-Metrics/Training(Step): loss,1610360208272,0.10533019155263901
12872,-Metrics/Training(Step): loss,1610360210465,0.11197144538164139
12874,-Metrics/Training(Step): loss,1610360212612,0.09102493524551392
12876,-Metrics/Training(Step): loss,1610360214737,0.11732645332813263
12878,-Metrics/Training(Step): loss,1610360216859,0.12008289992809296
12880,-Metrics/Training(Step): loss,1610360218926,0.14131388068199158
12882,-Metrics/Training(Step): loss,1610360220999,0.1384935826063156
12884,-Metrics/Training(Step): loss,1610360223046,0.12451168894767761
12886,-Metrics/Training(Step): loss,1610360224991,0.11381031572818756
12888,-Metrics/Training(Step): loss,1610360227115,0.1103573888540268
12890,-Metrics/Training(Step): loss,1610360229143,0.10927172750234604
12892,-Metrics/Training(Step): loss,1610360231047,0.10843128710985184
12894,-Metrics/Training(Step): loss,1610360233073,0.08909251540899277
12896,-Metrics/Training(Step): loss,1610360235120,0.10435862094163895
12898,-Metrics/Training(Step): loss,1610360237201,0.14471159875392914
12900,-Metrics/Training(Step): loss,1610360239263,0.144180029630661
12902,-Metrics/Training(Step): loss,1610360263831,0.11144890636205673
12904,-Metrics/Training(Step): loss,1610360267719,0.10624783486127853
12906,-Metrics/Training(Step): loss,1610360271224,0.12803615629673004
12908,-Metrics/Training(Step): loss,1610360275417,0.11162548512220383
12910,-Metrics/Training(Step): loss,1610360279427,0.12493709474802017
12912,-Metrics/Training(Step): loss,1610360283226,0.0994279682636261
12914,-Metrics/Training(Step): loss,1610360287119,0.1021614745259285
12916,-Metrics/Training(Step): loss,1610360290524,0.12674735486507416
12918,-Metrics/Training(Step): loss,1610360294920,0.12965892255306244
12920,-Metrics/Training(Step): loss,1610360298821,0.10265042632818222
12922,-Metrics/Training(Step): loss,1610360302520,0.15550220012664795
12924,-Metrics/Training(Step): loss,1610360306219,0.12039363384246826
12926,-Metrics/Training(Step): loss,1610360309622,0.10816019773483276
12928,-Metrics/Training(Step): loss,1610360313446,0.1334143429994583
12930,-Metrics/Training(Step): loss,1610360316811,0.08694832026958466
12932,-Metrics/Training(Step): loss,1610360320520,0.1171034500002861
12934,-Metrics/Training(Step): loss,1610360324459,0.1070883721113205
12936,-Metrics/Training(Step): loss,1610360328018,0.13114029169082642
12938,-Metrics/Training(Step): loss,1610360331421,0.0921870693564415
12940,-Metrics/Training(Step): loss,1610360335026,0.11613179743289948
12942,-Metrics/Training(Step): loss,1610360338646,0.11950954049825668
12944,-Metrics/Training(Step): loss,1610360342423,0.10639644414186478
12946,-Metrics/Training(Step): loss,1610360345820,0.10249701142311096
12948,-Metrics/Training(Step): loss,1610360348748,0.10307003557682037
12950,-Metrics/Training(Step): loss,1610360350994,0.11326579749584198
12952,-Metrics/Training(Step): loss,1610360353186,0.08635153621435165
12954,-Metrics/Training(Step): loss,1610360355306,0.12118697911500931
12956,-Metrics/Training(Step): loss,1610360357399,0.10883598774671555
12958,-Metrics/Training(Step): loss,1610360359561,0.11453830450773239
12960,-Metrics/Training(Step): loss,1610360361741,0.13158948719501495
12962,-Metrics/Training(Step): loss,1610360363914,0.08806432038545609
12964,-Metrics/Training(Step): loss,1610360365921,0.10559716820716858
12966,-Metrics/Training(Step): loss,1610360367925,0.12435255944728851
12968,-Metrics/Training(Step): loss,1610360369950,0.0988423302769661
12970,-Metrics/Training(Step): loss,1610360372092,0.12927818298339844
12972,-Metrics/Training(Step): loss,1610360374219,0.10585355013608932
12974,-Metrics/Training(Step): loss,1610360376320,0.12318243831396103
12976,-Metrics/Training(Step): loss,1610360378148,0.12916846573352814
12978,-Metrics/Training(Step): loss,1610360380185,0.13067573308944702
12980,-Metrics/Training(Step): loss,1610360382191,0.12953397631645203
12982,-Metrics/Training(Step): loss,1610360384219,0.09999340027570724
12984,-Metrics/Training(Step): loss,1610360386286,0.13378459215164185
12986,-Metrics/Training(Step): loss,1610360388344,0.11937171965837479
12988,-Metrics/Training(Step): loss,1610360401030,0.10294165462255478
12990,-Metrics/Training(Step): loss,1610360405130,0.11412360519170761
12992,-Metrics/Training(Step): loss,1610360408958,0.15469752252101898
12994,-Metrics/Training(Step): loss,1610360413227,0.13469946384429932
12996,-Metrics/Training(Step): loss,1610360416933,0.10854553431272507
12998,-Metrics/Training(Step): loss,1610360421328,0.1252908557653427
13000,-Metrics/Training(Step): loss,1610360425623,0.10374843329191208
13002,-Metrics/Training(Step): loss,1610360429236,0.13719189167022705
13004,-Metrics/Training(Step): loss,1610360433022,0.11442018300294876
13006,-Metrics/Training(Step): loss,1610360435921,0.10294698178768158
13008,-Metrics/Training(Step): loss,1610360439621,0.131441131234169
13010,-Metrics/Training(Step): loss,1610360443132,0.09385426342487335
13012,-Metrics/Training(Step): loss,1610360446506,0.12839041650295258
13014,-Metrics/Training(Step): loss,1610360450092,0.13697531819343567
13016,-Metrics/Training(Step): loss,1610360453887,0.09322406351566315
13018,-Metrics/Training(Step): loss,1610360457134,0.10940223932266235
13020,-Metrics/Training(Step): loss,1610360460725,0.13174577057361603
13022,-Metrics/Training(Step): loss,1610360464019,0.11890757083892822
13024,-Metrics/Training(Step): loss,1610360467445,0.1116967499256134
13026,-Metrics/Training(Step): loss,1610360470824,0.1391790509223938
13028,-Metrics/Training(Step): loss,1610360474320,0.12160345166921616
13030,-Metrics/Training(Step): loss,1610360477820,0.14716173708438873
13032,-Metrics/Training(Step): loss,1610360481579,0.11155962944030762
13034,-Metrics/Training(Step): loss,1610360484765,0.10877246409654617
13036,-Metrics/Training(Step): loss,1610360487319,0.1055183932185173
13038,-Metrics/Training(Step): loss,1610360489242,0.11121654510498047
13040,-Metrics/Training(Step): loss,1610360491336,0.148080512881279
13042,-Metrics/Training(Step): loss,1610360493544,0.09177488088607788
13044,-Metrics/Training(Step): loss,1610360495636,0.09430378675460815
13046,-Metrics/Training(Step): loss,1610360497761,0.10065388679504395
13048,-Metrics/Training(Step): loss,1610360499854,0.08930008858442307
13050,-Metrics/Training(Step): loss,1610360501731,0.14862793684005737
13052,-Metrics/Training(Step): loss,1610360503805,0.1027764305472374
13054,-Metrics/Training(Step): loss,1610360505719,0.09424678981304169
13056,-Metrics/Training(Step): loss,1610360507657,0.10978590697050095
13058,-Metrics/Training(Step): loss,1610360509745,0.1340762972831726
13060,-Metrics/Training(Step): loss,1610360511693,0.12190313637256622
13062,-Metrics/Training(Step): loss,1610360513578,0.11271696537733078
13064,-Metrics/Training(Step): loss,1610360515562,0.12122931331396103
13066,-Metrics/Training(Step): loss,1610360517621,0.11758170276880264
13068,-Metrics/Training(Step): loss,1610360519699,0.13129280507564545
13070,-Metrics/Training(Step): loss,1610360521708,0.1468181014060974
13072,-Metrics/Training(Step): loss,1610360523760,0.09601560980081558
13074,-Metrics/Training(Step): loss,1610360537130,0.11969860643148422
13076,-Metrics/Training(Step): loss,1610360541120,0.14225995540618896
13078,-Metrics/Training(Step): loss,1610360545019,0.1381310373544693
13080,-Metrics/Training(Step): loss,1610360549055,0.11825248599052429
13082,-Metrics/Training(Step): loss,1610360553020,0.09290529787540436
13084,-Metrics/Training(Step): loss,1610360557333,0.11864887923002243
13086,-Metrics/Training(Step): loss,1610360561323,0.08537796884775162
13088,-Metrics/Training(Step): loss,1610360565220,0.11791441589593887
13090,-Metrics/Training(Step): loss,1610360569125,0.09578952193260193
13092,-Metrics/Training(Step): loss,1610360572720,0.09069712460041046
13094,-Metrics/Training(Step): loss,1610360576122,0.11028792709112167
13096,-Metrics/Training(Step): loss,1610360579748,0.12550392746925354
13098,-Metrics/Training(Step): loss,1610360583645,0.09796815365552902
13100,-Metrics/Training(Step): loss,1610360587292,0.11686066538095474
13102,-Metrics/Training(Step): loss,1610360591419,0.11434659361839294
13104,-Metrics/Training(Step): loss,1610360594858,0.10403623431921005
13106,-Metrics/Training(Step): loss,1610360598262,0.11458225548267365
13108,-Metrics/Training(Step): loss,1610360601850,0.09427588433027267
13110,-Metrics/Training(Step): loss,1610360605477,0.1155320554971695
13112,-Metrics/Training(Step): loss,1610360609019,0.1081819012761116
13114,-Metrics/Training(Step): loss,1610360612636,0.09102936834096909
13116,-Metrics/Training(Step): loss,1610360615846,0.11522436887025833
13118,-Metrics/Training(Step): loss,1610360618954,0.14351317286491394
13120,-Metrics/Training(Step): loss,1610360621680,0.13207724690437317
13122,-Metrics/Training(Step): loss,1610360624068,0.10436788201332092
13124,-Metrics/Training(Step): loss,1610360626283,0.12340715527534485
13126,-Metrics/Training(Step): loss,1610360628411,0.14184467494487762
13128,-Metrics/Training(Step): loss,1610360630481,0.10201180726289749
13130,-Metrics/Training(Step): loss,1610360632590,0.10037395358085632
13132,-Metrics/Training(Step): loss,1610360634629,0.11267997324466705
13134,-Metrics/Training(Step): loss,1610360636684,0.12201110273599625
13136,-Metrics/Training(Step): loss,1610360638746,0.12854158878326416
13138,-Metrics/Training(Step): loss,1610360640850,0.12962347269058228
13140,-Metrics/Training(Step): loss,1610360642987,0.09148665517568588
13142,-Metrics/Training(Step): loss,1610360645037,0.10619325935840607
13144,-Metrics/Training(Step): loss,1610360647148,0.12581570446491241
13146,-Metrics/Training(Step): loss,1610360649133,0.11289449781179428
13148,-Metrics/Training(Step): loss,1610360651210,0.109749935567379
13150,-Metrics/Training(Step): loss,1610360653275,0.08833254128694534
13152,-Metrics/Training(Step): loss,1610360655343,0.11958883702754974
13154,-Metrics/Training(Step): loss,1610360657326,0.12418198585510254
13156,-Metrics/Training(Step): loss,1610360659412,0.10026440024375916
13158,-Metrics/Training(Step): loss,1610360661454,0.10505443066358566
13160,-Metrics/Training(Step): loss,1610360674724,0.10051585733890533
13162,-Metrics/Training(Step): loss,1610360678755,0.13575144112110138
13164,-Metrics/Training(Step): loss,1610360682631,0.13018450140953064
13166,-Metrics/Training(Step): loss,1610360686547,0.11401079595088959
13168,-Metrics/Training(Step): loss,1610360690815,0.11369480937719345
13170,-Metrics/Training(Step): loss,1610360695119,0.13854840397834778
13172,-Metrics/Training(Step): loss,1610360699542,0.10542798042297363
13174,-Metrics/Training(Step): loss,1610360703424,0.10224747657775879
13176,-Metrics/Training(Step): loss,1610360707221,0.11932703107595444
13178,-Metrics/Training(Step): loss,1610360710622,0.11580438911914825
13180,-Metrics/Training(Step): loss,1610360713978,0.12262524664402008
13182,-Metrics/Training(Step): loss,1610360717278,0.13539919257164001
13184,-Metrics/Training(Step): loss,1610360721420,0.11703892797231674
13186,-Metrics/Training(Step): loss,1610360725221,0.1231570690870285
13188,-Metrics/Training(Step): loss,1610360728640,0.11561021208763123
13190,-Metrics/Training(Step): loss,1610360732155,0.11247663199901581
13192,-Metrics/Training(Step): loss,1610360735542,0.10525988787412643
13194,-Metrics/Training(Step): loss,1610360739383,0.13416936993598938
13196,-Metrics/Training(Step): loss,1610360743020,0.10874723643064499
13198,-Metrics/Training(Step): loss,1610360746520,0.11469872295856476
13200,-Metrics/Training(Step): loss,1610360750121,0.1371992528438568
13202,-Metrics/Training(Step): loss,1610360753417,0.1264573484659195
13204,-Metrics/Training(Step): loss,1610360756554,0.11321984231472015
13206,-Metrics/Training(Step): loss,1610360759224,0.12702655792236328
13208,-Metrics/Training(Step): loss,1610360761736,0.1136140376329422
13210,-Metrics/Training(Step): loss,1610360763944,0.12410183250904083
13212,-Metrics/Training(Step): loss,1610360766086,0.1348458081483841
13214,-Metrics/Training(Step): loss,1610360768222,0.1415603756904602
13216,-Metrics/Training(Step): loss,1610360770158,0.09364882856607437
13218,-Metrics/Training(Step): loss,1610360772240,0.1097254529595375
13220,-Metrics/Training(Step): loss,1610360774342,0.09536682814359665
13222,-Metrics/Training(Step): loss,1610360776368,0.10489483177661896
13224,-Metrics/Training(Step): loss,1610360778386,0.10686533898115158
13226,-Metrics/Training(Step): loss,1610360780470,0.10793446004390717
13228,-Metrics/Training(Step): loss,1610360782599,0.09540428966283798
13230,-Metrics/Training(Step): loss,1610360784615,0.126563161611557
13232,-Metrics/Training(Step): loss,1610360786552,0.10318639129400253
13234,-Metrics/Training(Step): loss,1610360788700,0.11144447326660156
13236,-Metrics/Training(Step): loss,1610360790796,0.10988019406795502
13238,-Metrics/Training(Step): loss,1610360792852,0.07504411041736603
13240,-Metrics/Training(Step): loss,1610360794942,0.1328599601984024
13242,-Metrics/Training(Step): loss,1610360797048,0.1039261668920517
13244,-Metrics/Training(Step): loss,1610360798996,0.11280494928359985
13246,-Metrics/Training(Step): loss,1610360813434,0.12236856669187546
13248,-Metrics/Training(Step): loss,1610360817442,0.11643766611814499
13250,-Metrics/Training(Step): loss,1610360821519,0.13302288949489594
13252,-Metrics/Training(Step): loss,1610360825420,0.09891071170568466
13254,-Metrics/Training(Step): loss,1610360829319,0.08521398156881332
13256,-Metrics/Training(Step): loss,1610360833325,0.12477555871009827
13258,-Metrics/Training(Step): loss,1610360837256,0.09580338001251221
13260,-Metrics/Training(Step): loss,1610360840908,0.06818768382072449
13262,-Metrics/Training(Step): loss,1610360844516,0.09616918861865997
13264,-Metrics/Training(Step): loss,1610360848230,0.1061587929725647
13266,-Metrics/Training(Step): loss,1610360851805,0.11354987323284149
13268,-Metrics/Training(Step): loss,1610360855323,0.13261766731739044
13270,-Metrics/Training(Step): loss,1610360858720,0.12043128162622452
13272,-Metrics/Training(Step): loss,1610360862219,0.10278766602277756
13274,-Metrics/Training(Step): loss,1610360865619,0.12667216360569
13276,-Metrics/Training(Step): loss,1610360869650,0.1044500321149826
13278,-Metrics/Training(Step): loss,1610360873183,0.10280651599168777
13280,-Metrics/Training(Step): loss,1610360876920,0.13038498163223267
13282,-Metrics/Training(Step): loss,1610360880633,0.13017266988754272
13284,-Metrics/Training(Step): loss,1610360883619,0.10938498377799988
13286,-Metrics/Training(Step): loss,1610360887021,0.15637820959091187
13288,-Metrics/Training(Step): loss,1610360890619,0.13602623343467712
13290,-Metrics/Training(Step): loss,1610360894420,0.13546906411647797
13292,-Metrics/Training(Step): loss,1610360897366,0.10985654592514038
13294,-Metrics/Training(Step): loss,1610360900046,0.08896970003843307
13296,-Metrics/Training(Step): loss,1610360902416,0.13456566631793976
13298,-Metrics/Training(Step): loss,1610360904439,0.10333672910928726
13300,-Metrics/Training(Step): loss,1610360906447,0.12704961001873016
13302,-Metrics/Training(Step): loss,1610360908571,0.15569497644901276
13304,-Metrics/Training(Step): loss,1610360910725,0.12793944776058197
13306,-Metrics/Training(Step): loss,1610360912764,0.12437183409929276
13308,-Metrics/Training(Step): loss,1610360914766,0.12062833458185196
13310,-Metrics/Training(Step): loss,1610360916865,0.12186892330646515
13312,-Metrics/Training(Step): loss,1610360918995,0.11384850740432739
13314,-Metrics/Training(Step): loss,1610360920982,0.0959150418639183
13316,-Metrics/Training(Step): loss,1610360923072,0.1179049015045166
13318,-Metrics/Training(Step): loss,1610360925108,0.10836630314588547
13320,-Metrics/Training(Step): loss,1610360927277,0.08688554912805557
13322,-Metrics/Training(Step): loss,1610360929337,0.11887627840042114
13324,-Metrics/Training(Step): loss,1610360931397,0.10202007740736008
13326,-Metrics/Training(Step): loss,1610360933367,0.09918095171451569
13328,-Metrics/Training(Step): loss,1610360935426,0.10190390795469284
13330,-Metrics/Training(Step): loss,1610360937551,0.08199432492256165
13332,-Metrics/Training(Step): loss,1610360950236,0.13984446227550507
13334,-Metrics/Training(Step): loss,1610360954320,0.09381286799907684
13336,-Metrics/Training(Step): loss,1610360958426,0.11863420903682709
13338,-Metrics/Training(Step): loss,1610360962630,0.1356564611196518
13340,-Metrics/Training(Step): loss,1610360966520,0.10321205854415894
13342,-Metrics/Training(Step): loss,1610360970648,0.1081913486123085
13344,-Metrics/Training(Step): loss,1610360974719,0.08805615454912186
13346,-Metrics/Training(Step): loss,1610360978322,0.10637567937374115
13348,-Metrics/Training(Step): loss,1610360981821,0.09325902163982391
13350,-Metrics/Training(Step): loss,1610360985720,0.11099845170974731
13352,-Metrics/Training(Step): loss,1610360988823,0.07253281027078629
13354,-Metrics/Training(Step): loss,1610360992320,0.10237856209278107
13356,-Metrics/Training(Step): loss,1610360996123,0.10394766926765442
13358,-Metrics/Training(Step): loss,1610360999820,0.11661437898874283
13360,-Metrics/Training(Step): loss,1610361003319,0.11590339988470078
13362,-Metrics/Training(Step): loss,1610361006843,0.09693486243486404
13364,-Metrics/Training(Step): loss,1610361010320,0.12342878431081772
13366,-Metrics/Training(Step): loss,1610361013921,0.1249169334769249
13368,-Metrics/Training(Step): loss,1610361017220,0.11162184923887253
13370,-Metrics/Training(Step): loss,1610361020637,0.11536811292171478
13372,-Metrics/Training(Step): loss,1610361024023,0.11328210681676865
13374,-Metrics/Training(Step): loss,1610361028039,0.1266237497329712
13376,-Metrics/Training(Step): loss,1610361031019,0.13110849261283875
13378,-Metrics/Training(Step): loss,1610361034315,0.10971014201641083
13380,-Metrics/Training(Step): loss,1610361036702,0.1649206280708313
13382,-Metrics/Training(Step): loss,1610361038859,0.08925467729568481
13384,-Metrics/Training(Step): loss,1610361040978,0.10673902928829193
13386,-Metrics/Training(Step): loss,1610361043095,0.10761075466871262
13388,-Metrics/Training(Step): loss,1610361045178,0.1307539939880371
13390,-Metrics/Training(Step): loss,1610361047195,0.08819948136806488
13392,-Metrics/Training(Step): loss,1610361049289,0.12488404661417007
13394,-Metrics/Training(Step): loss,1610361051114,0.09326913952827454
13396,-Metrics/Training(Step): loss,1610361053084,0.09667151421308517
13398,-Metrics/Training(Step): loss,1610361055123,0.09607473015785217
13400,-Metrics/Training(Step): loss,1610361057101,0.11861966550350189
13402,-Metrics/Training(Step): loss,1610361059124,0.09803695976734161
13404,-Metrics/Training(Step): loss,1610361061143,0.1342829018831253
13406,-Metrics/Training(Step): loss,1610361063193,0.09267494827508926
13408,-Metrics/Training(Step): loss,1610361065219,0.1028154045343399
13410,-Metrics/Training(Step): loss,1610361067266,0.08614512532949448
13412,-Metrics/Training(Step): loss,1610361069338,0.08946021646261215
13414,-Metrics/Training(Step): loss,1610361071468,0.1261298656463623
13416,-Metrics/Training(Step): loss,1610361073499,0.07512401044368744
13418,-Metrics/Training(Step): loss,1610361086628,0.12513668835163116
13420,-Metrics/Training(Step): loss,1610361090626,0.12664757668972015
13422,-Metrics/Training(Step): loss,1610361094720,0.15757328271865845
13424,-Metrics/Training(Step): loss,1610361098719,0.09756717085838318
13426,-Metrics/Training(Step): loss,1610361102515,0.12066848576068878
13428,-Metrics/Training(Step): loss,1610361106745,0.11534002423286438
13430,-Metrics/Training(Step): loss,1610361110720,0.13788707554340363
13432,-Metrics/Training(Step): loss,1610361114720,0.10127761214971542
13434,-Metrics/Training(Step): loss,1610361117754,0.10663080960512161
13436,-Metrics/Training(Step): loss,1610361121625,0.0962686687707901
13438,-Metrics/Training(Step): loss,1610361125128,0.09841625392436981
13440,-Metrics/Training(Step): loss,1610361128619,0.14240041375160217
13442,-Metrics/Training(Step): loss,1610361132456,0.1356550008058548
13444,-Metrics/Training(Step): loss,1610361136220,0.11352037638425827
13446,-Metrics/Training(Step): loss,1610361139819,0.12693551182746887
13448,-Metrics/Training(Step): loss,1610361143121,0.12084795534610748
13450,-Metrics/Training(Step): loss,1610361146719,0.12141324579715729
13452,-Metrics/Training(Step): loss,1610361150383,0.13512544333934784
13454,-Metrics/Training(Step): loss,1610361154033,0.0991814136505127
13456,-Metrics/Training(Step): loss,1610361157684,0.0730859637260437
13458,-Metrics/Training(Step): loss,1610361161269,0.10015198588371277
13460,-Metrics/Training(Step): loss,1610361164854,0.09651511907577515
13462,-Metrics/Training(Step): loss,1610361168744,0.09214074909687042
13464,-Metrics/Training(Step): loss,1610361171614,0.08376758545637131
13466,-Metrics/Training(Step): loss,1610361174046,0.07470237463712692
13468,-Metrics/Training(Step): loss,1610361176170,0.11492657661437988
13470,-Metrics/Training(Step): loss,1610361178312,0.13031893968582153
13472,-Metrics/Training(Step): loss,1610361180388,0.11817515641450882
13474,-Metrics/Training(Step): loss,1610361182507,0.11087626218795776
13476,-Metrics/Training(Step): loss,1610361184560,0.10739107429981232
13478,-Metrics/Training(Step): loss,1610361186600,0.11058808863162994
13480,-Metrics/Training(Step): loss,1610361188635,0.10280195623636246
13482,-Metrics/Training(Step): loss,1610361190589,0.1346828043460846
13484,-Metrics/Training(Step): loss,1610361192409,0.10994267463684082
13486,-Metrics/Training(Step): loss,1610361194358,0.10766460001468658
13488,-Metrics/Training(Step): loss,1610361196464,0.09144028276205063
13490,-Metrics/Training(Step): loss,1610361198547,0.1084405779838562
13492,-Metrics/Training(Step): loss,1610361200603,0.11639583110809326
13494,-Metrics/Training(Step): loss,1610361202599,0.10313531756401062
13496,-Metrics/Training(Step): loss,1610361204566,0.10127586126327515
13498,-Metrics/Training(Step): loss,1610361206645,0.12046685069799423
13500,-Metrics/Training(Step): loss,1610361208683,0.11092416197061539
13502,-Metrics/Training(Step): loss,1610361210645,0.09144113957881927
13504,-Metrics/Training(Step): loss,1610361223926,0.14955942332744598
13506,-Metrics/Training(Step): loss,1610361227935,0.11075643450021744
13508,-Metrics/Training(Step): loss,1610361232250,0.10050805658102036
13510,-Metrics/Training(Step): loss,1610361236721,0.11668634414672852
13512,-Metrics/Training(Step): loss,1610361240516,0.0795486718416214
13514,-Metrics/Training(Step): loss,1610361244519,0.18056190013885498
13516,-Metrics/Training(Step): loss,1610361248720,0.1085088774561882
13518,-Metrics/Training(Step): loss,1610361252619,0.14084364473819733
13520,-Metrics/Training(Step): loss,1610361256125,0.13889148831367493
13522,-Metrics/Training(Step): loss,1610361259920,0.09334330260753632
13524,-Metrics/Training(Step): loss,1610361263519,0.13477511703968048
13526,-Metrics/Training(Step): loss,1610361267721,0.11519687622785568
13528,-Metrics/Training(Step): loss,1610361272045,0.13294850289821625
13530,-Metrics/Training(Step): loss,1610361275754,0.12287405878305435
13532,-Metrics/Training(Step): loss,1610361279719,0.16188916563987732
13534,-Metrics/Training(Step): loss,1610361282850,0.10072768479585648
13536,-Metrics/Training(Step): loss,1610361286660,0.0997658371925354
13538,-Metrics/Training(Step): loss,1610361290176,0.1121189072728157
13540,-Metrics/Training(Step): loss,1610361293518,0.12074741721153259
13542,-Metrics/Training(Step): loss,1610361296657,0.09048663824796677
13544,-Metrics/Training(Step): loss,1610361300521,0.09433639049530029
13546,-Metrics/Training(Step): loss,1610361304279,0.10876909643411636
13548,-Metrics/Training(Step): loss,1610361307726,0.10026617348194122
13550,-Metrics/Training(Step): loss,1610361310015,0.08535385131835938
13552,-Metrics/Training(Step): loss,1610361312099,0.12238456308841705
13554,-Metrics/Training(Step): loss,1610361314311,0.11172842979431152
13556,-Metrics/Training(Step): loss,1610361316328,0.11583252996206284
13558,-Metrics/Training(Step): loss,1610361318320,0.09528712928295135
13560,-Metrics/Training(Step): loss,1610361320393,0.11903093010187149
13562,-Metrics/Training(Step): loss,1610361322497,0.10833185911178589
13564,-Metrics/Training(Step): loss,1610361324586,0.11030737310647964
13566,-Metrics/Training(Step): loss,1610361326671,0.11671645194292068
13568,-Metrics/Training(Step): loss,1610361328688,0.13124457001686096
13570,-Metrics/Training(Step): loss,1610361330727,0.11777187138795853
13572,-Metrics/Training(Step): loss,1610361332796,0.0881064385175705
13574,-Metrics/Training(Step): loss,1610361334841,0.12183643877506256
13576,-Metrics/Training(Step): loss,1610361336735,0.11026688665151596
13578,-Metrics/Training(Step): loss,1610361338793,0.11292339116334915
13580,-Metrics/Training(Step): loss,1610361340807,0.08238570392131805
13582,-Metrics/Training(Step): loss,1610361342922,0.11136209219694138
13584,-Metrics/Training(Step): loss,1610361345034,0.0840149074792862
13586,-Metrics/Training(Step): loss,1610361347097,0.09595584869384766
13588,-Metrics/Training(Step): loss,1610361349120,0.10297166556119919
13590,-Metrics/Training(Step): loss,1610361362026,0.10672193765640259
13592,-Metrics/Training(Step): loss,1610361366120,0.09350163489580154
13594,-Metrics/Training(Step): loss,1610361370121,0.12126266956329346
13596,-Metrics/Training(Step): loss,1610361373958,0.1074894443154335
13598,-Metrics/Training(Step): loss,1610361377645,0.12337406724691391
13600,-Metrics/Training(Step): loss,1610361382048,0.08726470917463303
13602,-Metrics/Training(Step): loss,1610361386036,0.08796248584985733
13604,-Metrics/Training(Step): loss,1610361389804,0.10245359688997269
13606,-Metrics/Training(Step): loss,1610361393649,0.1086115688085556
13608,-Metrics/Training(Step): loss,1610361397635,0.10856013745069504
13610,-Metrics/Training(Step): loss,1610361401542,0.10020900517702103
13612,-Metrics/Training(Step): loss,1610361405419,0.11974217742681503
13614,-Metrics/Training(Step): loss,1610361408819,0.07667411118745804
13616,-Metrics/Training(Step): loss,1610361412613,0.1469300389289856
13618,-Metrics/Training(Step): loss,1610361416131,0.11744268983602524
13620,-Metrics/Training(Step): loss,1610361419345,0.08617469668388367
13622,-Metrics/Training(Step): loss,1610361423120,0.09241343289613724
13624,-Metrics/Training(Step): loss,1610361426953,0.11387306451797485
13626,-Metrics/Training(Step): loss,1610361430291,0.0769553929567337
13628,-Metrics/Training(Step): loss,1610361434219,0.11027658730745316
13630,-Metrics/Training(Step): loss,1610361438265,0.11157319694757462
13632,-Metrics/Training(Step): loss,1610361441605,0.10486306250095367
13634,-Metrics/Training(Step): loss,1610361444789,0.130923792719841
13636,-Metrics/Training(Step): loss,1610361447424,0.10867923498153687
13638,-Metrics/Training(Step): loss,1610361449714,0.11287104338407516
13640,-Metrics/Training(Step): loss,1610361451830,0.10870490968227386
13642,-Metrics/Training(Step): loss,1610361453889,0.10631760954856873
13644,-Metrics/Training(Step): loss,1610361455944,0.10158073157072067
13646,-Metrics/Training(Step): loss,1610361457849,0.13280653953552246
13648,-Metrics/Training(Step): loss,1610361459983,0.11697831004858017
13650,-Metrics/Training(Step): loss,1610361462054,0.10492410510778427
13652,-Metrics/Training(Step): loss,1610361464042,0.13085588812828064
13654,-Metrics/Training(Step): loss,1610361466137,0.1320701390504837
13656,-Metrics/Training(Step): loss,1610361468230,0.09091607481241226
13658,-Metrics/Training(Step): loss,1610361470333,0.09494854509830475
13660,-Metrics/Training(Step): loss,1610361472427,0.09878363460302353
13662,-Metrics/Training(Step): loss,1610361474433,0.10238805413246155
13664,-Metrics/Training(Step): loss,1610361476469,0.11043275147676468
13666,-Metrics/Training(Step): loss,1610361478411,0.12080859392881393
13668,-Metrics/Training(Step): loss,1610361480498,0.11495324969291687
13670,-Metrics/Training(Step): loss,1610361482625,0.12151853740215302
13672,-Metrics/Training(Step): loss,1610361484669,0.09460006654262543
13674,-Metrics/Training(Step): loss,1610361486730,0.10135983675718307
13676,-Metrics/Training(Step): loss,1610361498834,0.08404725044965744
13678,-Metrics/Training(Step): loss,1610361503231,0.10511847585439682
13680,-Metrics/Training(Step): loss,1610361507432,0.09134349972009659
13682,-Metrics/Training(Step): loss,1610361511620,0.10791818052530289
13684,-Metrics/Training(Step): loss,1610361515437,0.06610914319753647
13686,-Metrics/Training(Step): loss,1610361519220,0.10104825347661972
13688,-Metrics/Training(Step): loss,1610361523322,0.11668933182954788
13690,-Metrics/Training(Step): loss,1610361527019,0.11857365071773529
13692,-Metrics/Training(Step): loss,1610361530546,0.09933453053236008
13694,-Metrics/Training(Step): loss,1610361533948,0.08548050373792648
13696,-Metrics/Training(Step): loss,1610361537425,0.09728904813528061
13698,-Metrics/Training(Step): loss,1610361540720,0.08989179879426956
13700,-Metrics/Training(Step): loss,1610361544348,0.1259671300649643
13702,-Metrics/Training(Step): loss,1610361548357,0.06259380280971527
13704,-Metrics/Training(Step): loss,1610361551889,0.08807934820652008
13706,-Metrics/Training(Step): loss,1610361555548,0.08934234082698822
13708,-Metrics/Training(Step): loss,1610361559054,0.09286174923181534
13710,-Metrics/Training(Step): loss,1610361562600,0.09834971278905869
13712,-Metrics/Training(Step): loss,1610361566420,0.11870025098323822
13714,-Metrics/Training(Step): loss,1610361570148,0.12296940386295319
13716,-Metrics/Training(Step): loss,1610361574301,0.0940518006682396
13718,-Metrics/Training(Step): loss,1610361577920,0.11142875254154205
13720,-Metrics/Training(Step): loss,1610361580919,0.12188106775283813
13722,-Metrics/Training(Step): loss,1610361583681,0.11975481361150742
13724,-Metrics/Training(Step): loss,1610361586218,0.1352868527173996
13726,-Metrics/Training(Step): loss,1610361588265,0.12428741902112961
13728,-Metrics/Training(Step): loss,1610361590316,0.12367112934589386
13730,-Metrics/Training(Step): loss,1610361592344,0.12331846356391907
13732,-Metrics/Training(Step): loss,1610361594387,0.0976981595158577
13734,-Metrics/Training(Step): loss,1610361596528,0.1167982667684555
13736,-Metrics/Training(Step): loss,1610361598650,0.07819867134094238
13738,-Metrics/Training(Step): loss,1610361600723,0.1183256283402443
13740,-Metrics/Training(Step): loss,1610361602851,0.09622398018836975
13742,-Metrics/Training(Step): loss,1610361604986,0.11406293511390686
13744,-Metrics/Training(Step): loss,1610361606929,0.119703009724617
13746,-Metrics/Training(Step): loss,1610361608962,0.10573334991931915
13748,-Metrics/Training(Step): loss,1610361611035,0.0955069288611412
13750,-Metrics/Training(Step): loss,1610361613127,0.12519297003746033
13752,-Metrics/Training(Step): loss,1610361615221,0.10867536813020706
13754,-Metrics/Training(Step): loss,1610361617301,0.11848299950361252
13756,-Metrics/Training(Step): loss,1610361619337,0.1536945253610611
13758,-Metrics/Training(Step): loss,1610361621342,0.10637837648391724
13760,-Metrics/Training(Step): loss,1610361623388,0.11077062040567398
13762,-Metrics/Training(Step): loss,1610361637043,0.10010311752557755
13764,-Metrics/Training(Step): loss,1610361641517,0.1093263253569603
13766,-Metrics/Training(Step): loss,1610361645819,0.10655997693538666
13768,-Metrics/Training(Step): loss,1610361650025,0.09974826127290726
13770,-Metrics/Training(Step): loss,1610361654420,0.1107630804181099
13772,-Metrics/Training(Step): loss,1610361658715,0.13987423479557037
13774,-Metrics/Training(Step): loss,1610361663035,0.1264622062444687
13776,-Metrics/Training(Step): loss,1610361667113,0.13852159678936005
13778,-Metrics/Training(Step): loss,1610361671307,0.10908322781324387
13780,-Metrics/Training(Step): loss,1610361674828,0.08279658854007721
13782,-Metrics/Training(Step): loss,1610361678720,0.1082371175289154
13784,-Metrics/Training(Step): loss,1610361682657,0.12558940052986145
13786,-Metrics/Training(Step): loss,1610361686154,0.08810890465974808
13788,-Metrics/Training(Step): loss,1610361689541,0.10180589556694031
13790,-Metrics/Training(Step): loss,1610361693346,0.14299525320529938
13792,-Metrics/Training(Step): loss,1610361696700,0.12529723346233368
13794,-Metrics/Training(Step): loss,1610361700520,0.13652615249156952
13796,-Metrics/Training(Step): loss,1610361704021,0.12689583003520966
13798,-Metrics/Training(Step): loss,1610361707620,0.09707342088222504
13800,-Metrics/Training(Step): loss,1610361711219,0.1269274204969406
13802,-Metrics/Training(Step): loss,1610361714721,0.123057521879673
13804,-Metrics/Training(Step): loss,1610361718119,0.10732997208833694
13806,-Metrics/Training(Step): loss,1610361721117,0.1315116435289383
13808,-Metrics/Training(Step): loss,1610361723771,0.11763694137334824
13810,-Metrics/Training(Step): loss,1610361725948,0.11596237123012543
13812,-Metrics/Training(Step): loss,1610361728126,0.09647578746080399
13814,-Metrics/Training(Step): loss,1610361730213,0.12538422644138336
13816,-Metrics/Training(Step): loss,1610361732293,0.11619511991739273
13818,-Metrics/Training(Step): loss,1610361734388,0.08767661452293396
13820,-Metrics/Training(Step): loss,1610361736451,0.0942205861210823
13822,-Metrics/Training(Step): loss,1610361738553,0.09111122041940689
13824,-Metrics/Training(Step): loss,1610361740689,0.12139450758695602
13826,-Metrics/Training(Step): loss,1610361742621,0.11614293605089188
13828,-Metrics/Training(Step): loss,1610361744751,0.10474348068237305
13830,-Metrics/Training(Step): loss,1610361746853,0.09834352880716324
13832,-Metrics/Training(Step): loss,1610361748832,0.13225853443145752
13834,-Metrics/Training(Step): loss,1610361750842,0.18158161640167236
13836,-Metrics/Training(Step): loss,1610361752952,0.11162849515676498
13838,-Metrics/Training(Step): loss,1610361755029,0.10384370386600494
13840,-Metrics/Training(Step): loss,1610361757063,0.10205520689487457
13842,-Metrics/Training(Step): loss,1610361759115,0.09343093633651733
13844,-Metrics/Training(Step): loss,1610361761189,0.11161692440509796
13846,-Metrics/Training(Step): loss,1610361763242,0.10961659997701645
13848,-Metrics/Training(Step): loss,1610361776428,0.08772092312574387
13850,-Metrics/Training(Step): loss,1610361780624,0.11863706260919571
13852,-Metrics/Training(Step): loss,1610361784822,0.10061375051736832
13854,-Metrics/Training(Step): loss,1610361788921,0.10280461609363556
13856,-Metrics/Training(Step): loss,1610361792716,0.11570015549659729
13858,-Metrics/Training(Step): loss,1610361796920,0.1021876186132431
13860,-Metrics/Training(Step): loss,1610361800819,0.11161055415868759
13862,-Metrics/Training(Step): loss,1610361804036,0.1242440715432167
13864,-Metrics/Training(Step): loss,1610361807635,0.12334606051445007
13866,-Metrics/Training(Step): loss,1610361811569,0.09812873601913452
13868,-Metrics/Training(Step): loss,1610361815044,0.11479712277650833
13870,-Metrics/Training(Step): loss,1610361818591,0.09426409751176834
13872,-Metrics/Training(Step): loss,1610361822028,0.11792833358049393
13874,-Metrics/Training(Step): loss,1610361825593,0.10564673691987991
13876,-Metrics/Training(Step): loss,1610361829138,0.09117405861616135
13878,-Metrics/Training(Step): loss,1610361832588,0.10660000145435333
13880,-Metrics/Training(Step): loss,1610361835876,0.09737648069858551
13882,-Metrics/Training(Step): loss,1610361839532,0.10063102841377258
13884,-Metrics/Training(Step): loss,1610361844128,0.10469825565814972
13886,-Metrics/Training(Step): loss,1610361848542,0.11972876638174057
13888,-Metrics/Training(Step): loss,1610361852668,0.12448269873857498
13890,-Metrics/Training(Step): loss,1610361856001,0.08662738651037216
13892,-Metrics/Training(Step): loss,1610361859549,0.09622737020254135
13894,-Metrics/Training(Step): loss,1610361861968,0.1212240681052208
13896,-Metrics/Training(Step): loss,1610361864091,0.12738674879074097
13898,-Metrics/Training(Step): loss,1610361866191,0.11223764717578888
13900,-Metrics/Training(Step): loss,1610361868178,0.09245225787162781
13902,-Metrics/Training(Step): loss,1610361870262,0.11510825902223587
13904,-Metrics/Training(Step): loss,1610361872385,0.10324683785438538
13906,-Metrics/Training(Step): loss,1610361874492,0.08228202909231186
13908,-Metrics/Training(Step): loss,1610361876612,0.1107693761587143
13910,-Metrics/Training(Step): loss,1610361878651,0.08792446553707123
13912,-Metrics/Training(Step): loss,1610361880698,0.09507779777050018
13914,-Metrics/Training(Step): loss,1610361882811,0.10004071891307831
13916,-Metrics/Training(Step): loss,1610361884836,0.10528643429279327
13918,-Metrics/Training(Step): loss,1610361886948,0.10078050941228867
13920,-Metrics/Training(Step): loss,1610361888873,0.09877821058034897
13922,-Metrics/Training(Step): loss,1610361890924,0.12296121567487717
13924,-Metrics/Training(Step): loss,1610361892952,0.10903668403625488
13926,-Metrics/Training(Step): loss,1610361895044,0.11890874803066254
13928,-Metrics/Training(Step): loss,1610361897019,0.10547015070915222
13930,-Metrics/Training(Step): loss,1610361899099,0.11457509547472
13932,-Metrics/Training(Step): loss,1610361901131,0.1240396574139595
13934,-Metrics/Training(Step): loss,1610361913825,0.12210007011890411
13936,-Metrics/Training(Step): loss,1610361917740,0.1250796616077423
13938,-Metrics/Training(Step): loss,1610361921620,0.13592921197414398
13940,-Metrics/Training(Step): loss,1610361925436,0.10030434280633926
13942,-Metrics/Training(Step): loss,1610361929420,0.10291819274425507
13944,-Metrics/Training(Step): loss,1610361933220,0.08277766406536102
13946,-Metrics/Training(Step): loss,1610361936853,0.10606878250837326
13948,-Metrics/Training(Step): loss,1610361940744,0.09632521122694016
13950,-Metrics/Training(Step): loss,1610361944587,0.11961111426353455
13952,-Metrics/Training(Step): loss,1610361948203,0.08931756019592285
13954,-Metrics/Training(Step): loss,1610361951720,0.0992594063282013
13956,-Metrics/Training(Step): loss,1610361955419,0.10375132411718369
13958,-Metrics/Training(Step): loss,1610361959020,0.10582765191793442
13960,-Metrics/Training(Step): loss,1610361962720,0.11461330205202103
13962,-Metrics/Training(Step): loss,1610361966120,0.12120752781629562
13964,-Metrics/Training(Step): loss,1610361969485,0.09231124073266983
13966,-Metrics/Training(Step): loss,1610361973219,0.10761658102273941
13968,-Metrics/Training(Step): loss,1610361976258,0.06690819561481476
13970,-Metrics/Training(Step): loss,1610361979722,0.1367887407541275
13972,-Metrics/Training(Step): loss,1610361983429,0.1005018875002861
13974,-Metrics/Training(Step): loss,1610361986519,0.12521561980247498
13976,-Metrics/Training(Step): loss,1610361989919,0.09053553640842438
13978,-Metrics/Training(Step): loss,1610361992739,0.10323339700698853
13980,-Metrics/Training(Step): loss,1610361996279,0.09386680275201797
13982,-Metrics/Training(Step): loss,1610361999176,0.11208426207304001
13984,-Metrics/Training(Step): loss,1610362001608,0.10500355809926987
13986,-Metrics/Training(Step): loss,1610362003695,0.13406062126159668
13988,-Metrics/Training(Step): loss,1610362005791,0.10696075856685638
13990,-Metrics/Training(Step): loss,1610362007892,0.0786001980304718
13992,-Metrics/Training(Step): loss,1610362009903,0.1176057979464531
13994,-Metrics/Training(Step): loss,1610362011925,0.14195914566516876
13996,-Metrics/Training(Step): loss,1610362014001,0.10905725508928299
13998,-Metrics/Training(Step): loss,1610362016083,0.11186949163675308
14000,-Metrics/Training(Step): loss,1610362018100,0.11847864091396332
14002,-Metrics/Training(Step): loss,1610362020035,0.10088537633419037
14004,-Metrics/Training(Step): loss,1610362021918,0.10928929597139359
14006,-Metrics/Training(Step): loss,1610362023850,0.07666005194187164
14008,-Metrics/Training(Step): loss,1610362025906,0.12613627314567566
14010,-Metrics/Training(Step): loss,1610362027868,0.13145707547664642
14012,-Metrics/Training(Step): loss,1610362029789,0.11425132304430008
14014,-Metrics/Training(Step): loss,1610362031744,0.11547164618968964
14016,-Metrics/Training(Step): loss,1610362033830,0.08789622038602829
14018,-Metrics/Training(Step): loss,1610362035881,0.10511106252670288
14020,-Metrics/Training(Step): loss,1610362047749,0.08608528226613998
14022,-Metrics/Training(Step): loss,1610362052131,0.11808821558952332
14024,-Metrics/Training(Step): loss,1610362056125,0.12402406334877014
14026,-Metrics/Training(Step): loss,1610362060058,0.12045078724622726
14028,-Metrics/Training(Step): loss,1610362064122,0.14086337387561798
14030,-Metrics/Training(Step): loss,1610362068047,0.09791532903909683
14032,-Metrics/Training(Step): loss,1610362072619,0.09533104300498962
14034,-Metrics/Training(Step): loss,1610362075622,0.12923315167427063
14036,-Metrics/Training(Step): loss,1610362079420,0.12460125237703323
14038,-Metrics/Training(Step): loss,1610362082715,0.09836018085479736
14040,-Metrics/Training(Step): loss,1610362086020,0.09438014775514603
14042,-Metrics/Training(Step): loss,1610362089435,0.09754583239555359
14044,-Metrics/Training(Step): loss,1610362093524,0.10800335556268692
14046,-Metrics/Training(Step): loss,1610362097415,0.11671847850084305
14048,-Metrics/Training(Step): loss,1610362100920,0.1135590597987175
14050,-Metrics/Training(Step): loss,1610362104757,0.09871509671211243
14052,-Metrics/Training(Step): loss,1610362108420,0.12440527230501175
14054,-Metrics/Training(Step): loss,1610362111459,0.08537513017654419
14056,-Metrics/Training(Step): loss,1610362115526,0.09788897633552551
14058,-Metrics/Training(Step): loss,1610362119073,0.12558116018772125
14060,-Metrics/Training(Step): loss,1610362122539,0.09843841940164566
14062,-Metrics/Training(Step): loss,1610362125957,0.10423825681209564
14064,-Metrics/Training(Step): loss,1610362129364,0.12067212909460068
14066,-Metrics/Training(Step): loss,1610362132163,0.09212330728769302
14068,-Metrics/Training(Step): loss,1610362134603,0.09665688127279282
14070,-Metrics/Training(Step): loss,1610362136632,0.11255858093500137
14072,-Metrics/Training(Step): loss,1610362138697,0.1169252023100853
14074,-Metrics/Training(Step): loss,1610362140800,0.12142818421125412
14076,-Metrics/Training(Step): loss,1610362142879,0.09506940096616745
14078,-Metrics/Training(Step): loss,1610362144973,0.08558003604412079
14080,-Metrics/Training(Step): loss,1610362147110,0.09617593884468079
14082,-Metrics/Training(Step): loss,1610362149217,0.10520527511835098
14084,-Metrics/Training(Step): loss,1610362151355,0.09822556376457214
14086,-Metrics/Training(Step): loss,1610362153284,0.08085846155881882
14088,-Metrics/Training(Step): loss,1610362155076,0.09973728656768799
14090,-Metrics/Training(Step): loss,1610362157189,0.12109547853469849
14092,-Metrics/Training(Step): loss,1610362158962,0.07702696323394775
14094,-Metrics/Training(Step): loss,1610362160898,0.08416575193405151
14096,-Metrics/Training(Step): loss,1610362162974,0.12404980510473251
14098,-Metrics/Training(Step): loss,1610362164953,0.10365395992994308
14100,-Metrics/Training(Step): loss,1610362166992,0.10023864358663559
14102,-Metrics/Training(Step): loss,1610362169046,0.0946354866027832
14104,-Metrics/Training(Step): loss,1610362171118,0.11111248284578323
14106,-Metrics/Training(Step): loss,1610362183919,0.09492143988609314
14108,-Metrics/Training(Step): loss,1610362188432,0.09565290063619614
14110,-Metrics/Training(Step): loss,1610362193020,0.10620680451393127
14112,-Metrics/Training(Step): loss,1610362197015,0.1149565577507019
14114,-Metrics/Training(Step): loss,1610362200946,0.11410129070281982
14116,-Metrics/Training(Step): loss,1610362205119,0.1171501949429512
14118,-Metrics/Training(Step): loss,1610362208922,0.10150936245918274
14120,-Metrics/Training(Step): loss,1610362212375,0.10725318640470505
14122,-Metrics/Training(Step): loss,1610362216038,0.1855488270521164
14124,-Metrics/Training(Step): loss,1610362219393,0.12409662455320358
14126,-Metrics/Training(Step): loss,1610362223120,0.11742545664310455
14128,-Metrics/Training(Step): loss,1610362226798,0.12648528814315796
14130,-Metrics/Training(Step): loss,1610362230042,0.12367057055234909
14132,-Metrics/Training(Step): loss,1610362233821,0.092841736972332
14134,-Metrics/Training(Step): loss,1610362237123,0.09100787341594696
14136,-Metrics/Training(Step): loss,1610362240820,0.12339077889919281
14138,-Metrics/Training(Step): loss,1610362244219,0.10737365484237671
14140,-Metrics/Training(Step): loss,1610362247819,0.15670420229434967
14142,-Metrics/Training(Step): loss,1610362250620,0.11430849879980087
14144,-Metrics/Training(Step): loss,1610362254383,0.11315278708934784
14146,-Metrics/Training(Step): loss,1610362257588,0.11785678565502167
14148,-Metrics/Training(Step): loss,1610362260916,0.08429072052240372
14150,-Metrics/Training(Step): loss,1610362264656,0.08883558958768845
14152,-Metrics/Training(Step): loss,1610362267519,0.12696583569049835
14154,-Metrics/Training(Step): loss,1610362269790,0.11433219164609909
14156,-Metrics/Training(Step): loss,1610362272079,0.13359548151493073
14158,-Metrics/Training(Step): loss,1610362274276,0.12128689140081406
14160,-Metrics/Training(Step): loss,1610362276323,0.13675926625728607
14162,-Metrics/Training(Step): loss,1610362278465,0.11134512722492218
14164,-Metrics/Training(Step): loss,1610362280557,0.13888859748840332
14166,-Metrics/Training(Step): loss,1610362282696,0.1079898476600647
14168,-Metrics/Training(Step): loss,1610362284840,0.12052455544471741
14170,-Metrics/Training(Step): loss,1610362286944,0.15318310260772705
14172,-Metrics/Training(Step): loss,1610362289046,0.12906761467456818
14174,-Metrics/Training(Step): loss,1610362291144,0.12469788640737534
14176,-Metrics/Training(Step): loss,1610362293168,0.10234902054071426
14178,-Metrics/Training(Step): loss,1610362295102,0.10325905680656433
14180,-Metrics/Training(Step): loss,1610362297140,0.08720286190509796
14182,-Metrics/Training(Step): loss,1610362299121,0.11796442419290543
14184,-Metrics/Training(Step): loss,1610362301202,0.11824673414230347
14186,-Metrics/Training(Step): loss,1610362303254,0.1147027537226677
14188,-Metrics/Training(Step): loss,1610362305324,0.09556202590465546
14190,-Metrics/Training(Step): loss,1610362307394,0.13706786930561066
14192,-Metrics/Training(Step): loss,1610362321123,0.09120535105466843
14194,-Metrics/Training(Step): loss,1610362325219,0.12028719484806061
14196,-Metrics/Training(Step): loss,1610362329535,0.09471993893384933
14198,-Metrics/Training(Step): loss,1610362333626,0.11019837856292725
14200,-Metrics/Training(Step): loss,1610362337436,0.12589941918849945
14202,-Metrics/Training(Step): loss,1610362341420,0.11569053679704666
14204,-Metrics/Training(Step): loss,1610362345416,0.1002521887421608
14206,-Metrics/Training(Step): loss,1610362348751,0.11741255223751068
14208,-Metrics/Training(Step): loss,1610362352323,0.0928860530257225
14210,-Metrics/Training(Step): loss,1610362356217,0.11258193850517273
14212,-Metrics/Training(Step): loss,1610362359797,0.13602285087108612
14214,-Metrics/Training(Step): loss,1610362363720,0.0824257954955101
14216,-Metrics/Training(Step): loss,1610362367219,0.0970303863286972
14218,-Metrics/Training(Step): loss,1610362371135,0.13032889366149902
14220,-Metrics/Training(Step): loss,1610362374817,0.11362585425376892
14222,-Metrics/Training(Step): loss,1610362378023,0.12291545420885086
14224,-Metrics/Training(Step): loss,1610362381820,0.10358031094074249
14226,-Metrics/Training(Step): loss,1610362385521,0.11993364244699478
14228,-Metrics/Training(Step): loss,1610362389220,0.10888395458459854
14230,-Metrics/Training(Step): loss,1610362392423,0.10376197099685669
14232,-Metrics/Training(Step): loss,1610362395653,0.10663357377052307
14234,-Metrics/Training(Step): loss,1610362398420,0.09448710083961487
14236,-Metrics/Training(Step): loss,1610362401340,0.12267226725816727
14238,-Metrics/Training(Step): loss,1610362404540,0.1109735444188118
14240,-Metrics/Training(Step): loss,1610362406967,0.10325680673122406
14242,-Metrics/Training(Step): loss,1610362409348,0.09887725859880447
14244,-Metrics/Training(Step): loss,1610362411415,0.09770888835191727
14246,-Metrics/Training(Step): loss,1610362413520,0.12525542080402374
14248,-Metrics/Training(Step): loss,1610362415651,0.11515375971794128
14250,-Metrics/Training(Step): loss,1610362417744,0.10153121501207352
14252,-Metrics/Training(Step): loss,1610362419686,0.11809825152158737
14254,-Metrics/Training(Step): loss,1610362421783,0.09661322087049484
14256,-Metrics/Training(Step): loss,1610362423904,0.10482578724622726
14258,-Metrics/Training(Step): loss,1610362425956,0.10248251259326935
14260,-Metrics/Training(Step): loss,1610362428074,0.11209152638912201
14262,-Metrics/Training(Step): loss,1610362430123,0.10381224006414413
14264,-Metrics/Training(Step): loss,1610362432244,0.11736185103654861
14266,-Metrics/Training(Step): loss,1610362434405,0.11926158517599106
14268,-Metrics/Training(Step): loss,1610362436421,0.10007621347904205
14270,-Metrics/Training(Step): loss,1610362438426,0.07873793691396713
14272,-Metrics/Training(Step): loss,1610362440493,0.09773732721805573
14274,-Metrics/Training(Step): loss,1610362442590,0.1287415772676468
14276,-Metrics/Training(Step): loss,1610362444551,0.1292761117219925
14278,-Metrics/Training(Step): loss,1610362457428,0.10675513744354248
14280,-Metrics/Training(Step): loss,1610362461815,0.11596812307834625
14282,-Metrics/Training(Step): loss,1610362466019,0.09907948970794678
14284,-Metrics/Training(Step): loss,1610362470250,0.09224220365285873
14286,-Metrics/Training(Step): loss,1610362474450,0.11622747033834457
14288,-Metrics/Training(Step): loss,1610362478322,0.10392951965332031
14290,-Metrics/Training(Step): loss,1610362481920,0.1280740350484848
14292,-Metrics/Training(Step): loss,1610362485122,0.11574545502662659
14294,-Metrics/Training(Step): loss,1610362488657,0.12046854943037033
14296,-Metrics/Training(Step): loss,1610362492419,0.09546495229005814
14298,-Metrics/Training(Step): loss,1610362496422,0.10355126112699509
14300,-Metrics/Training(Step): loss,1610362499920,0.11779127269983292
14302,-Metrics/Training(Step): loss,1610362503520,0.12031571567058563
14304,-Metrics/Training(Step): loss,1610362507236,0.1183195486664772
14306,-Metrics/Training(Step): loss,1610362510817,0.11927234381437302
14308,-Metrics/Training(Step): loss,1610362515087,0.09500421583652496
14310,-Metrics/Training(Step): loss,1610362518428,0.09065250307321548
14312,-Metrics/Training(Step): loss,1610362521446,0.1067313626408577
14314,-Metrics/Training(Step): loss,1610362525352,0.10595972090959549
14316,-Metrics/Training(Step): loss,1610362528807,0.11093413084745407
14318,-Metrics/Training(Step): loss,1610362532542,0.09137146174907684
14320,-Metrics/Training(Step): loss,1610362536525,0.11384854465723038
14322,-Metrics/Training(Step): loss,1610362539675,0.08117865771055222
14324,-Metrics/Training(Step): loss,1610362542568,0.09378376603126526
14326,-Metrics/Training(Step): loss,1610362544604,0.12402565032243729
14328,-Metrics/Training(Step): loss,1610362546655,0.10667067021131516
14330,-Metrics/Training(Step): loss,1610362548767,0.10691574215888977
14332,-Metrics/Training(Step): loss,1610362550863,0.14144444465637207
14334,-Metrics/Training(Step): loss,1610362552969,0.112591952085495
14336,-Metrics/Training(Step): loss,1610362555079,0.11800949275493622
14338,-Metrics/Training(Step): loss,1610362557189,0.11047618091106415
14340,-Metrics/Training(Step): loss,1610362559315,0.11153360456228256
14342,-Metrics/Training(Step): loss,1610362561355,0.08448278903961182
14344,-Metrics/Training(Step): loss,1610362563278,0.09170273691415787
14346,-Metrics/Training(Step): loss,1610362565414,0.12426013499498367
14348,-Metrics/Training(Step): loss,1610362567526,0.10756821185350418
14350,-Metrics/Training(Step): loss,1610362569520,0.09559234976768494
14352,-Metrics/Training(Step): loss,1610362571416,0.10382062196731567
14354,-Metrics/Training(Step): loss,1610362573432,0.10847001522779465
14356,-Metrics/Training(Step): loss,1610362575467,0.1017775610089302
14358,-Metrics/Training(Step): loss,1610362577401,0.12420033663511276
14360,-Metrics/Training(Step): loss,1610362579460,0.12339257448911667
14362,-Metrics/Training(Step): loss,1610362581494,0.12054787576198578
14364,-Metrics/Training(Step): loss,1610362596130,0.10536430031061172
14366,-Metrics/Training(Step): loss,1610362600335,0.0947275310754776
14368,-Metrics/Training(Step): loss,1610362604421,0.10940521210432053
14370,-Metrics/Training(Step): loss,1610362608419,0.09796024113893509
14372,-Metrics/Training(Step): loss,1610362612434,0.10904776304960251
14374,-Metrics/Training(Step): loss,1610362616621,0.11217866092920303
14376,-Metrics/Training(Step): loss,1610362620240,0.10468782484531403
14378,-Metrics/Training(Step): loss,1610362623957,0.12181685864925385
14380,-Metrics/Training(Step): loss,1610362627419,0.1175941526889801
14382,-Metrics/Training(Step): loss,1610362631120,0.11070923507213593
14384,-Metrics/Training(Step): loss,1610362634722,0.09815633296966553
14386,-Metrics/Training(Step): loss,1610362638655,0.11741403490304947
14388,-Metrics/Training(Step): loss,1610362642321,0.09090519696474075
14390,-Metrics/Training(Step): loss,1610362646651,0.10626526176929474
14392,-Metrics/Training(Step): loss,1610362650100,0.10363522171974182
14394,-Metrics/Training(Step): loss,1610362653120,0.1137508898973465
14396,-Metrics/Training(Step): loss,1610362657264,0.09242001920938492
14398,-Metrics/Training(Step): loss,1610362661217,0.12894123792648315
14400,-Metrics/Training(Step): loss,1610362664420,0.12356819957494736
14402,-Metrics/Training(Step): loss,1610362667580,0.10619612038135529
14404,-Metrics/Training(Step): loss,1610362670819,0.10243900865316391
14406,-Metrics/Training(Step): loss,1610362674434,0.1040080264210701
14408,-Metrics/Training(Step): loss,1610362678637,0.12852264940738678
14410,-Metrics/Training(Step): loss,1610362681485,0.1264214664697647
14412,-Metrics/Training(Step): loss,1610362683565,0.11979098618030548
14414,-Metrics/Training(Step): loss,1610362685580,0.09969589114189148
14416,-Metrics/Training(Step): loss,1610362687568,0.10838072746992111
14418,-Metrics/Training(Step): loss,1610362689602,0.11577370017766953
14420,-Metrics/Training(Step): loss,1610362691714,0.10349410772323608
14422,-Metrics/Training(Step): loss,1610362693780,0.11106468737125397
14424,-Metrics/Training(Step): loss,1610362695887,0.0791507288813591
14426,-Metrics/Training(Step): loss,1610362697977,0.10902786254882812
14428,-Metrics/Training(Step): loss,1610362700093,0.10310947895050049
14430,-Metrics/Training(Step): loss,1610362701824,0.09726591408252716
14432,-Metrics/Training(Step): loss,1610362703941,0.11556190997362137
14434,-Metrics/Training(Step): loss,1610362705943,0.12409309297800064
14436,-Metrics/Training(Step): loss,1610362707844,0.09909998625516891
14438,-Metrics/Training(Step): loss,1610362709925,0.0976550430059433
14440,-Metrics/Training(Step): loss,1610362711953,0.11984681338071823
14442,-Metrics/Training(Step): loss,1610362713904,0.09351971000432968
14444,-Metrics/Training(Step): loss,1610362715947,0.11088491976261139
14446,-Metrics/Training(Step): loss,1610362718033,0.09634693711996078
14448,-Metrics/Training(Step): loss,1610362720096,0.10389100015163422
14450,-Metrics/Training(Step): loss,1610362732826,0.09855010360479355
14452,-Metrics/Training(Step): loss,1610362736916,0.10575217753648758
14454,-Metrics/Training(Step): loss,1610362741157,0.09177672863006592
14456,-Metrics/Training(Step): loss,1610362745017,0.11010199785232544
14458,-Metrics/Training(Step): loss,1610362748922,0.13375523686408997
14460,-Metrics/Training(Step): loss,1610362752819,0.13140806555747986
14462,-Metrics/Training(Step): loss,1610362756619,0.08783576637506485
14464,-Metrics/Training(Step): loss,1610362760520,0.13804133236408234
14466,-Metrics/Training(Step): loss,1610362764339,0.12989568710327148
14468,-Metrics/Training(Step): loss,1610362767919,0.11743950843811035
14470,-Metrics/Training(Step): loss,1610362770820,0.07764604687690735
14472,-Metrics/Training(Step): loss,1610362773920,0.10059364140033722
14474,-Metrics/Training(Step): loss,1610362777519,0.08650954067707062
14476,-Metrics/Training(Step): loss,1610362781243,0.13104280829429626
14478,-Metrics/Training(Step): loss,1610362784599,0.10700028389692307
14480,-Metrics/Training(Step): loss,1610362788237,0.10603676736354828
14482,-Metrics/Training(Step): loss,1610362791521,0.11794260144233704
14484,-Metrics/Training(Step): loss,1610362795402,0.1198091134428978
14486,-Metrics/Training(Step): loss,1610362798920,0.10900886356830597
14488,-Metrics/Training(Step): loss,1610362802247,0.09339642524719238
14490,-Metrics/Training(Step): loss,1610362806084,0.10416647791862488
14492,-Metrics/Training(Step): loss,1610362809741,0.08774619549512863
14494,-Metrics/Training(Step): loss,1610362812738,0.10930048674345016
14496,-Metrics/Training(Step): loss,1610362815998,0.08973702788352966
14498,-Metrics/Training(Step): loss,1610362818497,0.09437243640422821
14500,-Metrics/Training(Step): loss,1610362820890,0.09184261411428452
14502,-Metrics/Training(Step): loss,1610362822988,0.10767462104558945
14504,-Metrics/Training(Step): loss,1610362825037,0.09247776120901108
14506,-Metrics/Training(Step): loss,1610362827157,0.1103382557630539
14508,-Metrics/Training(Step): loss,1610362829276,0.12167153507471085
14510,-Metrics/Training(Step): loss,1610362831376,0.07932376116514206
14512,-Metrics/Training(Step): loss,1610362833361,0.07765348255634308
14514,-Metrics/Training(Step): loss,1610362835455,0.09790016710758209
14516,-Metrics/Training(Step): loss,1610362837466,0.09822746366262436
14518,-Metrics/Training(Step): loss,1610362839596,0.10856599360704422
14520,-Metrics/Training(Step): loss,1610362841642,0.08857306092977524
14522,-Metrics/Training(Step): loss,1610362843679,0.11194887012243271
14524,-Metrics/Training(Step): loss,1610362845804,0.113837830722332
14526,-Metrics/Training(Step): loss,1610362847812,0.10513759404420853
14528,-Metrics/Training(Step): loss,1610362849760,0.10427645593881607
14530,-Metrics/Training(Step): loss,1610362851807,0.11961405724287033
14532,-Metrics/Training(Step): loss,1610362853854,0.11735276877880096
14534,-Metrics/Training(Step): loss,1610362855931,0.10899028182029724
14536,-Metrics/Training(Step): loss,1610362868927,0.11237238347530365
14538,-Metrics/Training(Step): loss,1610362873023,0.1000198945403099
14540,-Metrics/Training(Step): loss,1610362877219,0.10450490564107895
14542,-Metrics/Training(Step): loss,1610362881025,0.07340353727340698
14544,-Metrics/Training(Step): loss,1610362885219,0.08187109977006912
14546,-Metrics/Training(Step): loss,1610362888944,0.1312207132577896
14548,-Metrics/Training(Step): loss,1610362893019,0.10758479684591293
14550,-Metrics/Training(Step): loss,1610362897222,0.11590483039617538
14552,-Metrics/Training(Step): loss,1610362901321,0.12060035765171051
14554,-Metrics/Training(Step): loss,1610362904741,0.10521294921636581
14556,-Metrics/Training(Step): loss,1610362908419,0.11211180686950684
14558,-Metrics/Training(Step): loss,1610362911819,0.08281548321247101
14560,-Metrics/Training(Step): loss,1610362914620,0.10687052458524704
14562,-Metrics/Training(Step): loss,1610362918317,0.1213865652680397
14564,-Metrics/Training(Step): loss,1610362921658,0.09003086388111115
14566,-Metrics/Training(Step): loss,1610362925001,0.099022775888443
14568,-Metrics/Training(Step): loss,1610362928468,0.09850582480430603
14570,-Metrics/Training(Step): loss,1610362931920,0.12196875363588333
14572,-Metrics/Training(Step): loss,1610362935319,0.12586630880832672
14574,-Metrics/Training(Step): loss,1610362938819,0.08506681770086288
14576,-Metrics/Training(Step): loss,1610362942107,0.11071085184812546
14578,-Metrics/Training(Step): loss,1610362946115,0.08245384693145752
14580,-Metrics/Training(Step): loss,1610362949864,0.07995551824569702
14582,-Metrics/Training(Step): loss,1610362952944,0.08058282732963562
14584,-Metrics/Training(Step): loss,1610362955262,0.13195273280143738
14586,-Metrics/Training(Step): loss,1610362957400,0.09561596810817719
14588,-Metrics/Training(Step): loss,1610362959509,0.09811068326234818
14590,-Metrics/Training(Step): loss,1610362961657,0.09489943087100983
14592,-Metrics/Training(Step): loss,1610362963821,0.09575745463371277
14594,-Metrics/Training(Step): loss,1610362965878,0.10427836328744888
14596,-Metrics/Training(Step): loss,1610362967987,0.12328676879405975
14598,-Metrics/Training(Step): loss,1610362969916,0.12661100924015045
14600,-Metrics/Training(Step): loss,1610362971908,0.09350652992725372
14602,-Metrics/Training(Step): loss,1610362974019,0.09958840906620026
14604,-Metrics/Training(Step): loss,1610362976044,0.10327134281396866
14606,-Metrics/Training(Step): loss,1610362978131,0.11186525225639343
14608,-Metrics/Training(Step): loss,1610362980184,0.1142004057765007
14610,-Metrics/Training(Step): loss,1610362982122,0.09433451294898987
14612,-Metrics/Training(Step): loss,1610362984137,0.09827218949794769
14614,-Metrics/Training(Step): loss,1610362986205,0.10336611419916153
14616,-Metrics/Training(Step): loss,1610362988159,0.12688098847866058
14618,-Metrics/Training(Step): loss,1610362990197,0.1165938526391983
14620,-Metrics/Training(Step): loss,1610362992288,0.09653499722480774
14622,-Metrics/Training(Step): loss,1610363005126,0.11086327582597733
14624,-Metrics/Training(Step): loss,1610363008941,0.09698639810085297
14626,-Metrics/Training(Step): loss,1610363013120,0.11631080508232117
14628,-Metrics/Training(Step): loss,1610363016825,0.08364740759134293
14630,-Metrics/Training(Step): loss,1610363021124,0.10043506324291229
14632,-Metrics/Training(Step): loss,1610363025019,0.09611979871988297
14634,-Metrics/Training(Step): loss,1610363028819,0.10804834961891174
14636,-Metrics/Training(Step): loss,1610363032817,0.1036982461810112
14638,-Metrics/Training(Step): loss,1610363036121,0.12277743220329285
14640,-Metrics/Training(Step): loss,1610363039619,0.08703021705150604
14642,-Metrics/Training(Step): loss,1610363043531,0.10658598691225052
14644,-Metrics/Training(Step): loss,1610363047010,0.09592463821172714
14646,-Metrics/Training(Step): loss,1610363050838,0.09896861761808395
14648,-Metrics/Training(Step): loss,1610363054528,0.09221305698156357
14650,-Metrics/Training(Step): loss,1610363058301,0.09638620913028717
14652,-Metrics/Training(Step): loss,1610363061717,0.10651810467243195
14654,-Metrics/Training(Step): loss,1610363065055,0.10053270310163498
14656,-Metrics/Training(Step): loss,1610363068720,0.11129404604434967
14658,-Metrics/Training(Step): loss,1610363072616,0.0999138131737709
14660,-Metrics/Training(Step): loss,1610363075921,0.11703598499298096
14662,-Metrics/Training(Step): loss,1610363079620,0.08656951040029526
14664,-Metrics/Training(Step): loss,1610363082917,0.10723084211349487
14666,-Metrics/Training(Step): loss,1610363086657,0.1269073486328125
14668,-Metrics/Training(Step): loss,1610363089186,0.09041959047317505
14670,-Metrics/Training(Step): loss,1610363091369,0.10901839286088943
14672,-Metrics/Training(Step): loss,1610363093593,0.11901586502790451
14674,-Metrics/Training(Step): loss,1610363095741,0.11033840477466583
14676,-Metrics/Training(Step): loss,1610363097831,0.08455032855272293
14678,-Metrics/Training(Step): loss,1610363099928,0.06720907986164093
14680,-Metrics/Training(Step): loss,1610363102008,0.09426958858966827
14682,-Metrics/Training(Step): loss,1610363104099,0.10123809427022934
14684,-Metrics/Training(Step): loss,1610363106186,0.11032267659902573
14686,-Metrics/Training(Step): loss,1610363108162,0.11655492335557938
14688,-Metrics/Training(Step): loss,1610363110254,0.10659331828355789
14690,-Metrics/Training(Step): loss,1610363112371,0.09934011846780777
14692,-Metrics/Training(Step): loss,1610363114434,0.12955985963344574
14694,-Metrics/Training(Step): loss,1610363116378,0.11034499853849411
14696,-Metrics/Training(Step): loss,1610363118435,0.12205418944358826
14698,-Metrics/Training(Step): loss,1610363120432,0.11493206769227982
14700,-Metrics/Training(Step): loss,1610363122414,0.10468446463346481
14702,-Metrics/Training(Step): loss,1610363124452,0.10267072170972824
14704,-Metrics/Training(Step): loss,1610363126493,0.107230044901371
14706,-Metrics/Training(Step): loss,1610363128557,0.08106999844312668
14708,-Metrics/Training(Step): loss,1610363141331,0.10304494947195053
14710,-Metrics/Training(Step): loss,1610363145320,0.07330935448408127
14712,-Metrics/Training(Step): loss,1610363148933,0.10935347527265549
14714,-Metrics/Training(Step): loss,1610363153320,0.12188108265399933
14716,-Metrics/Training(Step): loss,1610363157155,0.1121106892824173
14718,-Metrics/Training(Step): loss,1610363161019,0.1218806579709053
14720,-Metrics/Training(Step): loss,1610363164315,0.09173160791397095
14722,-Metrics/Training(Step): loss,1610363168132,0.09855175018310547
14724,-Metrics/Training(Step): loss,1610363171622,0.10341528058052063
14726,-Metrics/Training(Step): loss,1610363175619,0.12734787166118622
14728,-Metrics/Training(Step): loss,1610363178850,0.10408731549978256
14730,-Metrics/Training(Step): loss,1610363182515,0.0837855190038681
14732,-Metrics/Training(Step): loss,1610363185639,0.10048634558916092
14734,-Metrics/Training(Step): loss,1610363189086,0.11635110527276993
14736,-Metrics/Training(Step): loss,1610363192707,0.10050874203443527
14738,-Metrics/Training(Step): loss,1610363196321,0.1383458822965622
14740,-Metrics/Training(Step): loss,1610363199620,0.07416471093893051
14742,-Metrics/Training(Step): loss,1610363203020,0.11006204783916473
14744,-Metrics/Training(Step): loss,1610363206320,0.11412838101387024
14746,-Metrics/Training(Step): loss,1610363210041,0.10864667594432831
14748,-Metrics/Training(Step): loss,1610363213620,0.0935337096452713
14750,-Metrics/Training(Step): loss,1610363217386,0.11033731698989868
14752,-Metrics/Training(Step): loss,1610363220629,0.11038792878389359
14754,-Metrics/Training(Step): loss,1610363223842,0.09351393580436707
14756,-Metrics/Training(Step): loss,1610363226231,0.10552269965410233
14758,-Metrics/Training(Step): loss,1610363228386,0.11868505924940109
14760,-Metrics/Training(Step): loss,1610363230590,0.1260514259338379
14762,-Metrics/Training(Step): loss,1610363232739,0.09957018494606018
14764,-Metrics/Training(Step): loss,1610363234850,0.09127177298069
14766,-Metrics/Training(Step): loss,1610363236865,0.14394693076610565
14768,-Metrics/Training(Step): loss,1610363239013,0.10678131878376007
14770,-Metrics/Training(Step): loss,1610363241078,0.12439362704753876
14772,-Metrics/Training(Step): loss,1610363243049,0.12799307703971863
14774,-Metrics/Training(Step): loss,1610363245152,0.10253382474184036
14776,-Metrics/Training(Step): loss,1610363247235,0.08693614602088928
14778,-Metrics/Training(Step): loss,1610363249234,0.07837546616792679
14780,-Metrics/Training(Step): loss,1610363251147,0.11192470043897629
14782,-Metrics/Training(Step): loss,1610363253044,0.09387935698032379
14784,-Metrics/Training(Step): loss,1610363255123,0.10285293310880661
14786,-Metrics/Training(Step): loss,1610363257086,0.09812165051698685
14788,-Metrics/Training(Step): loss,1610363259124,0.11145532876253128
14790,-Metrics/Training(Step): loss,1610363261187,0.1062888354063034
14792,-Metrics/Training(Step): loss,1610363263249,0.1063031554222107
14794,-Metrics/Training(Step): loss,1610363276331,0.12282291054725647
14796,-Metrics/Training(Step): loss,1610363280523,0.11626238375902176
14798,-Metrics/Training(Step): loss,1610363284616,0.10813172906637192
14800,-Metrics/Training(Step): loss,1610363288433,0.10976279526948929
14802,-Metrics/Training(Step): loss,1610363292419,0.1064990684390068
14804,-Metrics/Training(Step): loss,1610363296421,0.10653560608625412
14806,-Metrics/Training(Step): loss,1610363300448,0.08735150098800659
14808,-Metrics/Training(Step): loss,1610363304119,0.10463526099920273
14810,-Metrics/Training(Step): loss,1610363308118,0.09596336632966995
14812,-Metrics/Training(Step): loss,1610363311802,0.08023986965417862
14814,-Metrics/Training(Step): loss,1610363315655,0.09006080776453018
14816,-Metrics/Training(Step): loss,1610363319345,0.10129138827323914
14818,-Metrics/Training(Step): loss,1610363323220,0.1242130771279335
14820,-Metrics/Training(Step): loss,1610363326529,0.09974458813667297
14822,-Metrics/Training(Step): loss,1610363330118,0.09314770251512527
14824,-Metrics/Training(Step): loss,1610363333800,0.0920616015791893
14826,-Metrics/Training(Step): loss,1610363337308,0.0940040573477745
14828,-Metrics/Training(Step): loss,1610363341019,0.11099648475646973
14830,-Metrics/Training(Step): loss,1610363344761,0.0996982678771019
14832,-Metrics/Training(Step): loss,1610363348031,0.09947824478149414
14834,-Metrics/Training(Step): loss,1610363351755,0.11118994653224945
14836,-Metrics/Training(Step): loss,1610363355067,0.12491235136985779
14838,-Metrics/Training(Step): loss,1610363358307,0.08723455667495728
14840,-Metrics/Training(Step): loss,1610363361520,0.11809788644313812
14842,-Metrics/Training(Step): loss,1610363363847,0.10817405581474304
14844,-Metrics/Training(Step): loss,1610363365998,0.0986621081829071
14846,-Metrics/Training(Step): loss,1610363368135,0.14163124561309814
14848,-Metrics/Training(Step): loss,1610363370156,0.10076498985290527
14850,-Metrics/Training(Step): loss,1610363372232,0.09505333751440048
14852,-Metrics/Training(Step): loss,1610363374284,0.11592576652765274
14854,-Metrics/Training(Step): loss,1610363376397,0.08510582894086838
14856,-Metrics/Training(Step): loss,1610363378399,0.08213380724191666
14858,-Metrics/Training(Step): loss,1610363380354,0.09709639847278595
14860,-Metrics/Training(Step): loss,1610363382455,0.07639838755130768
14862,-Metrics/Training(Step): loss,1610363384173,0.09077044576406479
14864,-Metrics/Training(Step): loss,1610363386118,0.1296551525592804
14866,-Metrics/Training(Step): loss,1610363388185,0.10600677132606506
14868,-Metrics/Training(Step): loss,1610363390256,0.11031439900398254
14870,-Metrics/Training(Step): loss,1610363392313,0.08348169177770615
14872,-Metrics/Training(Step): loss,1610363394332,0.09689512848854065
14874,-Metrics/Training(Step): loss,1610363396410,0.08872337639331818
14876,-Metrics/Training(Step): loss,1610363398417,0.09973375499248505
14878,-Metrics/Training(Step): loss,1610363400463,0.12015107274055481
14880,-Metrics/Training(Step): loss,1610363413523,0.11843803524971008
14882,-Metrics/Training(Step): loss,1610363417637,0.1538269966840744
14884,-Metrics/Training(Step): loss,1610363421615,0.10914960503578186
14886,-Metrics/Training(Step): loss,1610363426419,0.09830022603273392
14888,-Metrics/Training(Step): loss,1610363430320,0.11904985457658768
14890,-Metrics/Training(Step): loss,1610363433829,0.1272750347852707
14892,-Metrics/Training(Step): loss,1610363437720,0.09477842599153519
14894,-Metrics/Training(Step): loss,1610363441427,0.10596983134746552
14896,-Metrics/Training(Step): loss,1610363445228,0.08416762948036194
14898,-Metrics/Training(Step): loss,1610363449420,0.12073948234319687
14900,-Metrics/Training(Step): loss,1610363453020,0.09971436113119125
14902,-Metrics/Training(Step): loss,1610363456481,0.1101442277431488
14904,-Metrics/Training(Step): loss,1610363459901,0.09163380414247513
14906,-Metrics/Training(Step): loss,1610363463324,0.12239859998226166
14908,-Metrics/Training(Step): loss,1610363467041,0.12444949150085449
14910,-Metrics/Training(Step): loss,1610363470864,0.1181621104478836
14912,-Metrics/Training(Step): loss,1610363474443,0.11269605159759521
14914,-Metrics/Training(Step): loss,1610363477920,0.09276775270700455
14916,-Metrics/Training(Step): loss,1610363481027,0.09243516623973846
14918,-Metrics/Training(Step): loss,1610363484697,0.08496885001659393
14920,-Metrics/Training(Step): loss,1610363487920,0.1222907230257988
14922,-Metrics/Training(Step): loss,1610363491225,0.09295791387557983
14924,-Metrics/Training(Step): loss,1610363494476,0.10736894607543945
14926,-Metrics/Training(Step): loss,1610363497258,0.11491159349679947
14928,-Metrics/Training(Step): loss,1610363499566,0.09109111875295639
14930,-Metrics/Training(Step): loss,1610363501732,0.09640184789896011
14932,-Metrics/Training(Step): loss,1610363503733,0.10402362793684006
14934,-Metrics/Training(Step): loss,1610363505852,0.11872724443674088
14936,-Metrics/Training(Step): loss,1610363507970,0.11254820972681046
14938,-Metrics/Training(Step): loss,1610363510164,0.11972088366746902
14940,-Metrics/Training(Step): loss,1610363512289,0.11338486522436142
14942,-Metrics/Training(Step): loss,1610363514352,0.08085881173610687
14944,-Metrics/Training(Step): loss,1610363516291,0.10300260037183762
14946,-Metrics/Training(Step): loss,1610363518281,0.08289142698049545
14948,-Metrics/Training(Step): loss,1610363520384,0.08930952847003937
14950,-Metrics/Training(Step): loss,1610363522447,0.10311044007539749
14952,-Metrics/Training(Step): loss,1610363524396,0.11867529153823853
14954,-Metrics/Training(Step): loss,1610363526325,0.08029089868068695
14956,-Metrics/Training(Step): loss,1610363528388,0.0979977399110794
14958,-Metrics/Training(Step): loss,1610363530521,0.11898928880691528
14960,-Metrics/Training(Step): loss,1610363532550,0.08922469615936279
14962,-Metrics/Training(Step): loss,1610363534611,0.09420359134674072
14964,-Metrics/Training(Step): loss,1610363536652,0.12318100035190582
14966,-Metrics/Training(Step): loss,1610363550021,0.11180366575717926
14968,-Metrics/Training(Step): loss,1610363554320,0.1092715933918953
14970,-Metrics/Training(Step): loss,1610363558020,0.13362054526805878
14972,-Metrics/Training(Step): loss,1610363561558,0.09920009225606918
14974,-Metrics/Training(Step): loss,1610363565821,0.10302124172449112
14976,-Metrics/Training(Step): loss,1610363570242,0.08594252169132233
14978,-Metrics/Training(Step): loss,1610363574726,0.11391208320856094
14980,-Metrics/Training(Step): loss,1610363578124,0.09918499737977982
14982,-Metrics/Training(Step): loss,1610363581420,0.09875316172838211
14984,-Metrics/Training(Step): loss,1610363584867,0.09636455029249191
14986,-Metrics/Training(Step): loss,1610363588007,0.10011814534664154
14988,-Metrics/Training(Step): loss,1610363591719,0.08118318021297455
14990,-Metrics/Training(Step): loss,1610363595019,0.08885660022497177
14992,-Metrics/Training(Step): loss,1610363598721,0.10679113864898682
14994,-Metrics/Training(Step): loss,1610363602890,0.09587761014699936
14996,-Metrics/Training(Step): loss,1610363606320,0.10694972425699234
14998,-Metrics/Training(Step): loss,1610363610080,0.09389527887105942
15000,-Metrics/Training(Step): loss,1610363613521,0.11686927080154419
15002,-Metrics/Training(Step): loss,1610363616617,0.09651762992143631
15004,-Metrics/Training(Step): loss,1610363620200,0.11217617988586426
15006,-Metrics/Training(Step): loss,1610363623020,0.10063235461711884
15008,-Metrics/Training(Step): loss,1610363626319,0.10113005340099335
15010,-Metrics/Training(Step): loss,1610363630084,0.10898783802986145
15012,-Metrics/Training(Step): loss,1610363633544,0.11818259209394455
15014,-Metrics/Training(Step): loss,1610363635841,0.1104292944073677
15016,-Metrics/Training(Step): loss,1610363637970,0.13042128086090088
15018,-Metrics/Training(Step): loss,1610363640197,0.09341125935316086
15020,-Metrics/Training(Step): loss,1610363642288,0.09912294149398804
15022,-Metrics/Training(Step): loss,1610363644403,0.08969184011220932
15024,-Metrics/Training(Step): loss,1610363646411,0.10113099962472916
15026,-Metrics/Training(Step): loss,1610363648553,0.11956395953893661
15028,-Metrics/Training(Step): loss,1610363650648,0.11717507988214493
15030,-Metrics/Training(Step): loss,1610363652754,0.06965486705303192
15032,-Metrics/Training(Step): loss,1610363654817,0.09929851442575455
15034,-Metrics/Training(Step): loss,1610363656932,0.11225320398807526
15036,-Metrics/Training(Step): loss,1610363658889,0.08414928615093231
15038,-Metrics/Training(Step): loss,1610363660860,0.11174749583005905
15040,-Metrics/Training(Step): loss,1610363662962,0.10344142466783524
15042,-Metrics/Training(Step): loss,1610363664997,0.11881262063980103
15044,-Metrics/Training(Step): loss,1610363666947,0.10131770372390747
15046,-Metrics/Training(Step): loss,1610363669025,0.1112397164106369
15048,-Metrics/Training(Step): loss,1610363670923,0.09401758015155792
15050,-Metrics/Training(Step): loss,1610363672987,0.06832244247198105
15052,-Metrics/Training(Step): loss,1610363685431,0.10931438952684402
15054,-Metrics/Training(Step): loss,1610363689621,0.11790738254785538
15056,-Metrics/Training(Step): loss,1610363693520,0.06775514036417007
15058,-Metrics/Training(Step): loss,1610363697339,0.126124307513237
15060,-Metrics/Training(Step): loss,1610363701243,0.08920615911483765
15062,-Metrics/Training(Step): loss,1610363705432,0.10869892686605453
15064,-Metrics/Training(Step): loss,1610363709551,0.07992367446422577
15066,-Metrics/Training(Step): loss,1610363713060,0.09796043485403061
15068,-Metrics/Training(Step): loss,1610363717037,0.13012902438640594
15070,-Metrics/Training(Step): loss,1610363720146,0.0952787771821022
15072,-Metrics/Training(Step): loss,1610363723559,0.08187125623226166
15074,-Metrics/Training(Step): loss,1610363727415,0.1081620305776596
15076,-Metrics/Training(Step): loss,1610363731423,0.10812170803546906
15078,-Metrics/Training(Step): loss,1610363735020,0.09449239820241928
15080,-Metrics/Training(Step): loss,1610363738521,0.10888728499412537
15082,-Metrics/Training(Step): loss,1610363741921,0.13267658650875092
15084,-Metrics/Training(Step): loss,1610363745382,0.07861312478780746
15086,-Metrics/Training(Step): loss,1610363749486,0.1069469079375267
15088,-Metrics/Training(Step): loss,1610363752817,0.09055408090353012
15090,-Metrics/Training(Step): loss,1610363756138,0.07447302341461182
15092,-Metrics/Training(Step): loss,1610363760189,0.08775341510772705
15094,-Metrics/Training(Step): loss,1610363763220,0.1106361523270607
15096,-Metrics/Training(Step): loss,1610363766538,0.09844022244215012
15098,-Metrics/Training(Step): loss,1610363769867,0.08378379791975021
15100,-Metrics/Training(Step): loss,1610363772324,0.12139922380447388
15102,-Metrics/Training(Step): loss,1610363774557,0.11490773409605026
15104,-Metrics/Training(Step): loss,1610363776671,0.12050861865282059
15106,-Metrics/Training(Step): loss,1610363778778,0.08381211757659912
15108,-Metrics/Training(Step): loss,1610363780833,0.11981119960546494
15110,-Metrics/Training(Step): loss,1610363782838,0.09932906180620193
15112,-Metrics/Training(Step): loss,1610363784833,0.10801127552986145
15114,-Metrics/Training(Step): loss,1610363786837,0.10438200831413269
15116,-Metrics/Training(Step): loss,1610363788952,0.08369554579257965
15118,-Metrics/Training(Step): loss,1610363791053,0.09654498100280762
15120,-Metrics/Training(Step): loss,1610363793091,0.0980798527598381
15122,-Metrics/Training(Step): loss,1610363795210,0.11091715097427368
15124,-Metrics/Training(Step): loss,1610363797255,0.08521050214767456
15126,-Metrics/Training(Step): loss,1610363799306,0.09524185955524445
15128,-Metrics/Training(Step): loss,1610363801365,0.11020561307668686
15130,-Metrics/Training(Step): loss,1610363803493,0.10218767076730728
15132,-Metrics/Training(Step): loss,1610363805580,0.0837787464261055
15134,-Metrics/Training(Step): loss,1610363807650,0.11468403041362762
15136,-Metrics/Training(Step): loss,1610363809747,0.09270797669887543
15138,-Metrics/Training(Step): loss,1610363821929,0.09381958097219467
15140,-Metrics/Training(Step): loss,1610363826021,0.1204831451177597
15142,-Metrics/Training(Step): loss,1610363830316,0.10235484689474106
15144,-Metrics/Training(Step): loss,1610363834615,0.09631238877773285
15146,-Metrics/Training(Step): loss,1610363838956,0.09459234774112701
15148,-Metrics/Training(Step): loss,1610363843420,0.10932987928390503
15150,-Metrics/Training(Step): loss,1610363847756,0.09722505509853363
15152,-Metrics/Training(Step): loss,1610363851919,0.10296713560819626
15154,-Metrics/Training(Step): loss,1610363855614,0.09389688074588776
15156,-Metrics/Training(Step): loss,1610363858919,0.0937596932053566
15158,-Metrics/Training(Step): loss,1610363862498,0.11287154257297516
15160,-Metrics/Training(Step): loss,1610363865942,0.10946089774370193
15162,-Metrics/Training(Step): loss,1610363869503,0.10296069830656052
15164,-Metrics/Training(Step): loss,1610363873059,0.10160716623067856
15166,-Metrics/Training(Step): loss,1610363876597,0.08632905036211014
15168,-Metrics/Training(Step): loss,1610363880026,0.08953913301229477
15170,-Metrics/Training(Step): loss,1610363883520,0.07121050357818604
15172,-Metrics/Training(Step): loss,1610363887654,0.09529909491539001
15174,-Metrics/Training(Step): loss,1610363891020,0.09824251383543015
15176,-Metrics/Training(Step): loss,1610363894420,0.08243785798549652
15178,-Metrics/Training(Step): loss,1610363897420,0.12705986201763153
15180,-Metrics/Training(Step): loss,1610363901103,0.09466730803251266
15182,-Metrics/Training(Step): loss,1610363905034,0.094778873026371
15184,-Metrics/Training(Step): loss,1610363907835,0.0956009030342102
15186,-Metrics/Training(Step): loss,1610363910147,0.10721604526042938
15188,-Metrics/Training(Step): loss,1610363912311,0.11758257448673248
15190,-Metrics/Training(Step): loss,1610363914346,0.12228067219257355
15192,-Metrics/Training(Step): loss,1610363916477,0.13723409175872803
15194,-Metrics/Training(Step): loss,1610363918573,0.09335245937108994
15196,-Metrics/Training(Step): loss,1610363920654,0.10191971063613892
15198,-Metrics/Training(Step): loss,1610363922769,0.09309129416942596
15200,-Metrics/Training(Step): loss,1610363924543,0.1086297333240509
15202,-Metrics/Training(Step): loss,1610363926590,0.10818181931972504
15204,-Metrics/Training(Step): loss,1610363928587,0.10793475061655045
15206,-Metrics/Training(Step): loss,1610363930670,0.11196169257164001
15208,-Metrics/Training(Step): loss,1610363932565,0.11149007827043533
15210,-Metrics/Training(Step): loss,1610363934636,0.08795063197612762
15212,-Metrics/Training(Step): loss,1610363936575,0.09646638482809067
15214,-Metrics/Training(Step): loss,1610363938716,0.08504878729581833
15216,-Metrics/Training(Step): loss,1610363940684,0.10732454806566238
15218,-Metrics/Training(Step): loss,1610363942733,0.11109127104282379
15220,-Metrics/Training(Step): loss,1610363944764,0.11752937734127045
15222,-Metrics/Training(Step): loss,1610363946828,0.0948815569281578
15224,-Metrics/Training(Step): loss,1610363960336,0.11020393669605255
15226,-Metrics/Training(Step): loss,1610363964416,0.11110159009695053
15228,-Metrics/Training(Step): loss,1610363968121,0.11041856557130814
15230,-Metrics/Training(Step): loss,1610363972525,0.0930556207895279
15232,-Metrics/Training(Step): loss,1610363976925,0.09551150351762772
15234,-Metrics/Training(Step): loss,1610363980724,0.1175462082028389
15236,-Metrics/Training(Step): loss,1610363985021,0.07624366134405136
15238,-Metrics/Training(Step): loss,1610363989241,0.08717966824769974
15240,-Metrics/Training(Step): loss,1610363993021,0.10741694271564484
15242,-Metrics/Training(Step): loss,1610363996425,0.10058172047138214
15244,-Metrics/Training(Step): loss,1610364000348,0.11747594177722931
15246,-Metrics/Training(Step): loss,1610364003955,0.10493311285972595
15248,-Metrics/Training(Step): loss,1610364008021,0.12342776358127594
15250,-Metrics/Training(Step): loss,1610364011762,0.10347443073987961
15252,-Metrics/Training(Step): loss,1610364015427,0.09199916571378708
15254,-Metrics/Training(Step): loss,1610364019124,0.09397385269403458
15256,-Metrics/Training(Step): loss,1610364022844,0.0994354635477066
15258,-Metrics/Training(Step): loss,1610364026417,0.0977277010679245
15260,-Metrics/Training(Step): loss,1610364030455,0.1262837052345276
15262,-Metrics/Training(Step): loss,1610364033943,0.11529126763343811
15264,-Metrics/Training(Step): loss,1610364037060,0.14313244819641113
15266,-Metrics/Training(Step): loss,1610364041028,0.09992159903049469
15268,-Metrics/Training(Step): loss,1610364043931,0.15299829840660095
15270,-Metrics/Training(Step): loss,1610364046294,0.11389213800430298
15272,-Metrics/Training(Step): loss,1610364048335,0.07038410007953644
15274,-Metrics/Training(Step): loss,1610364050442,0.10131862759590149
15276,-Metrics/Training(Step): loss,1610364052527,0.09727253764867783
15278,-Metrics/Training(Step): loss,1610364054510,0.08714888244867325
15280,-Metrics/Training(Step): loss,1610364056540,0.09396163374185562
15282,-Metrics/Training(Step): loss,1610364058660,0.10479629784822464
15284,-Metrics/Training(Step): loss,1610364060804,0.08797075599431992
15286,-Metrics/Training(Step): loss,1610364062890,0.09723290055990219
15288,-Metrics/Training(Step): loss,1610364065021,0.0966682881116867
15290,-Metrics/Training(Step): loss,1610364067113,0.10936146974563599
15292,-Metrics/Training(Step): loss,1610364069048,0.08922260254621506
15294,-Metrics/Training(Step): loss,1610364071165,0.1046040803194046
15296,-Metrics/Training(Step): loss,1610364073162,0.11497858166694641
15298,-Metrics/Training(Step): loss,1610364075121,0.10490749031305313
15300,-Metrics/Training(Step): loss,1610364077246,0.10702617466449738
15302,-Metrics/Training(Step): loss,1610364079353,0.09180140495300293
15304,-Metrics/Training(Step): loss,1610364081408,0.10703956335783005
15306,-Metrics/Training(Step): loss,1610364083442,0.12998315691947937
15308,-Metrics/Training(Step): loss,1610364085381,0.084049753844738
15310,-Metrics/Training(Step): loss,1610364097128,0.09596695005893707
15312,-Metrics/Training(Step): loss,1610364100919,0.09171189367771149
15314,-Metrics/Training(Step): loss,1610364104920,0.1125146672129631
15316,-Metrics/Training(Step): loss,1610364108828,0.07257752865552902
15318,-Metrics/Training(Step): loss,1610364113319,0.11532335728406906
15320,-Metrics/Training(Step): loss,1610364117328,0.10575654357671738
15322,-Metrics/Training(Step): loss,1610364121634,0.09457072615623474
15324,-Metrics/Training(Step): loss,1610364125520,0.11711643636226654
15326,-Metrics/Training(Step): loss,1610364129486,0.1059797927737236
15328,-Metrics/Training(Step): loss,1610364133720,0.10362361371517181
15330,-Metrics/Training(Step): loss,1610364137686,0.09359754621982574
15332,-Metrics/Training(Step): loss,1610364141242,0.10196090489625931
15334,-Metrics/Training(Step): loss,1610364144424,0.09769364446401596
15336,-Metrics/Training(Step): loss,1610364148128,0.11090302467346191
15338,-Metrics/Training(Step): loss,1610364151728,0.09849899262189865
15340,-Metrics/Training(Step): loss,1610364154731,0.08694831281900406
15342,-Metrics/Training(Step): loss,1610364158420,0.10341508686542511
15344,-Metrics/Training(Step): loss,1610364162362,0.09420311450958252
15346,-Metrics/Training(Step): loss,1610364165815,0.11633848398923874
15348,-Metrics/Training(Step): loss,1610364168528,0.10797560960054398
15350,-Metrics/Training(Step): loss,1610364172356,0.11603648960590363
15352,-Metrics/Training(Step): loss,1610364175726,0.10701290518045425
15354,-Metrics/Training(Step): loss,1610364179322,0.11430636048316956
15356,-Metrics/Training(Step): loss,1610364182326,0.07237815111875534
15358,-Metrics/Training(Step): loss,1610364184947,0.10689735412597656
15360,-Metrics/Training(Step): loss,1610364187127,0.11609723418951035
15362,-Metrics/Training(Step): loss,1610364189232,0.10803143680095673
15364,-Metrics/Training(Step): loss,1610364191336,0.0997651144862175
15366,-Metrics/Training(Step): loss,1610364193490,0.0988919734954834
15368,-Metrics/Training(Step): loss,1610364195592,0.09020832180976868
15370,-Metrics/Training(Step): loss,1610364197666,0.09305407106876373
15372,-Metrics/Training(Step): loss,1610364199785,0.10030960291624069
15374,-Metrics/Training(Step): loss,1610364201764,0.10847148299217224
15376,-Metrics/Training(Step): loss,1610364203905,0.113807313144207
15378,-Metrics/Training(Step): loss,1610364206078,0.08235099166631699
15380,-Metrics/Training(Step): loss,1610364208223,0.09121871739625931
15382,-Metrics/Training(Step): loss,1610364210234,0.1067633181810379
15384,-Metrics/Training(Step): loss,1610364212211,0.0974186509847641
15386,-Metrics/Training(Step): loss,1610364214227,0.061980511993169785
15388,-Metrics/Training(Step): loss,1610364216302,0.07906250655651093
15390,-Metrics/Training(Step): loss,1610364218345,0.09176376461982727
15392,-Metrics/Training(Step): loss,1610364220433,0.0923837423324585
15394,-Metrics/Training(Step): loss,1610364222444,0.10716873407363892
15396,-Metrics/Training(Step): loss,1610364234630,0.08123244345188141
15398,-Metrics/Training(Step): loss,1610364238821,0.11432979255914688
15400,-Metrics/Training(Step): loss,1610364242620,0.10197407752275467
15402,-Metrics/Training(Step): loss,1610364246830,0.12440013140439987
15404,-Metrics/Training(Step): loss,1610364250919,0.09557079523801804
15406,-Metrics/Training(Step): loss,1610364254758,0.08219942450523376
15408,-Metrics/Training(Step): loss,1610364259123,0.11359571665525436
15410,-Metrics/Training(Step): loss,1610364262758,0.10201014578342438
15412,-Metrics/Training(Step): loss,1610364266429,0.09268830716609955
15414,-Metrics/Training(Step): loss,1610364270215,0.09592802077531815
15416,-Metrics/Training(Step): loss,1610364274029,0.11018088459968567
15418,-Metrics/Training(Step): loss,1610364277492,0.11166715621948242
15420,-Metrics/Training(Step): loss,1610364281023,0.09960085153579712
15422,-Metrics/Training(Step): loss,1610364284724,0.12189951539039612
15424,-Metrics/Training(Step): loss,1610364288556,0.09692113101482391
15426,-Metrics/Training(Step): loss,1610364292524,0.12531878054141998
15428,-Metrics/Training(Step): loss,1610364296519,0.07242736965417862
15430,-Metrics/Training(Step): loss,1610364300247,0.11779867112636566
15432,-Metrics/Training(Step): loss,1610364303371,0.09702431410551071
15434,-Metrics/Training(Step): loss,1610364306855,0.09422614425420761
15436,-Metrics/Training(Step): loss,1610364310920,0.10404074192047119
15438,-Metrics/Training(Step): loss,1610364314049,0.10325979441404343
15440,-Metrics/Training(Step): loss,1610364317548,0.09367102384567261
15442,-Metrics/Training(Step): loss,1610364320351,0.10811442881822586
15444,-Metrics/Training(Step): loss,1610364322799,0.09425194561481476
15446,-Metrics/Training(Step): loss,1610364324884,0.09926943480968475
15448,-Metrics/Training(Step): loss,1610364326981,0.10229868441820145
15450,-Metrics/Training(Step): loss,1610364329094,0.10253524780273438
15452,-Metrics/Training(Step): loss,1610364331121,0.1027466431260109
15454,-Metrics/Training(Step): loss,1610364333196,0.1166200190782547
15456,-Metrics/Training(Step): loss,1610364335120,0.11603138595819473
15458,-Metrics/Training(Step): loss,1610364337049,0.09635211527347565
15460,-Metrics/Training(Step): loss,1610364339147,0.09893088042736053
15462,-Metrics/Training(Step): loss,1610364341208,0.09867903590202332
15464,-Metrics/Training(Step): loss,1610364343284,0.09576539695262909
15466,-Metrics/Training(Step): loss,1610364345190,0.10924214124679565
15468,-Metrics/Training(Step): loss,1610364347266,0.1145721822977066
15470,-Metrics/Training(Step): loss,1610364349335,0.08827341347932816
15472,-Metrics/Training(Step): loss,1610364351147,0.11567359417676926
15474,-Metrics/Training(Step): loss,1610364353207,0.09548722207546234
15476,-Metrics/Training(Step): loss,1610364355293,0.07496685534715652
15478,-Metrics/Training(Step): loss,1610364357358,0.10547758638858795
15480,-Metrics/Training(Step): loss,1610364359436,0.09084003418684006
15482,-Metrics/Training(Step): loss,1610364383522,0.09676560759544373
15484,-Metrics/Training(Step): loss,1610364387518,0.10348981618881226
15486,-Metrics/Training(Step): loss,1610364391221,0.11894060671329498
15488,-Metrics/Training(Step): loss,1610364395619,0.10220713913440704
15490,-Metrics/Training(Step): loss,1610364399319,0.1022581085562706
15492,-Metrics/Training(Step): loss,1610364403027,0.07986730337142944
15494,-Metrics/Training(Step): loss,1610364406821,0.10637199878692627
15496,-Metrics/Training(Step): loss,1610364410637,0.12401314079761505
15498,-Metrics/Training(Step): loss,1610364414131,0.09248926490545273
15500,-Metrics/Training(Step): loss,1610364417819,0.12732543051242828
15502,-Metrics/Training(Step): loss,1610364421508,0.09537771344184875
15504,-Metrics/Training(Step): loss,1610364425120,0.1050477996468544
15506,-Metrics/Training(Step): loss,1610364429321,0.11918310075998306
15508,-Metrics/Training(Step): loss,1610364432622,0.09918190538883209
15510,-Metrics/Training(Step): loss,1610364436027,0.10389181226491928
15512,-Metrics/Training(Step): loss,1610364439759,0.08953974395990372
15514,-Metrics/Training(Step): loss,1610364443328,0.10617697983980179
15516,-Metrics/Training(Step): loss,1610364446886,0.10587630420923233
15518,-Metrics/Training(Step): loss,1610364450627,0.09120727330446243
15520,-Metrics/Training(Step): loss,1610364454220,0.10345220565795898
15522,-Metrics/Training(Step): loss,1610364457619,0.10112722963094711
15524,-Metrics/Training(Step): loss,1610364461242,0.10101383924484253
15526,-Metrics/Training(Step): loss,1610364464719,0.1118224486708641
15528,-Metrics/Training(Step): loss,1610364467934,0.11410905420780182
15530,-Metrics/Training(Step): loss,1610364470241,0.08845571428537369
15532,-Metrics/Training(Step): loss,1610364472431,0.09449564665555954
15534,-Metrics/Training(Step): loss,1610364474550,0.12044224143028259
15536,-Metrics/Training(Step): loss,1610364476531,0.10909231752157211
15538,-Metrics/Training(Step): loss,1610364478537,0.10470259934663773
15540,-Metrics/Training(Step): loss,1610364480619,0.1111985519528389
15542,-Metrics/Training(Step): loss,1610364482618,0.10624469071626663
15544,-Metrics/Training(Step): loss,1610364484695,0.09358593076467514
15546,-Metrics/Training(Step): loss,1610364486790,0.08329049497842789
15548,-Metrics/Training(Step): loss,1610364488907,0.09324291348457336
15550,-Metrics/Training(Step): loss,1610364490849,0.10102908313274384
15552,-Metrics/Training(Step): loss,1610364492983,0.08995521068572998
15554,-Metrics/Training(Step): loss,1610364495093,0.11321700364351273
15556,-Metrics/Training(Step): loss,1610364497136,0.0921211689710617
15558,-Metrics/Training(Step): loss,1610364499094,0.09321993589401245
15560,-Metrics/Training(Step): loss,1610364501126,0.1001005545258522
15562,-Metrics/Training(Step): loss,1610364503197,0.07338375598192215
15564,-Metrics/Training(Step): loss,1610364505263,0.1170239970088005
15566,-Metrics/Training(Step): loss,1610364507296,0.08567745238542557
15568,-Metrics/Training(Step): loss,1610364519625,0.08574001491069794
15570,-Metrics/Training(Step): loss,1610364523620,0.1043141558766365
15572,-Metrics/Training(Step): loss,1610364527515,0.1219891607761383
15574,-Metrics/Training(Step): loss,1610364531517,0.09256178140640259
15576,-Metrics/Training(Step): loss,1610364535618,0.10208242386579514
15578,-Metrics/Training(Step): loss,1610364539948,0.08197754621505737
15580,-Metrics/Training(Step): loss,1610364544325,0.09081321954727173
15582,-Metrics/Training(Step): loss,1610364548334,0.10241005569696426
15584,-Metrics/Training(Step): loss,1610364551903,0.12562288343906403
15586,-Metrics/Training(Step): loss,1610364555724,0.0908971056342125
15588,-Metrics/Training(Step): loss,1610364559485,0.13120223581790924
15590,-Metrics/Training(Step): loss,1610364563219,0.09221591055393219
15592,-Metrics/Training(Step): loss,1610364566844,0.10112366080284119
15594,-Metrics/Training(Step): loss,1610364570521,0.10842295736074448
15596,-Metrics/Training(Step): loss,1610364573967,0.08100439608097076
15598,-Metrics/Training(Step): loss,1610364577821,0.12025415152311325
15600,-Metrics/Training(Step): loss,1610364581919,0.08404505252838135
15602,-Metrics/Training(Step): loss,1610364585630,0.09694000333547592
15604,-Metrics/Training(Step): loss,1610364588919,0.12211331725120544
15606,-Metrics/Training(Step): loss,1610364592319,0.09965413808822632
15608,-Metrics/Training(Step): loss,1610364596091,0.11671639233827591
15610,-Metrics/Training(Step): loss,1610364599492,0.10098137706518173
15612,-Metrics/Training(Step): loss,1610364603194,0.09348974376916885
15614,-Metrics/Training(Step): loss,1610364605713,0.13653306663036346
15616,-Metrics/Training(Step): loss,1610364607953,0.09060841053724289
15618,-Metrics/Training(Step): loss,1610364610208,0.09365906566381454
15620,-Metrics/Training(Step): loss,1610364612386,0.10264018923044205
15622,-Metrics/Training(Step): loss,1610364614442,0.0902022123336792
15624,-Metrics/Training(Step): loss,1610364616549,0.10741223394870758
15626,-Metrics/Training(Step): loss,1610364618650,0.10133173316717148
15628,-Metrics/Training(Step): loss,1610364620767,0.10537635535001755
15630,-Metrics/Training(Step): loss,1610364622886,0.09397386014461517
15632,-Metrics/Training(Step): loss,1610364624903,0.1079811230301857
15634,-Metrics/Training(Step): loss,1610364627009,0.10971645265817642
15636,-Metrics/Training(Step): loss,1610364629091,0.09348499029874802
15638,-Metrics/Training(Step): loss,1610364631049,0.09202928096055984
15640,-Metrics/Training(Step): loss,1610364633141,0.09864246845245361
15642,-Metrics/Training(Step): loss,1610364635087,0.11126630753278732
15644,-Metrics/Training(Step): loss,1610364637176,0.09848075360059738
15646,-Metrics/Training(Step): loss,1610364639286,0.09802418202161789
15648,-Metrics/Training(Step): loss,1610364641349,0.09380153566598892
15650,-Metrics/Training(Step): loss,1610364643382,0.0731775164604187
15652,-Metrics/Training(Step): loss,1610364645482,0.08341079205274582
15654,-Metrics/Training(Step): loss,1610364657126,0.08346734941005707
15656,-Metrics/Training(Step): loss,1610364661119,0.07685042917728424
15658,-Metrics/Training(Step): loss,1610364665555,0.0813034251332283
15660,-Metrics/Training(Step): loss,1610364669623,0.11671927571296692
15662,-Metrics/Training(Step): loss,1610364673649,0.11114401370286942
15664,-Metrics/Training(Step): loss,1610364678058,0.09252975136041641
15666,-Metrics/Training(Step): loss,1610364682142,0.1188182607293129
15668,-Metrics/Training(Step): loss,1610364686129,0.07191861420869827
15670,-Metrics/Training(Step): loss,1610364690120,0.09053871035575867
15672,-Metrics/Training(Step): loss,1610364694021,0.1055125892162323
15674,-Metrics/Training(Step): loss,1610364697420,0.10496098548173904
15676,-Metrics/Training(Step): loss,1610364700418,0.1024824008345604
15678,-Metrics/Training(Step): loss,1610364703721,0.10508191585540771
15680,-Metrics/Training(Step): loss,1610364707558,0.09631486982107162
15682,-Metrics/Training(Step): loss,1610364711463,0.11666295677423477
15684,-Metrics/Training(Step): loss,1610364714852,0.09338003396987915
15686,-Metrics/Training(Step): loss,1610364718720,0.08737815171480179
15688,-Metrics/Training(Step): loss,1610364722321,0.10382282733917236
15690,-Metrics/Training(Step): loss,1610364725443,0.0770815908908844
15692,-Metrics/Training(Step): loss,1610364728324,0.09265757352113724
15694,-Metrics/Training(Step): loss,1610364731237,0.12260592728853226
15696,-Metrics/Training(Step): loss,1610364734829,0.09219463169574738
15698,-Metrics/Training(Step): loss,1610364738019,0.09233406186103821
15700,-Metrics/Training(Step): loss,1610364741109,0.10612554103136063
15702,-Metrics/Training(Step): loss,1610364743452,0.08642654120922089
15704,-Metrics/Training(Step): loss,1610364745629,0.115545354783535
15706,-Metrics/Training(Step): loss,1610364747609,0.10964252799749374
15708,-Metrics/Training(Step): loss,1610364749628,0.09571075439453125
15710,-Metrics/Training(Step): loss,1610364751759,0.09430629014968872
15712,-Metrics/Training(Step): loss,1610364753870,0.09082780033349991
15714,-Metrics/Training(Step): loss,1610364755937,0.1079397201538086
15716,-Metrics/Training(Step): loss,1610364758036,0.11986467987298965
15718,-Metrics/Training(Step): loss,1610364759968,0.10019300132989883
15720,-Metrics/Training(Step): loss,1610364761966,0.10483759641647339
15722,-Metrics/Training(Step): loss,1610364763922,0.09721650183200836
15724,-Metrics/Training(Step): loss,1610364765905,0.10810339450836182
15726,-Metrics/Training(Step): loss,1610364767985,0.11269842833280563
15728,-Metrics/Training(Step): loss,1610364769941,0.09374082833528519
15730,-Metrics/Training(Step): loss,1610364771997,0.10744913667440414
15732,-Metrics/Training(Step): loss,1610364774045,0.09468844532966614
15734,-Metrics/Training(Step): loss,1610364776092,0.10191425681114197
15736,-Metrics/Training(Step): loss,1610364777994,0.11551091074943542
15738,-Metrics/Training(Step): loss,1610364780046,0.0898108258843422
15740,-Metrics/Training(Step): loss,1610364792320,0.10111590474843979
15742,-Metrics/Training(Step): loss,1610364796120,0.0865461677312851
15744,-Metrics/Training(Step): loss,1610364800149,0.10539496690034866
15746,-Metrics/Training(Step): loss,1610364804254,0.09356182813644409
15748,-Metrics/Training(Step): loss,1610364808334,0.11540722101926804
15750,-Metrics/Training(Step): loss,1610364812137,0.09925007820129395
15752,-Metrics/Training(Step): loss,1610364815939,0.09402666985988617
15754,-Metrics/Training(Step): loss,1610364820319,0.08618669211864471
15756,-Metrics/Training(Step): loss,1610364824226,0.09918752312660217
15758,-Metrics/Training(Step): loss,1610364827840,0.07578518986701965
15760,-Metrics/Training(Step): loss,1610364831648,0.07795031368732452
15762,-Metrics/Training(Step): loss,1610364835624,0.10429058969020844
15764,-Metrics/Training(Step): loss,1610364838841,0.09712845832109451
15766,-Metrics/Training(Step): loss,1610364842620,0.08978477865457535
15768,-Metrics/Training(Step): loss,1610364846458,0.12031996995210648
15770,-Metrics/Training(Step): loss,1610364850120,0.09912003576755524
15772,-Metrics/Training(Step): loss,1610364853020,0.11048673093318939
15774,-Metrics/Training(Step): loss,1610364856248,0.06185206398367882
15776,-Metrics/Training(Step): loss,1610364859620,0.08760242909193039
15778,-Metrics/Training(Step): loss,1610364862819,0.11316663771867752
15780,-Metrics/Training(Step): loss,1610364866746,0.09120505303144455
15782,-Metrics/Training(Step): loss,1610364870328,0.11521825194358826
15784,-Metrics/Training(Step): loss,1610364873762,0.08957023918628693
15786,-Metrics/Training(Step): loss,1610364876667,0.11430229246616364
15788,-Metrics/Training(Step): loss,1610364879151,0.09486624598503113
15790,-Metrics/Training(Step): loss,1610364881424,0.07562332600355148
15792,-Metrics/Training(Step): loss,1610364883495,0.10877957195043564
15794,-Metrics/Training(Step): loss,1610364885581,0.1038132905960083
15796,-Metrics/Training(Step): loss,1610364887669,0.09983067214488983
15798,-Metrics/Training(Step): loss,1610364889805,0.08050850033760071
15800,-Metrics/Training(Step): loss,1610364891922,0.11874865740537643
15802,-Metrics/Training(Step): loss,1610364893831,0.11735006421804428
15804,-Metrics/Training(Step): loss,1610364895918,0.10453492403030396
15806,-Metrics/Training(Step): loss,1610364897950,0.12116578966379166
15808,-Metrics/Training(Step): loss,1610364900068,0.09694451838731766
15810,-Metrics/Training(Step): loss,1610364902139,0.10919295251369476
15812,-Metrics/Training(Step): loss,1610364904067,0.11694244295358658
15814,-Metrics/Training(Step): loss,1610364906052,0.11401022970676422
15816,-Metrics/Training(Step): loss,1610364908108,0.10051429271697998
15818,-Metrics/Training(Step): loss,1610364910136,0.0955200344324112
15820,-Metrics/Training(Step): loss,1610364912186,0.11663134396076202
15822,-Metrics/Training(Step): loss,1610364914131,0.09538938850164413
15824,-Metrics/Training(Step): loss,1610364916192,0.08174441754817963
15826,-Metrics/Training(Step): loss,1610364929138,0.11280868947505951
15828,-Metrics/Training(Step): loss,1610364932920,0.10593380033969879
15830,-Metrics/Training(Step): loss,1610364937048,0.10441161692142487
15832,-Metrics/Training(Step): loss,1610364940819,0.08220427483320236
15834,-Metrics/Training(Step): loss,1610364944620,0.10742492973804474
15836,-Metrics/Training(Step): loss,1610364948620,0.0897984579205513
15838,-Metrics/Training(Step): loss,1610364952915,0.10727555304765701
15840,-Metrics/Training(Step): loss,1610364957119,0.11263510584831238
15842,-Metrics/Training(Step): loss,1610364960897,0.07916010916233063
15844,-Metrics/Training(Step): loss,1610364964509,0.07493336498737335
15846,-Metrics/Training(Step): loss,1610364967938,0.08884592354297638
15848,-Metrics/Training(Step): loss,1610364971019,0.10450763255357742
15850,-Metrics/Training(Step): loss,1610364974711,0.09256621450185776
15852,-Metrics/Training(Step): loss,1610364977815,0.09625648707151413
15854,-Metrics/Training(Step): loss,1610364981042,0.0987049788236618
15856,-Metrics/Training(Step): loss,1610364984519,0.09381444752216339
15858,-Metrics/Training(Step): loss,1610364987619,0.08392155170440674
15860,-Metrics/Training(Step): loss,1610364991731,0.12740056216716766
15862,-Metrics/Training(Step): loss,1610364995202,0.09691248089075089
15864,-Metrics/Training(Step): loss,1610364998657,0.09823781251907349
15866,-Metrics/Training(Step): loss,1610365002020,0.08723084628582001
15868,-Metrics/Training(Step): loss,1610365005831,0.09617072343826294
15870,-Metrics/Training(Step): loss,1610365008936,0.07951556146144867
15872,-Metrics/Training(Step): loss,1610365012507,0.09538108855485916
15874,-Metrics/Training(Step): loss,1610365014733,0.08209938555955887
15876,-Metrics/Training(Step): loss,1610365016935,0.118360735476017
15878,-Metrics/Training(Step): loss,1610365019186,0.1007208600640297
15880,-Metrics/Training(Step): loss,1610365021330,0.09651158004999161
15882,-Metrics/Training(Step): loss,1610365023436,0.09911242872476578
15884,-Metrics/Training(Step): loss,1610365025537,0.10334393382072449
15886,-Metrics/Training(Step): loss,1610365027462,0.11245976388454437
15888,-Metrics/Training(Step): loss,1610365029587,0.11083067208528519
15890,-Metrics/Training(Step): loss,1610365031342,0.1148286759853363
15892,-Metrics/Training(Step): loss,1610365033409,0.09760427474975586
15894,-Metrics/Training(Step): loss,1610365035505,0.11177913844585419
15896,-Metrics/Training(Step): loss,1610365037593,0.11749165505170822
15898,-Metrics/Training(Step): loss,1610365039523,0.09463758021593094
15900,-Metrics/Training(Step): loss,1610365041416,0.095132976770401
15902,-Metrics/Training(Step): loss,1610365043433,0.10583256930112839
15904,-Metrics/Training(Step): loss,1610365045546,0.11689873039722443
15906,-Metrics/Training(Step): loss,1610365047421,0.09110358357429504
15908,-Metrics/Training(Step): loss,1610365049388,0.11457867175340652
15910,-Metrics/Training(Step): loss,1610365051479,0.10234931856393814
15912,-Metrics/Training(Step): loss,1610365064920,0.10316935926675797
15914,-Metrics/Training(Step): loss,1610365069225,0.11561457067728043
15916,-Metrics/Training(Step): loss,1610365072919,0.10517043620347977
15918,-Metrics/Training(Step): loss,1610365076819,0.10948153585195541
15920,-Metrics/Training(Step): loss,1610365080722,0.10147760063409805
15922,-Metrics/Training(Step): loss,1610365084344,0.10961724817752838
15924,-Metrics/Training(Step): loss,1610365088427,0.1051444411277771
15926,-Metrics/Training(Step): loss,1610365092130,0.14335912466049194
15928,-Metrics/Training(Step): loss,1610365095626,0.08681081980466843
15930,-Metrics/Training(Step): loss,1610365099823,0.10465636849403381
15932,-Metrics/Training(Step): loss,1610365103222,0.09993082284927368
15934,-Metrics/Training(Step): loss,1610365106920,0.10416518896818161
15936,-Metrics/Training(Step): loss,1610365110519,0.1004118099808693
15938,-Metrics/Training(Step): loss,1610365113921,0.09139726310968399
15940,-Metrics/Training(Step): loss,1610365117444,0.08970248699188232
15942,-Metrics/Training(Step): loss,1610365120619,0.11460647732019424
15944,-Metrics/Training(Step): loss,1610365124522,0.08304045349359512
15946,-Metrics/Training(Step): loss,1610365127865,0.10665837675333023
15948,-Metrics/Training(Step): loss,1610365131119,0.08838598430156708
15950,-Metrics/Training(Step): loss,1610365134819,0.11079007387161255
15952,-Metrics/Training(Step): loss,1610365138219,0.08260580897331238
15954,-Metrics/Training(Step): loss,1610365140949,0.0907188206911087
15956,-Metrics/Training(Step): loss,1610365144520,0.11925269663333893
15958,-Metrics/Training(Step): loss,1610365148201,0.0962652638554573
15960,-Metrics/Training(Step): loss,1610365151046,0.11177065968513489
15962,-Metrics/Training(Step): loss,1610365153208,0.07011304795742035
15964,-Metrics/Training(Step): loss,1610365155303,0.0799332708120346
15966,-Metrics/Training(Step): loss,1610365157378,0.0948883667588234
15968,-Metrics/Training(Step): loss,1610365159423,0.1159459799528122
15970,-Metrics/Training(Step): loss,1610365161515,0.09327391535043716
15972,-Metrics/Training(Step): loss,1610365163588,0.07205047458410263
15974,-Metrics/Training(Step): loss,1610365165715,0.11172261089086533
15976,-Metrics/Training(Step): loss,1610365167786,0.10295861959457397
15978,-Metrics/Training(Step): loss,1610365169910,0.09680243581533432
15980,-Metrics/Training(Step): loss,1610365171835,0.10801401734352112
15982,-Metrics/Training(Step): loss,1610365173843,0.10704383999109268
15984,-Metrics/Training(Step): loss,1610365175724,0.10966198146343231
15986,-Metrics/Training(Step): loss,1610365177699,0.0968359187245369
15988,-Metrics/Training(Step): loss,1610365179749,0.10633257776498795
15990,-Metrics/Training(Step): loss,1610365181755,0.08269137889146805
15992,-Metrics/Training(Step): loss,1610365183855,0.10048530250787735
15994,-Metrics/Training(Step): loss,1610365185885,0.0941699743270874
15996,-Metrics/Training(Step): loss,1610365187910,0.07728398591279984
15998,-Metrics/Training(Step): loss,1610365199729,0.0859663113951683
16000,-Metrics/Training(Step): loss,1610365203819,0.11099047213792801
16002,-Metrics/Training(Step): loss,1610365207624,0.09078177809715271
16004,-Metrics/Training(Step): loss,1610365211519,0.12700028717517853
16006,-Metrics/Training(Step): loss,1610365215742,0.08652777224779129
16008,-Metrics/Training(Step): loss,1610365219617,0.11704976856708527
16010,-Metrics/Training(Step): loss,1610365223666,0.11050993204116821
16012,-Metrics/Training(Step): loss,1610365227187,0.0952167734503746
16014,-Metrics/Training(Step): loss,1610365230716,0.085054911673069
16016,-Metrics/Training(Step): loss,1610365234320,0.09809882193803787
16018,-Metrics/Training(Step): loss,1610365237920,0.07432633638381958
16020,-Metrics/Training(Step): loss,1610365241320,0.10528554767370224
16022,-Metrics/Training(Step): loss,1610365244719,0.09148301184177399
16024,-Metrics/Training(Step): loss,1610365248319,0.09502691775560379
16026,-Metrics/Training(Step): loss,1610365251821,0.08923749625682831
16028,-Metrics/Training(Step): loss,1610365255752,0.12084045261144638
16030,-Metrics/Training(Step): loss,1610365259229,0.08225405961275101
16032,-Metrics/Training(Step): loss,1610365263024,0.09243211150169373
16034,-Metrics/Training(Step): loss,1610365266842,0.0647425726056099
16036,-Metrics/Training(Step): loss,1610365270319,0.09788603335618973
16038,-Metrics/Training(Step): loss,1610365273999,0.11416489630937576
16040,-Metrics/Training(Step): loss,1610365277619,0.10984796285629272
16042,-Metrics/Training(Step): loss,1610365280914,0.10747060924768448
16044,-Metrics/Training(Step): loss,1610365284034,0.10543119162321091
16046,-Metrics/Training(Step): loss,1610365286338,0.0948067158460617
16048,-Metrics/Training(Step): loss,1610365288427,0.10529328137636185
16050,-Metrics/Training(Step): loss,1610365290441,0.11497984081506729
16052,-Metrics/Training(Step): loss,1610365292351,0.0859309658408165
16054,-Metrics/Training(Step): loss,1610365294457,0.06913524866104126
16056,-Metrics/Training(Step): loss,1610365296575,0.09183136373758316
16058,-Metrics/Training(Step): loss,1610365298666,0.08708911389112473
16060,-Metrics/Training(Step): loss,1610365300753,0.08677837997674942
16062,-Metrics/Training(Step): loss,1610365302736,0.09197112172842026
16064,-Metrics/Training(Step): loss,1610365304789,0.10039585083723068
16066,-Metrics/Training(Step): loss,1610365306800,0.09706854820251465
16068,-Metrics/Training(Step): loss,1610365308878,0.08653969317674637
16070,-Metrics/Training(Step): loss,1610365310849,0.10226736962795258
16072,-Metrics/Training(Step): loss,1610365312971,0.0875411331653595
16074,-Metrics/Training(Step): loss,1610365314952,0.11572133749723434
16076,-Metrics/Training(Step): loss,1610365316936,0.08277333527803421
16078,-Metrics/Training(Step): loss,1610365319027,0.08422952890396118
16080,-Metrics/Training(Step): loss,1610365321077,0.1114150658249855
16082,-Metrics/Training(Step): loss,1610365323040,0.10299932211637497
16084,-Metrics/Training(Step): loss,1610365336641,0.08778712153434753
16086,-Metrics/Training(Step): loss,1610365340720,0.13090035319328308
16088,-Metrics/Training(Step): loss,1610365344619,0.11562808603048325
16090,-Metrics/Training(Step): loss,1610365349030,0.10611121356487274
16092,-Metrics/Training(Step): loss,1610365352919,0.11506553739309311
16094,-Metrics/Training(Step): loss,1610365356658,0.07160992175340652
16096,-Metrics/Training(Step): loss,1610365360653,0.10173603892326355
16098,-Metrics/Training(Step): loss,1610365364771,0.10650184005498886
16100,-Metrics/Training(Step): loss,1610365368320,0.10930296778678894
16102,-Metrics/Training(Step): loss,1610365371436,0.10282381623983383
16104,-Metrics/Training(Step): loss,1610365374832,0.11167794466018677
16106,-Metrics/Training(Step): loss,1610365378520,0.08910997211933136
16108,-Metrics/Training(Step): loss,1610365382134,0.12169800698757172
16110,-Metrics/Training(Step): loss,1610365385530,0.08885490149259567
16112,-Metrics/Training(Step): loss,1610365389020,0.08351060003042221
16114,-Metrics/Training(Step): loss,1610365392415,0.08102799952030182
16116,-Metrics/Training(Step): loss,1610365396018,0.11076144129037857
16118,-Metrics/Training(Step): loss,1610365399014,0.058538246899843216
16120,-Metrics/Training(Step): loss,1610365402720,0.09913241118192673
16122,-Metrics/Training(Step): loss,1610365406319,0.13001199066638947
16124,-Metrics/Training(Step): loss,1610365409519,0.10354124009609222
16126,-Metrics/Training(Step): loss,1610365413032,0.11775784939527512
16128,-Metrics/Training(Step): loss,1610365416428,0.09052346646785736
16130,-Metrics/Training(Step): loss,1610365419919,0.0973324254155159
16132,-Metrics/Training(Step): loss,1610365422464,0.07539109140634537
16134,-Metrics/Training(Step): loss,1610365424528,0.11446497589349747
16136,-Metrics/Training(Step): loss,1610365426627,0.10950014740228653
16138,-Metrics/Training(Step): loss,1610365428702,0.08857078850269318
16140,-Metrics/Training(Step): loss,1610365430758,0.12147437036037445
16142,-Metrics/Training(Step): loss,1610365432865,0.11942898482084274
16144,-Metrics/Training(Step): loss,1610365434916,0.111282579600811
16146,-Metrics/Training(Step): loss,1610365436856,0.10132641345262527
16148,-Metrics/Training(Step): loss,1610365439004,0.09133144468069077
16150,-Metrics/Training(Step): loss,1610365441051,0.1276465803384781
16152,-Metrics/Training(Step): loss,1610365442990,0.09462164342403412
16154,-Metrics/Training(Step): loss,1610365444955,0.09940341114997864
16156,-Metrics/Training(Step): loss,1610365446850,0.09427258372306824
16158,-Metrics/Training(Step): loss,1610365448715,0.08120685070753098
16160,-Metrics/Training(Step): loss,1610365450801,0.08762073516845703
16162,-Metrics/Training(Step): loss,1610365452811,0.10792544484138489
16164,-Metrics/Training(Step): loss,1610365454733,0.11128165572881699
16166,-Metrics/Training(Step): loss,1610365456708,0.10305328667163849
16168,-Metrics/Training(Step): loss,1610365458806,0.10555993020534515
16170,-Metrics/Training(Step): loss,1610365471232,0.10758404433727264
16172,-Metrics/Training(Step): loss,1610365475320,0.08130460232496262
16174,-Metrics/Training(Step): loss,1610365479437,0.08915036916732788
16176,-Metrics/Training(Step): loss,1610365483417,0.11642620712518692
16178,-Metrics/Training(Step): loss,1610365487529,0.09866988658905029
16180,-Metrics/Training(Step): loss,1610365491458,0.12278740108013153
16182,-Metrics/Training(Step): loss,1610365495683,0.09354300051927567
16184,-Metrics/Training(Step): loss,1610365499631,0.10542052239179611
16186,-Metrics/Training(Step): loss,1610365503858,0.10072100907564163
16188,-Metrics/Training(Step): loss,1610365507636,0.08237718790769577
16190,-Metrics/Training(Step): loss,1610365511219,0.12352044880390167
16192,-Metrics/Training(Step): loss,1610365514316,0.10518132895231247
16194,-Metrics/Training(Step): loss,1610365518116,0.07737778127193451
16196,-Metrics/Training(Step): loss,1610365521532,0.0823087990283966
16198,-Metrics/Training(Step): loss,1610365524829,0.07415169477462769
16200,-Metrics/Training(Step): loss,1610365528146,0.10150467604398727
16202,-Metrics/Training(Step): loss,1610365531619,0.10273608565330505
16204,-Metrics/Training(Step): loss,1610365535320,0.12472113221883774
16206,-Metrics/Training(Step): loss,1610365538765,0.10888618975877762
16208,-Metrics/Training(Step): loss,1610365542240,0.08750271797180176
16210,-Metrics/Training(Step): loss,1610365545455,0.14203548431396484
16212,-Metrics/Training(Step): loss,1610365549120,0.0944565087556839
16214,-Metrics/Training(Step): loss,1610365551743,0.10461056977510452
16216,-Metrics/Training(Step): loss,1610365554952,0.11425057798624039
16218,-Metrics/Training(Step): loss,1610365557830,0.081931933760643
16220,-Metrics/Training(Step): loss,1610365560042,0.10154949873685837
16222,-Metrics/Training(Step): loss,1610365562147,0.09031929075717926
16224,-Metrics/Training(Step): loss,1610365564243,0.09730760753154755
16226,-Metrics/Training(Step): loss,1610365566356,0.10465572029352188
16228,-Metrics/Training(Step): loss,1610365568432,0.1117953285574913
16230,-Metrics/Training(Step): loss,1610365570518,0.10408873856067657
16232,-Metrics/Training(Step): loss,1610365572615,0.12144868820905685
16234,-Metrics/Training(Step): loss,1610365574655,0.10161633044481277
16236,-Metrics/Training(Step): loss,1610365576763,0.0871439129114151
16238,-Metrics/Training(Step): loss,1610365578684,0.11714693158864975
16240,-Metrics/Training(Step): loss,1610365580680,0.08537879586219788
16242,-Metrics/Training(Step): loss,1610365582740,0.08672291785478592
16244,-Metrics/Training(Step): loss,1610365584736,0.11566445976495743
16246,-Metrics/Training(Step): loss,1610365586774,0.11319810152053833
16248,-Metrics/Training(Step): loss,1610365588816,0.12014376372098923
16250,-Metrics/Training(Step): loss,1610365590874,0.08679640293121338
16252,-Metrics/Training(Step): loss,1610365592900,0.11769214272499084
16254,-Metrics/Training(Step): loss,1610365594908,0.1304931640625
16256,-Metrics/Training(Step): loss,1610365608127,0.11797695606946945
16258,-Metrics/Training(Step): loss,1610365612131,0.10030413419008255
16260,-Metrics/Training(Step): loss,1610365616124,0.11746679991483688
16262,-Metrics/Training(Step): loss,1610365619936,0.11818539351224899
16264,-Metrics/Training(Step): loss,1610365624156,0.08160679042339325
16266,-Metrics/Training(Step): loss,1610365627719,0.10522366315126419
16268,-Metrics/Training(Step): loss,1610365631385,0.11799075454473495
16270,-Metrics/Training(Step): loss,1610365634727,0.09231876581907272
16272,-Metrics/Training(Step): loss,1610365638119,0.12897461652755737
16274,-Metrics/Training(Step): loss,1610365640981,0.11158056557178497
16276,-Metrics/Training(Step): loss,1610365644556,0.09680460393428802
16278,-Metrics/Training(Step): loss,1610365647854,0.08440882712602615
16280,-Metrics/Training(Step): loss,1610365651822,0.0754755511879921
16282,-Metrics/Training(Step): loss,1610365655393,0.11266119033098221
16284,-Metrics/Training(Step): loss,1610365659235,0.07682295888662338
16286,-Metrics/Training(Step): loss,1610365662743,0.07080400735139847
16288,-Metrics/Training(Step): loss,1610365666249,0.08103083074092865
16290,-Metrics/Training(Step): loss,1610365670043,0.09808223694562912
16292,-Metrics/Training(Step): loss,1610365673773,0.09332898259162903
16294,-Metrics/Training(Step): loss,1610365677085,0.08103223145008087
16296,-Metrics/Training(Step): loss,1610365680643,0.09078054875135422
16298,-Metrics/Training(Step): loss,1610365684115,0.09314315766096115
16300,-Metrics/Training(Step): loss,1610365687861,0.09527076780796051
16302,-Metrics/Training(Step): loss,1610365691129,0.11693407595157623
16304,-Metrics/Training(Step): loss,1610365693472,0.09760597348213196
16306,-Metrics/Training(Step): loss,1610365695426,0.1073588877916336
16308,-Metrics/Training(Step): loss,1610365697491,0.08789905905723572
16310,-Metrics/Training(Step): loss,1610365699574,0.09765543788671494
16312,-Metrics/Training(Step): loss,1610365701707,0.10852045565843582
16314,-Metrics/Training(Step): loss,1610365703866,0.1118423193693161
16316,-Metrics/Training(Step): loss,1610365705880,0.11082302778959274
16318,-Metrics/Training(Step): loss,1610365707972,0.08802851289510727
16320,-Metrics/Training(Step): loss,1610365710073,0.09983476251363754
16322,-Metrics/Training(Step): loss,1610365712138,0.09873449802398682
16324,-Metrics/Training(Step): loss,1610365714185,0.07659188657999039
16326,-Metrics/Training(Step): loss,1610365716238,0.10561691969633102
16328,-Metrics/Training(Step): loss,1610365718212,0.08379773050546646
16330,-Metrics/Training(Step): loss,1610365720287,0.10178884863853455
16332,-Metrics/Training(Step): loss,1610365722367,0.09710786491632462
16334,-Metrics/Training(Step): loss,1610365724488,0.10099539160728455
16336,-Metrics/Training(Step): loss,1610365726390,0.09982676059007645
16338,-Metrics/Training(Step): loss,1610365728463,0.10050845146179199
16340,-Metrics/Training(Step): loss,1610365730513,0.09059537947177887
16342,-Metrics/Training(Step): loss,1610365743925,0.08591405302286148
16344,-Metrics/Training(Step): loss,1610365747935,0.10285831987857819
16346,-Metrics/Training(Step): loss,1610365751858,0.09234122186899185
16348,-Metrics/Training(Step): loss,1610365756053,0.12567834556102753
16350,-Metrics/Training(Step): loss,1610365759822,0.09843012690544128
16352,-Metrics/Training(Step): loss,1610365763723,0.09379599243402481
16354,-Metrics/Training(Step): loss,1610365767822,0.09814441204071045
16356,-Metrics/Training(Step): loss,1610365771621,0.09852639585733414
16358,-Metrics/Training(Step): loss,1610365775237,0.12137701362371445
16360,-Metrics/Training(Step): loss,1610365778826,0.10184739530086517
16362,-Metrics/Training(Step): loss,1610365781919,0.11186661571264267
16364,-Metrics/Training(Step): loss,1610365785620,0.09805928170681
16366,-Metrics/Training(Step): loss,1610365788647,0.10349345207214355
16368,-Metrics/Training(Step): loss,1610365791949,0.11823994666337967
16370,-Metrics/Training(Step): loss,1610365795346,0.09200567752122879
16372,-Metrics/Training(Step): loss,1610365799108,0.07942186295986176
16374,-Metrics/Training(Step): loss,1610365802219,0.07866571098566055
16376,-Metrics/Training(Step): loss,1610365805620,0.06529871374368668
16378,-Metrics/Training(Step): loss,1610365809421,0.12131553143262863
16380,-Metrics/Training(Step): loss,1610365813016,0.08385812491178513
16382,-Metrics/Training(Step): loss,1610365816916,0.09000935405492783
16384,-Metrics/Training(Step): loss,1610365820572,0.1070409044623375
16386,-Metrics/Training(Step): loss,1610365824119,0.10543444752693176
16388,-Metrics/Training(Step): loss,1610365827419,0.10490059107542038
16390,-Metrics/Training(Step): loss,1610365830210,0.13080348074436188
16392,-Metrics/Training(Step): loss,1610365832342,0.10327891260385513
16394,-Metrics/Training(Step): loss,1610365834464,0.10977313667535782
16396,-Metrics/Training(Step): loss,1610365836603,0.09366677701473236
16398,-Metrics/Training(Step): loss,1610365838728,0.0993439331650734
16400,-Metrics/Training(Step): loss,1610365840851,0.08656490594148636
16402,-Metrics/Training(Step): loss,1610365842960,0.09317058324813843
16404,-Metrics/Training(Step): loss,1610365845057,0.09281162917613983
16406,-Metrics/Training(Step): loss,1610365847150,0.13256880640983582
16408,-Metrics/Training(Step): loss,1610365849234,0.11059463024139404
16410,-Metrics/Training(Step): loss,1610365851350,0.10566379874944687
16412,-Metrics/Training(Step): loss,1610365853253,0.09347477555274963
16414,-Metrics/Training(Step): loss,1610365855346,0.08351539075374603
16416,-Metrics/Training(Step): loss,1610365857422,0.11821116507053375
16418,-Metrics/Training(Step): loss,1610365859515,0.11910112202167511
16420,-Metrics/Training(Step): loss,1610365861537,0.12058262526988983
16422,-Metrics/Training(Step): loss,1610365863478,0.08623209595680237
16424,-Metrics/Training(Step): loss,1610365865545,0.12121813744306564
16426,-Metrics/Training(Step): loss,1610365867583,0.08784730732440948
16428,-Metrics/Training(Step): loss,1610365880535,0.08811917155981064
16430,-Metrics/Training(Step): loss,1610365884819,0.0895136147737503
16432,-Metrics/Training(Step): loss,1610365888824,0.11739721894264221
16434,-Metrics/Training(Step): loss,1610365892755,0.081355519592762
16436,-Metrics/Training(Step): loss,1610365897017,0.08165672421455383
16438,-Metrics/Training(Step): loss,1610365901121,0.07747343927621841
16440,-Metrics/Training(Step): loss,1610365904822,0.08226614445447922
16442,-Metrics/Training(Step): loss,1610365908320,0.09883426129817963
16444,-Metrics/Training(Step): loss,1610365911720,0.1455312967300415
16446,-Metrics/Training(Step): loss,1610365915417,0.090145044028759
16448,-Metrics/Training(Step): loss,1610365918948,0.10172419250011444
16450,-Metrics/Training(Step): loss,1610365922424,0.10028108954429626
16452,-Metrics/Training(Step): loss,1610365926020,0.09408508986234665
16454,-Metrics/Training(Step): loss,1610365929619,0.10926847159862518
16456,-Metrics/Training(Step): loss,1610365933019,0.09999794512987137
16458,-Metrics/Training(Step): loss,1610365936624,0.11048325151205063
16460,-Metrics/Training(Step): loss,1610365940619,0.09966249763965607
16462,-Metrics/Training(Step): loss,1610365944419,0.10139907151460648
16464,-Metrics/Training(Step): loss,1610365948117,0.1119014099240303
16466,-Metrics/Training(Step): loss,1610365951587,0.11673364043235779
16468,-Metrics/Training(Step): loss,1610365955667,0.08779922872781754
16470,-Metrics/Training(Step): loss,1610365958719,0.09838348627090454
16472,-Metrics/Training(Step): loss,1610365962219,0.08328422158956528
16474,-Metrics/Training(Step): loss,1610365965492,0.06537965685129166
16476,-Metrics/Training(Step): loss,1610365967520,0.12429851293563843
16478,-Metrics/Training(Step): loss,1610365969637,0.08085620403289795
16480,-Metrics/Training(Step): loss,1610365971755,0.09486804157495499
16482,-Metrics/Training(Step): loss,1610365973790,0.11187571287155151
16484,-Metrics/Training(Step): loss,1610365975694,0.08013253659009933
16486,-Metrics/Training(Step): loss,1610365977789,0.11855550110340118
16488,-Metrics/Training(Step): loss,1610365979887,0.10421101748943329
16490,-Metrics/Training(Step): loss,1610365981900,0.10411984473466873
16492,-Metrics/Training(Step): loss,1610365983895,0.09462255239486694
16494,-Metrics/Training(Step): loss,1610365985953,0.0955054759979248
16496,-Metrics/Training(Step): loss,1610365987976,0.10246284306049347
16498,-Metrics/Training(Step): loss,1610365990027,0.09144209325313568
16500,-Metrics/Training(Step): loss,1610365992055,0.10295458883047104
16502,-Metrics/Training(Step): loss,1610365994116,0.08960267156362534
16504,-Metrics/Training(Step): loss,1610365996068,0.10205259919166565
16506,-Metrics/Training(Step): loss,1610365998076,0.09638462960720062
16508,-Metrics/Training(Step): loss,1610366000075,0.07966922968626022
16510,-Metrics/Training(Step): loss,1610366002122,0.09241354465484619
16512,-Metrics/Training(Step): loss,1610366004161,0.11197031289339066
16514,-Metrics/Training(Step): loss,1610366016431,0.09420483559370041
16516,-Metrics/Training(Step): loss,1610366020319,0.09181349724531174
16518,-Metrics/Training(Step): loss,1610366024121,0.11100894957780838
16520,-Metrics/Training(Step): loss,1610366028220,0.08393830060958862
16522,-Metrics/Training(Step): loss,1610366032020,0.06732345372438431
16524,-Metrics/Training(Step): loss,1610366036326,0.09418629109859467
16526,-Metrics/Training(Step): loss,1610366040450,0.08494184166193008
16528,-Metrics/Training(Step): loss,1610366044242,0.08551256358623505
16530,-Metrics/Training(Step): loss,1610366047834,0.1063147559762001
16532,-Metrics/Training(Step): loss,1610366051358,0.07999145984649658
16534,-Metrics/Training(Step): loss,1610366054837,0.09447126090526581
16536,-Metrics/Training(Step): loss,1610366058619,0.08107481151819229
16538,-Metrics/Training(Step): loss,1610366062119,0.09451306611299515
16540,-Metrics/Training(Step): loss,1610366065420,0.10257826000452042
16542,-Metrics/Training(Step): loss,1610366068658,0.09601698070764542
16544,-Metrics/Training(Step): loss,1610366072320,0.11332458257675171
16546,-Metrics/Training(Step): loss,1610366075616,0.1003328412771225
16548,-Metrics/Training(Step): loss,1610366078620,0.09167115390300751
16550,-Metrics/Training(Step): loss,1610366082458,0.10550275444984436
16552,-Metrics/Training(Step): loss,1610366086050,0.10689374059438705
16554,-Metrics/Training(Step): loss,1610366089414,0.09613370895385742
16556,-Metrics/Training(Step): loss,1610366092058,0.08110445737838745
16558,-Metrics/Training(Step): loss,1610366095317,0.07550287991762161
16560,-Metrics/Training(Step): loss,1610366098719,0.08983471989631653
16562,-Metrics/Training(Step): loss,1610366101822,0.08522351831197739
16564,-Metrics/Training(Step): loss,1610366104153,0.08891072124242783
16566,-Metrics/Training(Step): loss,1610366106165,0.11391136795282364
16568,-Metrics/Training(Step): loss,1610366108258,0.10502461344003677
16570,-Metrics/Training(Step): loss,1610366110388,0.10504083335399628
16572,-Metrics/Training(Step): loss,1610366112497,0.11640609800815582
16574,-Metrics/Training(Step): loss,1610366114619,0.068755142390728
16576,-Metrics/Training(Step): loss,1610366116728,0.09159083664417267
16578,-Metrics/Training(Step): loss,1610366118816,0.08092451095581055
16580,-Metrics/Training(Step): loss,1610366120847,0.10920853167772293
16582,-Metrics/Training(Step): loss,1610366122966,0.09261433035135269
16584,-Metrics/Training(Step): loss,1610366125092,0.07583200931549072
16586,-Metrics/Training(Step): loss,1610366127086,0.11580675840377808
16588,-Metrics/Training(Step): loss,1610366129192,0.11145129799842834
16590,-Metrics/Training(Step): loss,1610366131227,0.06923635303974152
16592,-Metrics/Training(Step): loss,1610366133340,0.10471796244382858
16594,-Metrics/Training(Step): loss,1610366135407,0.16626949608325958
16596,-Metrics/Training(Step): loss,1610366137437,0.12062849849462509
16598,-Metrics/Training(Step): loss,1610366139488,0.09151531010866165
16600,-Metrics/Training(Step): loss,1610366153025,0.10204863548278809
16602,-Metrics/Training(Step): loss,1610366157116,0.08859390765428543
16604,-Metrics/Training(Step): loss,1610366161020,0.1076461598277092
16606,-Metrics/Training(Step): loss,1610366164940,0.09118890017271042
16608,-Metrics/Training(Step): loss,1610366168824,0.08037223666906357
16610,-Metrics/Training(Step): loss,1610366172948,0.0970441922545433
16612,-Metrics/Training(Step): loss,1610366176920,0.09591532498598099
16614,-Metrics/Training(Step): loss,1610366180222,0.08080147951841354
16616,-Metrics/Training(Step): loss,1610366183919,0.07981617003679276
16618,-Metrics/Training(Step): loss,1610366187519,0.10465513169765472
16620,-Metrics/Training(Step): loss,1610366190820,0.07650921493768692
16622,-Metrics/Training(Step): loss,1610366194820,0.1200886070728302
16624,-Metrics/Training(Step): loss,1610366198321,0.11957228928804398
16626,-Metrics/Training(Step): loss,1610366201620,0.08331939578056335
16628,-Metrics/Training(Step): loss,1610366205424,0.1213202029466629
16630,-Metrics/Training(Step): loss,1610366208920,0.10573828220367432
16632,-Metrics/Training(Step): loss,1610366212969,0.08273666352033615
16634,-Metrics/Training(Step): loss,1610366216266,0.08268263190984726
16636,-Metrics/Training(Step): loss,1610366219820,0.10020890086889267
16638,-Metrics/Training(Step): loss,1610366223419,0.08572737127542496
16640,-Metrics/Training(Step): loss,1610366226620,0.10728953033685684
16642,-Metrics/Training(Step): loss,1610366229925,0.09218863397836685
16644,-Metrics/Training(Step): loss,1610366233456,0.09689433872699738
16646,-Metrics/Training(Step): loss,1610366236939,0.10939563810825348
16648,-Metrics/Training(Step): loss,1610366239414,0.10629902780056
16650,-Metrics/Training(Step): loss,1610366241490,0.09323418885469437
16652,-Metrics/Training(Step): loss,1610366243518,0.10762221366167068
16654,-Metrics/Training(Step): loss,1610366245652,0.1021486222743988
16656,-Metrics/Training(Step): loss,1610366247677,0.10491295903921127
16658,-Metrics/Training(Step): loss,1610366249807,0.09957502037286758
16660,-Metrics/Training(Step): loss,1610366251928,0.09729251265525818
16662,-Metrics/Training(Step): loss,1610366254014,0.10549398511648178
16664,-Metrics/Training(Step): loss,1610366256108,0.099410280585289
16666,-Metrics/Training(Step): loss,1610366258149,0.09525123983621597
16668,-Metrics/Training(Step): loss,1610366260263,0.11172177642583847
16670,-Metrics/Training(Step): loss,1610366262333,0.08303287625312805
16672,-Metrics/Training(Step): loss,1610366264408,0.13750262558460236
16674,-Metrics/Training(Step): loss,1610366266452,0.08318779617547989
16676,-Metrics/Training(Step): loss,1610366268512,0.09727351367473602
16678,-Metrics/Training(Step): loss,1610366270522,0.08139678835868835
16680,-Metrics/Training(Step): loss,1610366272472,0.09486493468284607
16682,-Metrics/Training(Step): loss,1610366274583,0.12326984852552414
16684,-Metrics/Training(Step): loss,1610366276563,0.1260376125574112
16686,-Metrics/Training(Step): loss,1610366289222,0.10815635323524475
16688,-Metrics/Training(Step): loss,1610366293019,0.09247293323278427
16690,-Metrics/Training(Step): loss,1610366296739,0.09057212620973587
16692,-Metrics/Training(Step): loss,1610366301320,0.10607664287090302
16694,-Metrics/Training(Step): loss,1610366305028,0.10137742012739182
16696,-Metrics/Training(Step): loss,1610366309420,0.10524503141641617
16698,-Metrics/Training(Step): loss,1610366313420,0.11175718158483505
16700,-Metrics/Training(Step): loss,1610366317319,0.091153085231781
16702,-Metrics/Training(Step): loss,1610366320942,0.10369070619344711
16704,-Metrics/Training(Step): loss,1610366324537,0.10517892241477966
16706,-Metrics/Training(Step): loss,1610366328240,0.11883250623941422
16708,-Metrics/Training(Step): loss,1610366331819,0.09747964143753052
16710,-Metrics/Training(Step): loss,1610366335902,0.10224204510450363
16712,-Metrics/Training(Step): loss,1610366339600,0.12893126904964447
16714,-Metrics/Training(Step): loss,1610366343446,0.09558624774217606
16716,-Metrics/Training(Step): loss,1610366346950,0.10268442332744598
16718,-Metrics/Training(Step): loss,1610366350720,0.09042398631572723
16720,-Metrics/Training(Step): loss,1610366354023,0.11177954077720642
16722,-Metrics/Training(Step): loss,1610366357719,0.07823516428470612
16724,-Metrics/Training(Step): loss,1610366361019,0.08476051688194275
16726,-Metrics/Training(Step): loss,1610366365017,0.12089008837938309
16728,-Metrics/Training(Step): loss,1610366368207,0.10710090398788452
16730,-Metrics/Training(Step): loss,1610366371119,0.09436667710542679
16732,-Metrics/Training(Step): loss,1610366374032,0.1120896264910698
16734,-Metrics/Training(Step): loss,1610366376569,0.08155965059995651
16736,-Metrics/Training(Step): loss,1610366378677,0.08565296977758408
16738,-Metrics/Training(Step): loss,1610366380826,0.06587698310613632
16740,-Metrics/Training(Step): loss,1610366382881,0.09281812608242035
16742,-Metrics/Training(Step): loss,1610366384906,0.10047761350870132
16744,-Metrics/Training(Step): loss,1610366386950,0.09320804476737976
16746,-Metrics/Training(Step): loss,1610366389012,0.09303635358810425
16748,-Metrics/Training(Step): loss,1610366390996,0.1000852882862091
16750,-Metrics/Training(Step): loss,1610366393080,0.12052439898252487
16752,-Metrics/Training(Step): loss,1610366395170,0.06736046820878983
16754,-Metrics/Training(Step): loss,1610366397169,0.10071969777345657
16756,-Metrics/Training(Step): loss,1610366399190,0.09886123985052109
16758,-Metrics/Training(Step): loss,1610366401059,0.09787056595087051
16760,-Metrics/Training(Step): loss,1610366403023,0.08259307593107224
16762,-Metrics/Training(Step): loss,1610366404996,0.07503780722618103
16764,-Metrics/Training(Step): loss,1610366406980,0.07784562557935715
16766,-Metrics/Training(Step): loss,1610366408988,0.07889113575220108
16768,-Metrics/Training(Step): loss,1610366411015,0.07900815457105637
16770,-Metrics/Training(Step): loss,1610366413071,0.08872386813163757
16772,-Metrics/Training(Step): loss,1610366425850,0.08138379454612732
16774,-Metrics/Training(Step): loss,1610366430120,0.11613940447568893
16776,-Metrics/Training(Step): loss,1610366434015,0.0886281281709671
16778,-Metrics/Training(Step): loss,1610366437919,0.09032335132360458
16780,-Metrics/Training(Step): loss,1610366442148,0.10667627304792404
16782,-Metrics/Training(Step): loss,1610366445239,0.09222570806741714
16784,-Metrics/Training(Step): loss,1610366448721,0.10438256710767746
16786,-Metrics/Training(Step): loss,1610366452024,0.10160692781209946
16788,-Metrics/Training(Step): loss,1610366455520,0.10109014809131622
16790,-Metrics/Training(Step): loss,1610366459225,0.11127534508705139
16792,-Metrics/Training(Step): loss,1610366462422,0.0954679548740387
16794,-Metrics/Training(Step): loss,1610366466120,0.12486572563648224
16796,-Metrics/Training(Step): loss,1610366469751,0.10439721494913101
16798,-Metrics/Training(Step): loss,1610366473683,0.08065874129533768
16800,-Metrics/Training(Step): loss,1610366477421,0.09620432555675507
16802,-Metrics/Training(Step): loss,1610366481119,0.1128680631518364
16804,-Metrics/Training(Step): loss,1610366484324,0.09434518963098526
16806,-Metrics/Training(Step): loss,1610366488120,0.10435047745704651
16808,-Metrics/Training(Step): loss,1610366491250,0.1115226000547409
16810,-Metrics/Training(Step): loss,1610366494630,0.11046347767114639
16812,-Metrics/Training(Step): loss,1610366498019,0.11737458407878876
16814,-Metrics/Training(Step): loss,1610366501319,0.0890779122710228
16816,-Metrics/Training(Step): loss,1610366504543,0.10134469717741013
16818,-Metrics/Training(Step): loss,1610366507640,0.09364628791809082
16820,-Metrics/Training(Step): loss,1610366510350,0.06951020658016205
16822,-Metrics/Training(Step): loss,1610366512634,0.10181112587451935
16824,-Metrics/Training(Step): loss,1610366514818,0.09110752493143082
16826,-Metrics/Training(Step): loss,1610366516874,0.0904582291841507
16828,-Metrics/Training(Step): loss,1610366518983,0.08745062351226807
16830,-Metrics/Training(Step): loss,1610366521095,0.05480378866195679
16832,-Metrics/Training(Step): loss,1610366523118,0.10176154226064682
16834,-Metrics/Training(Step): loss,1610366525012,0.10044652223587036
16836,-Metrics/Training(Step): loss,1610366527107,0.09391924738883972
16838,-Metrics/Training(Step): loss,1610366529133,0.1001865491271019
16840,-Metrics/Training(Step): loss,1610366531164,0.10459083318710327
16842,-Metrics/Training(Step): loss,1610366533249,0.08805807679891586
16844,-Metrics/Training(Step): loss,1610366535365,0.10572050511837006
16846,-Metrics/Training(Step): loss,1610366537318,0.10111870616674423
16848,-Metrics/Training(Step): loss,1610366539290,0.07766462862491608
16850,-Metrics/Training(Step): loss,1610366541376,0.08787339925765991
16852,-Metrics/Training(Step): loss,1610366543444,0.09578721970319748
16854,-Metrics/Training(Step): loss,1610366545478,0.05833087116479874
16856,-Metrics/Training(Step): loss,1610366547503,0.07546161115169525
16858,-Metrics/Training(Step): loss,1610366559334,0.10415620356798172
16860,-Metrics/Training(Step): loss,1610366563328,0.09644702076911926
16862,-Metrics/Training(Step): loss,1610366567341,0.09796890616416931
16864,-Metrics/Training(Step): loss,1610366571327,0.12035214155912399
16866,-Metrics/Training(Step): loss,1610366575320,0.10376888513565063
16868,-Metrics/Training(Step): loss,1610366579257,0.0926300659775734
16870,-Metrics/Training(Step): loss,1610366583318,0.1125914677977562
16872,-Metrics/Training(Step): loss,1610366587321,0.13166671991348267
16874,-Metrics/Training(Step): loss,1610366590650,0.09247457981109619
16876,-Metrics/Training(Step): loss,1610366593913,0.10284934937953949
16878,-Metrics/Training(Step): loss,1610366597121,0.11952783912420273
16880,-Metrics/Training(Step): loss,1610366600923,0.08834315091371536
16882,-Metrics/Training(Step): loss,1610366605124,0.1016644686460495
16884,-Metrics/Training(Step): loss,1610366608821,0.12753331661224365
16886,-Metrics/Training(Step): loss,1610366612943,0.11775441467761993
16888,-Metrics/Training(Step): loss,1610366616348,0.08793487399816513
16890,-Metrics/Training(Step): loss,1610366620324,0.09163063764572144
16892,-Metrics/Training(Step): loss,1610366623788,0.08593640476465225
16894,-Metrics/Training(Step): loss,1610366627330,0.1005689799785614
16896,-Metrics/Training(Step): loss,1610366630620,0.07657868415117264
16898,-Metrics/Training(Step): loss,1610366634019,0.10340075939893723
16900,-Metrics/Training(Step): loss,1610366637748,0.0915878564119339
16902,-Metrics/Training(Step): loss,1610366641238,0.10452432185411453
16904,-Metrics/Training(Step): loss,1610366644258,0.09859853982925415
16906,-Metrics/Training(Step): loss,1610366646607,0.11620168387889862
16908,-Metrics/Training(Step): loss,1610366648621,0.09646056592464447
16910,-Metrics/Training(Step): loss,1610366650625,0.11958952248096466
16912,-Metrics/Training(Step): loss,1610366652699,0.09336325526237488
16914,-Metrics/Training(Step): loss,1610366654802,0.10832902044057846
16916,-Metrics/Training(Step): loss,1610366656872,0.1129966676235199
16918,-Metrics/Training(Step): loss,1610366658960,0.09307146072387695
16920,-Metrics/Training(Step): loss,1610366661046,0.10423792898654938
16922,-Metrics/Training(Step): loss,1610366662940,0.0854782909154892
16924,-Metrics/Training(Step): loss,1610366664932,0.07834462076425552
16926,-Metrics/Training(Step): loss,1610366667019,0.08815742284059525
16928,-Metrics/Training(Step): loss,1610366668939,0.08609512448310852
16930,-Metrics/Training(Step): loss,1610366670962,0.06734417378902435
16932,-Metrics/Training(Step): loss,1610366672966,0.08401591330766678
16934,-Metrics/Training(Step): loss,1610366675062,0.09535318613052368
16936,-Metrics/Training(Step): loss,1610366677088,0.07270951569080353
16938,-Metrics/Training(Step): loss,1610366679072,0.08364775031805038
16940,-Metrics/Training(Step): loss,1610366681123,0.0809824988245964
16942,-Metrics/Training(Step): loss,1610366683145,0.10238602012395859
16944,-Metrics/Training(Step): loss,1610366695540,0.11884479969739914
16946,-Metrics/Training(Step): loss,1610366699421,0.09477682411670685
16948,-Metrics/Training(Step): loss,1610366703258,0.10041753202676773
16950,-Metrics/Training(Step): loss,1610366707426,0.08047120273113251
16952,-Metrics/Training(Step): loss,1610366711922,0.08500261604785919
16954,-Metrics/Training(Step): loss,1610366716121,0.12702447175979614
16956,-Metrics/Training(Step): loss,1610366720179,0.08527659624814987
16958,-Metrics/Training(Step): loss,1610366723921,0.09776509553194046
16960,-Metrics/Training(Step): loss,1610366727524,0.10742341727018356
16962,-Metrics/Training(Step): loss,1610366731023,0.09908686578273773
16964,-Metrics/Training(Step): loss,1610366734734,0.07681459188461304
16966,-Metrics/Training(Step): loss,1610366738131,0.11353319138288498
16968,-Metrics/Training(Step): loss,1610366741919,0.10276658087968826
16970,-Metrics/Training(Step): loss,1610366745619,0.08532018214464188
16972,-Metrics/Training(Step): loss,1610366749580,0.11525235325098038
16974,-Metrics/Training(Step): loss,1610366752643,0.10679715126752853
16976,-Metrics/Training(Step): loss,1610366756454,0.07710428535938263
16978,-Metrics/Training(Step): loss,1610366759967,0.08229124546051025
16980,-Metrics/Training(Step): loss,1610366763295,0.08150704205036163
16982,-Metrics/Training(Step): loss,1610366766958,0.0983891710639
16984,-Metrics/Training(Step): loss,1610366770682,0.08827614039182663
16986,-Metrics/Training(Step): loss,1610366774219,0.07955502718687057
16988,-Metrics/Training(Step): loss,1610366777465,0.0920817106962204
16990,-Metrics/Training(Step): loss,1610366779963,0.1061297282576561
16992,-Metrics/Training(Step): loss,1610366782241,0.0922289714217186
16994,-Metrics/Training(Step): loss,1610366784312,0.10726603865623474
16996,-Metrics/Training(Step): loss,1610366786407,0.10450849682092667
16998,-Metrics/Training(Step): loss,1610366788490,0.1034635528922081
17000,-Metrics/Training(Step): loss,1610366790554,0.1149662435054779
17002,-Metrics/Training(Step): loss,1610366792659,0.10458555817604065
17004,-Metrics/Training(Step): loss,1610366794664,0.11673635244369507
17006,-Metrics/Training(Step): loss,1610366796727,0.09043322503566742
17008,-Metrics/Training(Step): loss,1610366798644,0.08901558071374893
17010,-Metrics/Training(Step): loss,1610366800549,0.08588017523288727
17012,-Metrics/Training(Step): loss,1610366802623,0.09040296077728271
17014,-Metrics/Training(Step): loss,1610366804401,0.10765199363231659
17016,-Metrics/Training(Step): loss,1610366806301,0.07690086960792542
17018,-Metrics/Training(Step): loss,1610366808327,0.09365098923444748
17020,-Metrics/Training(Step): loss,1610366810263,0.11045784503221512
17022,-Metrics/Training(Step): loss,1610366812294,0.09637019038200378
17024,-Metrics/Training(Step): loss,1610366814314,0.08220359683036804
17026,-Metrics/Training(Step): loss,1610366816362,0.08798474073410034
17028,-Metrics/Training(Step): loss,1610366818421,0.10892767459154129
17030,-Metrics/Training(Step): loss,1610366830338,0.10164818912744522
17032,-Metrics/Training(Step): loss,1610366834419,0.0735822394490242
17034,-Metrics/Training(Step): loss,1610366838319,0.11514858156442642
17036,-Metrics/Training(Step): loss,1610366842143,0.08748228847980499
17038,-Metrics/Training(Step): loss,1610366845929,0.08991144597530365
17040,-Metrics/Training(Step): loss,1610366849756,0.08541253209114075
17042,-Metrics/Training(Step): loss,1610366853620,0.1143101304769516
17044,-Metrics/Training(Step): loss,1610366857041,0.06620898842811584
17046,-Metrics/Training(Step): loss,1610366860926,0.12056330591440201
17048,-Metrics/Training(Step): loss,1610366864419,0.07518111914396286
17050,-Metrics/Training(Step): loss,1610366867933,0.06736761331558228
17052,-Metrics/Training(Step): loss,1610366871332,0.08729052543640137
17054,-Metrics/Training(Step): loss,1610366875098,0.08628086745738983
17056,-Metrics/Training(Step): loss,1610366878569,0.08343543857336044
17058,-Metrics/Training(Step): loss,1610366882228,0.0775519385933876
17060,-Metrics/Training(Step): loss,1610366886019,0.11443042010068893
17062,-Metrics/Training(Step): loss,1610366889619,0.10284870117902756
17064,-Metrics/Training(Step): loss,1610366893542,0.10504002124071121
17066,-Metrics/Training(Step): loss,1610366897220,0.12237311899662018
17068,-Metrics/Training(Step): loss,1610366900242,0.10505971312522888
17070,-Metrics/Training(Step): loss,1610366903380,0.10318306088447571
17072,-Metrics/Training(Step): loss,1610366907268,0.11732061207294464
17074,-Metrics/Training(Step): loss,1610366910368,0.09802500903606415
17076,-Metrics/Training(Step): loss,1610366913261,0.10066806524991989
17078,-Metrics/Training(Step): loss,1610366915745,0.09014678746461868
17080,-Metrics/Training(Step): loss,1610366917963,0.12088853120803833
17082,-Metrics/Training(Step): loss,1610366920134,0.07333371788263321
17084,-Metrics/Training(Step): loss,1610366922203,0.10686442255973816
17086,-Metrics/Training(Step): loss,1610366924294,0.08547600358724594
17088,-Metrics/Training(Step): loss,1610366926392,0.10336057096719742
17090,-Metrics/Training(Step): loss,1610366928489,0.10973816365003586
17092,-Metrics/Training(Step): loss,1610366930588,0.08298154920339584
17094,-Metrics/Training(Step): loss,1610366932589,0.10588094592094421
17096,-Metrics/Training(Step): loss,1610366934595,0.0829077810049057
17098,-Metrics/Training(Step): loss,1610366936572,0.0804152563214302
17100,-Metrics/Training(Step): loss,1610366938492,0.10694853216409683
17102,-Metrics/Training(Step): loss,1610366940592,0.10343613475561142
17104,-Metrics/Training(Step): loss,1610366942612,0.09086670726537704
17106,-Metrics/Training(Step): loss,1610366944637,0.09949399530887604
17108,-Metrics/Training(Step): loss,1610366946559,0.09452483057975769
17110,-Metrics/Training(Step): loss,1610366948525,0.07019416242837906
17112,-Metrics/Training(Step): loss,1610366950608,0.10512737929821014
17114,-Metrics/Training(Step): loss,1610366952633,0.09878846257925034
17116,-Metrics/Training(Step): loss,1610366964622,0.09592100977897644
17118,-Metrics/Training(Step): loss,1610366968446,0.09414758533239365
17120,-Metrics/Training(Step): loss,1610366972421,0.08618281781673431
17122,-Metrics/Training(Step): loss,1610366976320,0.09068235009908676
17124,-Metrics/Training(Step): loss,1610366980319,0.09762188047170639
17126,-Metrics/Training(Step): loss,1610366984136,0.09259378910064697
17128,-Metrics/Training(Step): loss,1610366988319,0.09095701575279236
17130,-Metrics/Training(Step): loss,1610366991955,0.11148624867200851
17132,-Metrics/Training(Step): loss,1610366995919,0.07166861742734909
17134,-Metrics/Training(Step): loss,1610366998981,0.10921674966812134
17136,-Metrics/Training(Step): loss,1610367002889,0.10403378307819366
17138,-Metrics/Training(Step): loss,1610367006722,0.09581886231899261
17140,-Metrics/Training(Step): loss,1610367009822,0.09194232523441315
17142,-Metrics/Training(Step): loss,1610367012758,0.09745892137289047
17144,-Metrics/Training(Step): loss,1610367016329,0.10820809751749039
17146,-Metrics/Training(Step): loss,1610367020017,0.10916627943515778
17148,-Metrics/Training(Step): loss,1610367023620,0.10997921228408813
17150,-Metrics/Training(Step): loss,1610367027085,0.12341322749853134
17152,-Metrics/Training(Step): loss,1610367030456,0.08230220526456833
17154,-Metrics/Training(Step): loss,1610367034316,0.09107460826635361
17156,-Metrics/Training(Step): loss,1610367037973,0.08711753785610199
17158,-Metrics/Training(Step): loss,1610367041819,0.10845962911844254
17160,-Metrics/Training(Step): loss,1610367044648,0.10061785578727722
17162,-Metrics/Training(Step): loss,1610367047427,0.09904884546995163
17164,-Metrics/Training(Step): loss,1610367049768,0.08512324839830399
17166,-Metrics/Training(Step): loss,1610367051921,0.08971879631280899
17168,-Metrics/Training(Step): loss,1610367053996,0.09002391993999481
17170,-Metrics/Training(Step): loss,1610367055985,0.13007965683937073
17172,-Metrics/Training(Step): loss,1610367058093,0.10919968038797379
17174,-Metrics/Training(Step): loss,1610367060185,0.10979457199573517
17176,-Metrics/Training(Step): loss,1610367062267,0.08869452774524689
17178,-Metrics/Training(Step): loss,1610367064189,0.08784537017345428
17180,-Metrics/Training(Step): loss,1610367066271,0.09316354990005493
17182,-Metrics/Training(Step): loss,1610367068302,0.08977287262678146
17184,-Metrics/Training(Step): loss,1610367070330,0.11091833561658859
17186,-Metrics/Training(Step): loss,1610367072250,0.08176430314779282
17188,-Metrics/Training(Step): loss,1610367074351,0.06744977831840515
17190,-Metrics/Training(Step): loss,1610367076299,0.09797928482294083
17192,-Metrics/Training(Step): loss,1610367078335,0.11804597079753876
17194,-Metrics/Training(Step): loss,1610367080371,0.07369126379489899
17196,-Metrics/Training(Step): loss,1610367082339,0.06392741203308105
17198,-Metrics/Training(Step): loss,1610367084302,0.11061020940542221
17200,-Metrics/Training(Step): loss,1610367086239,0.09210269898176193
17202,-Metrics/Training(Step): loss,1610367098335,0.09963809698820114
17204,-Metrics/Training(Step): loss,1610367102519,0.10757971554994583
17206,-Metrics/Training(Step): loss,1610367106521,0.10114629566669464
17208,-Metrics/Training(Step): loss,1610367110621,0.09904744476079941
17210,-Metrics/Training(Step): loss,1610367114421,0.1015695109963417
17212,-Metrics/Training(Step): loss,1610367118131,0.11518843472003937
17214,-Metrics/Training(Step): loss,1610367121930,0.11685799807310104
17216,-Metrics/Training(Step): loss,1610367125320,0.06529746204614639
17218,-Metrics/Training(Step): loss,1610367128817,0.09826952964067459
17220,-Metrics/Training(Step): loss,1610367132422,0.09895388036966324
17222,-Metrics/Training(Step): loss,1610367135355,0.1094743087887764
17224,-Metrics/Training(Step): loss,1610367138939,0.07590394467115402
17226,-Metrics/Training(Step): loss,1610367142919,0.09651458263397217
17228,-Metrics/Training(Step): loss,1610367146746,0.08350291103124619
17230,-Metrics/Training(Step): loss,1610367150515,0.09266991913318634
17232,-Metrics/Training(Step): loss,1610367153898,0.09372755140066147
17234,-Metrics/Training(Step): loss,1610367157558,0.0779627114534378
17236,-Metrics/Training(Step): loss,1610367161323,0.07766899466514587
17238,-Metrics/Training(Step): loss,1610367164716,0.09591308236122131
17240,-Metrics/Training(Step): loss,1610367168339,0.07186879217624664
17242,-Metrics/Training(Step): loss,1610367171919,0.09379789978265762
17244,-Metrics/Training(Step): loss,1610367175394,0.0991610586643219
17246,-Metrics/Training(Step): loss,1610367178695,0.09241440147161484
17248,-Metrics/Training(Step): loss,1610367180955,0.09692692756652832
17250,-Metrics/Training(Step): loss,1610367183157,0.1117769405245781
17252,-Metrics/Training(Step): loss,1610367185170,0.05881482735276222
17254,-Metrics/Training(Step): loss,1610367187125,0.10328203439712524
17256,-Metrics/Training(Step): loss,1610367189194,0.07046898454427719
17258,-Metrics/Training(Step): loss,1610367191281,0.08817720413208008
17260,-Metrics/Training(Step): loss,1610367193390,0.0687485858798027
17262,-Metrics/Training(Step): loss,1610367195499,0.11041168123483658
17264,-Metrics/Training(Step): loss,1610367197554,0.08338888734579086
17266,-Metrics/Training(Step): loss,1610367199655,0.0895921140909195
17268,-Metrics/Training(Step): loss,1610367201731,0.08867694437503815
17270,-Metrics/Training(Step): loss,1610367203806,0.09063801169395447
17272,-Metrics/Training(Step): loss,1610367205884,0.0907498225569725
17274,-Metrics/Training(Step): loss,1610367207705,0.10500751435756683
17276,-Metrics/Training(Step): loss,1610367209573,0.10321594774723053
17278,-Metrics/Training(Step): loss,1610367211591,0.10274137556552887
17280,-Metrics/Training(Step): loss,1610367213646,0.10663645714521408
17282,-Metrics/Training(Step): loss,1610367215663,0.11918100714683533
17284,-Metrics/Training(Step): loss,1610367217725,0.08006493747234344
17286,-Metrics/Training(Step): loss,1610367219784,0.08451923727989197
17288,-Metrics/Training(Step): loss,1610367232127,0.10031214356422424
17290,-Metrics/Training(Step): loss,1610367236319,0.08758515119552612
17292,-Metrics/Training(Step): loss,1610367240322,0.09762876480817795
17294,-Metrics/Training(Step): loss,1610367244340,0.11777780205011368
17296,-Metrics/Training(Step): loss,1610367248120,0.11123389005661011
17298,-Metrics/Training(Step): loss,1610367251620,0.10244473069906235
17300,-Metrics/Training(Step): loss,1610367255581,0.07946127653121948
17302,-Metrics/Training(Step): loss,1610367259530,0.09650977700948715
17304,-Metrics/Training(Step): loss,1610367263208,0.10615596920251846
17306,-Metrics/Training(Step): loss,1610367266921,0.10157330334186554
17308,-Metrics/Training(Step): loss,1610367271019,0.06612320989370346
17310,-Metrics/Training(Step): loss,1610367274620,0.11399886757135391
17312,-Metrics/Training(Step): loss,1610367278119,0.09638254344463348
17314,-Metrics/Training(Step): loss,1610367281937,0.10277684777975082
17316,-Metrics/Training(Step): loss,1610367285458,0.08987069875001907
17318,-Metrics/Training(Step): loss,1610367288642,0.09754228591918945
17320,-Metrics/Training(Step): loss,1610367292118,0.08050081133842468
17322,-Metrics/Training(Step): loss,1610367295543,0.09125878661870956
17324,-Metrics/Training(Step): loss,1610367298915,0.08709952235221863
17326,-Metrics/Training(Step): loss,1610367302715,0.10095754265785217
17328,-Metrics/Training(Step): loss,1610367306714,0.07857028394937515
17330,-Metrics/Training(Step): loss,1610367310120,0.0947205200791359
17332,-Metrics/Training(Step): loss,1610367313645,0.08078310638666153
17334,-Metrics/Training(Step): loss,1610367316145,0.09438292682170868
17336,-Metrics/Training(Step): loss,1610367318345,0.1391000747680664
17338,-Metrics/Training(Step): loss,1610367320402,0.09512928128242493
17340,-Metrics/Training(Step): loss,1610367322471,0.11517439037561417
17342,-Metrics/Training(Step): loss,1610367324543,0.09879936277866364
17344,-Metrics/Training(Step): loss,1610367326643,0.11469809710979462
17346,-Metrics/Training(Step): loss,1610367328727,0.12376846373081207
17348,-Metrics/Training(Step): loss,1610367330765,0.08946549892425537
17350,-Metrics/Training(Step): loss,1610367332730,0.10447880625724792
17352,-Metrics/Training(Step): loss,1610367334820,0.07413238286972046
17354,-Metrics/Training(Step): loss,1610367336905,0.08859192579984665
17356,-Metrics/Training(Step): loss,1610367338764,0.08407758176326752
17358,-Metrics/Training(Step): loss,1610367340763,0.1018974706530571
17360,-Metrics/Training(Step): loss,1610367342596,0.11421596258878708
17362,-Metrics/Training(Step): loss,1610367344596,0.09657920151948929
17364,-Metrics/Training(Step): loss,1610367346591,0.09726684540510178
17366,-Metrics/Training(Step): loss,1610367348634,0.10076850652694702
17368,-Metrics/Training(Step): loss,1610367350702,0.10414120554924011
17370,-Metrics/Training(Step): loss,1610367352736,0.11282162368297577
17372,-Metrics/Training(Step): loss,1610367354653,0.10780393332242966
17374,-Metrics/Training(Step): loss,1610367367026,0.09237352758646011
17376,-Metrics/Training(Step): loss,1610367371020,0.07816480100154877
17378,-Metrics/Training(Step): loss,1610367374825,0.06828263401985168
17380,-Metrics/Training(Step): loss,1610367378940,0.0693819671869278
17382,-Metrics/Training(Step): loss,1610367383019,0.09479881078004837
17384,-Metrics/Training(Step): loss,1610367386620,0.11228592693805695
17386,-Metrics/Training(Step): loss,1610367390019,0.09337401390075684
17388,-Metrics/Training(Step): loss,1610367393420,0.0882129818201065
17390,-Metrics/Training(Step): loss,1610367396921,0.09526073932647705
17392,-Metrics/Training(Step): loss,1610367400722,0.09046140313148499
17394,-Metrics/Training(Step): loss,1610367404660,0.08485670387744904
17396,-Metrics/Training(Step): loss,1610367408265,0.10233717411756516
17398,-Metrics/Training(Step): loss,1610367412020,0.09124751389026642
17400,-Metrics/Training(Step): loss,1610367415420,0.09533550590276718
17402,-Metrics/Training(Step): loss,1610367418828,0.07781218737363815
17404,-Metrics/Training(Step): loss,1610367422521,0.07994998246431351
17406,-Metrics/Training(Step): loss,1610367425840,0.11070835590362549
17408,-Metrics/Training(Step): loss,1610367429445,0.07936454564332962
17410,-Metrics/Training(Step): loss,1610367432919,0.10035749524831772
17412,-Metrics/Training(Step): loss,1610367436244,0.09499257057905197
17414,-Metrics/Training(Step): loss,1610367440163,0.10984330624341965
17416,-Metrics/Training(Step): loss,1610367444074,0.08553255349397659
17418,-Metrics/Training(Step): loss,1610367447156,0.1075102910399437
17420,-Metrics/Training(Step): loss,1610367449795,0.10887143015861511
17422,-Metrics/Training(Step): loss,1610367451986,0.07898486405611038
17424,-Metrics/Training(Step): loss,1610367453944,0.08957093954086304
17426,-Metrics/Training(Step): loss,1610367456012,0.10043729841709137
17428,-Metrics/Training(Step): loss,1610367458010,0.09325741976499557
17430,-Metrics/Training(Step): loss,1610367460075,0.11336451768875122
17432,-Metrics/Training(Step): loss,1610367462173,0.09931852668523788
17434,-Metrics/Training(Step): loss,1610367464176,0.09679935872554779
17436,-Metrics/Training(Step): loss,1610367466185,0.1196601390838623
17438,-Metrics/Training(Step): loss,1610367468183,0.08720120042562485
17440,-Metrics/Training(Step): loss,1610367470337,0.08778204023838043
17442,-Metrics/Training(Step): loss,1610367472403,0.10064937174320221
17444,-Metrics/Training(Step): loss,1610367474450,0.08828292042016983
17446,-Metrics/Training(Step): loss,1610367476447,0.08130741864442825
17448,-Metrics/Training(Step): loss,1610367478502,0.09445184469223022
17450,-Metrics/Training(Step): loss,1610367480482,0.08787565678358078
17452,-Metrics/Training(Step): loss,1610367482457,0.1206662654876709
17454,-Metrics/Training(Step): loss,1610367484482,0.11832503974437714
17456,-Metrics/Training(Step): loss,1610367486530,0.09974584728479385
17458,-Metrics/Training(Step): loss,1610367488544,0.10163159668445587
17460,-Metrics/Training(Step): loss,1610367501130,0.11921270936727524
17462,-Metrics/Training(Step): loss,1610367505220,0.08470362424850464
17464,-Metrics/Training(Step): loss,1610367508819,0.0891232043504715
17466,-Metrics/Training(Step): loss,1610367512427,0.0822986587882042
17468,-Metrics/Training(Step): loss,1610367516145,0.10269176214933395
17470,-Metrics/Training(Step): loss,1610367519885,0.08060821890830994
17472,-Metrics/Training(Step): loss,1610367523245,0.10352609306573868
17474,-Metrics/Training(Step): loss,1610367527419,0.08673569560050964
17476,-Metrics/Training(Step): loss,1610367531421,0.08430933952331543
17478,-Metrics/Training(Step): loss,1610367535422,0.09762954711914062
17480,-Metrics/Training(Step): loss,1610367539216,0.09103502333164215
17482,-Metrics/Training(Step): loss,1610367543016,0.09526656568050385
17484,-Metrics/Training(Step): loss,1610367546220,0.09935560822486877
17486,-Metrics/Training(Step): loss,1610367549719,0.1033845916390419
17488,-Metrics/Training(Step): loss,1610367553558,0.10765668749809265
17490,-Metrics/Training(Step): loss,1610367557120,0.0959232971072197
17492,-Metrics/Training(Step): loss,1610367560518,0.09277848154306412
17494,-Metrics/Training(Step): loss,1610367563742,0.09224342554807663
17496,-Metrics/Training(Step): loss,1610367567255,0.08668683469295502
17498,-Metrics/Training(Step): loss,1610367570592,0.11335893720388412
17500,-Metrics/Training(Step): loss,1610367574421,0.0877230167388916
17502,-Metrics/Training(Step): loss,1610367577359,0.09926789253950119
17504,-Metrics/Training(Step): loss,1610367580219,0.09933624416589737
17506,-Metrics/Training(Step): loss,1610367583351,0.10362772643566132
17508,-Metrics/Training(Step): loss,1610367585800,0.11894746869802475
17510,-Metrics/Training(Step): loss,1610367587853,0.1001368910074234
17512,-Metrics/Training(Step): loss,1610367589922,0.0946253165602684
17514,-Metrics/Training(Step): loss,1610367592008,0.08336843550205231
17516,-Metrics/Training(Step): loss,1610367594130,0.10282871127128601
17518,-Metrics/Training(Step): loss,1610367596244,0.0853266492486
17520,-Metrics/Training(Step): loss,1610367598334,0.10837969928979874
17522,-Metrics/Training(Step): loss,1610367600341,0.09357325732707977
17524,-Metrics/Training(Step): loss,1610367602607,0.10310390591621399
17526,-Metrics/Training(Step): loss,1610367604780,0.10210146754980087
17528,-Metrics/Training(Step): loss,1610367606974,0.10026650130748749
17530,-Metrics/Training(Step): loss,1610367608960,0.07953056693077087
17532,-Metrics/Training(Step): loss,1610367611041,0.06989981979131699
17534,-Metrics/Training(Step): loss,1610367613058,0.09483422338962555
17536,-Metrics/Training(Step): loss,1610367615171,0.0823197066783905
17538,-Metrics/Training(Step): loss,1610367617126,0.08750966191291809
17540,-Metrics/Training(Step): loss,1610367619119,0.09132499992847443
17542,-Metrics/Training(Step): loss,1610367621165,0.08504682779312134
17544,-Metrics/Training(Step): loss,1610367623185,0.0942043885588646
17546,-Metrics/Training(Step): loss,1610367635226,0.09298200160264969
17548,-Metrics/Training(Step): loss,1610367639520,0.11095038056373596
17550,-Metrics/Training(Step): loss,1610367643224,0.08580537140369415
17552,-Metrics/Training(Step): loss,1610367647334,0.09313934296369553
17554,-Metrics/Training(Step): loss,1610367651531,0.12175086885690689
17556,-Metrics/Training(Step): loss,1610367655419,0.09656332433223724
17558,-Metrics/Training(Step): loss,1610367659370,0.07032658159732819
17560,-Metrics/Training(Step): loss,1610367663219,0.1029285416007042
17562,-Metrics/Training(Step): loss,1610367667020,0.09610480070114136
17564,-Metrics/Training(Step): loss,1610367671220,0.08414522558450699
17566,-Metrics/Training(Step): loss,1610367674519,0.0874163880944252
17568,-Metrics/Training(Step): loss,1610367678119,0.07674749940633774
17570,-Metrics/Training(Step): loss,1610367681716,0.08919408917427063
17572,-Metrics/Training(Step): loss,1610367685186,0.09517978131771088
17574,-Metrics/Training(Step): loss,1610367688435,0.07809961587190628
17576,-Metrics/Training(Step): loss,1610367692020,0.11739519238471985
17578,-Metrics/Training(Step): loss,1610367695758,0.11862077564001083
17580,-Metrics/Training(Step): loss,1610367699213,0.09312843531370163
17582,-Metrics/Training(Step): loss,1610367702620,0.09562119841575623
17584,-Metrics/Training(Step): loss,1610367706392,0.09378615766763687
17586,-Metrics/Training(Step): loss,1610367709925,0.07895217090845108
17588,-Metrics/Training(Step): loss,1610367713519,0.10096067935228348
17590,-Metrics/Training(Step): loss,1610367716774,0.11094782501459122
17592,-Metrics/Training(Step): loss,1610367719171,0.07297082245349884
17594,-Metrics/Training(Step): loss,1610367721394,0.11319902539253235
17596,-Metrics/Training(Step): loss,1610367723548,0.07571566849946976
17598,-Metrics/Training(Step): loss,1610367725649,0.10239828377962112
17600,-Metrics/Training(Step): loss,1610367727695,0.0927320197224617
17602,-Metrics/Training(Step): loss,1610367729699,0.1033472940325737
17604,-Metrics/Training(Step): loss,1610367731781,0.10855923593044281
17606,-Metrics/Training(Step): loss,1610367733856,0.09159820526838303
17608,-Metrics/Training(Step): loss,1610367735799,0.10235346853733063
17610,-Metrics/Training(Step): loss,1610367737803,0.10170496255159378
17612,-Metrics/Training(Step): loss,1610367739729,0.1017957255244255
17614,-Metrics/Training(Step): loss,1610367741786,0.08089018613100052
17616,-Metrics/Training(Step): loss,1610367743894,0.10267132520675659
17618,-Metrics/Training(Step): loss,1610367745941,0.1144469827413559
17620,-Metrics/Training(Step): loss,1610367747869,0.08421767503023148
17622,-Metrics/Training(Step): loss,1610367749850,0.10598089545965195
17624,-Metrics/Training(Step): loss,1610367751910,0.12006422132253647
17626,-Metrics/Training(Step): loss,1610367753971,0.096910260617733
17628,-Metrics/Training(Step): loss,1610367756044,0.1018453910946846
17630,-Metrics/Training(Step): loss,1610367758052,0.08513133227825165
17632,-Metrics/Training(Step): loss,1610367770953,0.06952713429927826
17634,-Metrics/Training(Step): loss,1610367775020,0.07190726697444916
17636,-Metrics/Training(Step): loss,1610367779252,0.10114999115467072
17638,-Metrics/Training(Step): loss,1610367783620,0.09473541378974915
17640,-Metrics/Training(Step): loss,1610367787748,0.07858656346797943
17642,-Metrics/Training(Step): loss,1610367791536,0.09334910660982132
17644,-Metrics/Training(Step): loss,1610367795533,0.11269962787628174
17646,-Metrics/Training(Step): loss,1610367799240,0.09288735687732697
17648,-Metrics/Training(Step): loss,1610367802921,0.09628957509994507
17650,-Metrics/Training(Step): loss,1610367806721,0.0902128517627716
17652,-Metrics/Training(Step): loss,1610367810429,0.09624248743057251
17654,-Metrics/Training(Step): loss,1610367813925,0.10360287874937057
17656,-Metrics/Training(Step): loss,1610367817620,0.11827368289232254
17658,-Metrics/Training(Step): loss,1610367821015,0.09942144900560379
17660,-Metrics/Training(Step): loss,1610367824750,0.09916238486766815
17662,-Metrics/Training(Step): loss,1610367828343,0.09626847505569458
17664,-Metrics/Training(Step): loss,1610367831534,0.09683018922805786
17666,-Metrics/Training(Step): loss,1610367835164,0.11322691291570663
17668,-Metrics/Training(Step): loss,1610367838575,0.09212042391300201
17670,-Metrics/Training(Step): loss,1610367842064,0.08231811970472336
17672,-Metrics/Training(Step): loss,1610367845772,0.09146169573068619
17674,-Metrics/Training(Step): loss,1610367849446,0.08835898339748383
17676,-Metrics/Training(Step): loss,1610367851805,0.11532250791788101
17678,-Metrics/Training(Step): loss,1610367853940,0.08805514127016068
17680,-Metrics/Training(Step): loss,1610367856066,0.09419147670269012
17682,-Metrics/Training(Step): loss,1610367858124,0.07945600152015686
17684,-Metrics/Training(Step): loss,1610367860230,0.09769967198371887
17686,-Metrics/Training(Step): loss,1610367862226,0.09027859568595886
17688,-Metrics/Training(Step): loss,1610367864315,0.08376692235469818
17690,-Metrics/Training(Step): loss,1610367866386,0.09066034108400345
17692,-Metrics/Training(Step): loss,1610367868451,0.09633082151412964
17694,-Metrics/Training(Step): loss,1610367870336,0.10453716665506363
17696,-Metrics/Training(Step): loss,1610367872308,0.10579855740070343
17698,-Metrics/Training(Step): loss,1610367874138,0.10838130861520767
17700,-Metrics/Training(Step): loss,1610367876175,0.09050866961479187
17702,-Metrics/Training(Step): loss,1610367878118,0.07312246412038803
17704,-Metrics/Training(Step): loss,1610367880043,0.10953299701213837
17706,-Metrics/Training(Step): loss,1610367882066,0.1117289662361145
17708,-Metrics/Training(Step): loss,1610367883889,0.09295736998319626
17710,-Metrics/Training(Step): loss,1610367885891,0.09566283971071243
17712,-Metrics/Training(Step): loss,1610367887920,0.10312552005052567
17714,-Metrics/Training(Step): loss,1610367889962,0.11161647737026215
17716,-Metrics/Training(Step): loss,1610367891985,0.08144047111272812
17718,-Metrics/Training(Step): loss,1610367904122,0.11251798272132874
17720,-Metrics/Training(Step): loss,1610367907923,0.08510347455739975
17722,-Metrics/Training(Step): loss,1610367911918,0.09808976948261261
17724,-Metrics/Training(Step): loss,1610367916057,0.10433098673820496
17726,-Metrics/Training(Step): loss,1610367920221,0.08542521297931671
17728,-Metrics/Training(Step): loss,1610367923920,0.07213115692138672
17730,-Metrics/Training(Step): loss,1610367928020,0.10459490865468979
17732,-Metrics/Training(Step): loss,1610367932320,0.11527734249830246
17734,-Metrics/Training(Step): loss,1610367936423,0.11016549915075302
17736,-Metrics/Training(Step): loss,1610367939982,0.09944993257522583
17738,-Metrics/Training(Step): loss,1610367943419,0.07613584399223328
17740,-Metrics/Training(Step): loss,1610367947121,0.09624017030000687
17742,-Metrics/Training(Step): loss,1610367950957,0.08796447515487671
17744,-Metrics/Training(Step): loss,1610367954268,0.08760469406843185
17746,-Metrics/Training(Step): loss,1610367957792,0.11334170401096344
17748,-Metrics/Training(Step): loss,1610367961420,0.10270227491855621
17750,-Metrics/Training(Step): loss,1610367964460,0.09858541935682297
17752,-Metrics/Training(Step): loss,1610367967759,0.07651516050100327
17754,-Metrics/Training(Step): loss,1610367970973,0.08613338321447372
17756,-Metrics/Training(Step): loss,1610367974122,0.08604594320058823
17758,-Metrics/Training(Step): loss,1610367977654,0.09913387894630432
17760,-Metrics/Training(Step): loss,1610367980916,0.10268832743167877
17762,-Metrics/Training(Step): loss,1610367984590,0.12440283596515656
17764,-Metrics/Training(Step): loss,1610367987267,0.0896836668252945
17766,-Metrics/Training(Step): loss,1610367989684,0.10258596390485764
17768,-Metrics/Training(Step): loss,1610367991831,0.09206205606460571
17770,-Metrics/Training(Step): loss,1610367993912,0.09257014840841293
17772,-Metrics/Training(Step): loss,1610367995978,0.08912663906812668
17774,-Metrics/Training(Step): loss,1610367997991,0.10912904888391495
17776,-Metrics/Training(Step): loss,1610368000002,0.10214096307754517
17778,-Metrics/Training(Step): loss,1610368002044,0.11188840866088867
17780,-Metrics/Training(Step): loss,1610368004087,0.08109202235937119
17782,-Metrics/Training(Step): loss,1610368006080,0.08473379164934158
17784,-Metrics/Training(Step): loss,1610368008164,0.088471420109272
17786,-Metrics/Training(Step): loss,1610368010177,0.07745889574289322
17788,-Metrics/Training(Step): loss,1610368012228,0.09883324801921844
17790,-Metrics/Training(Step): loss,1610368014265,0.11719487607479095
17792,-Metrics/Training(Step): loss,1610368016314,0.09512341022491455
17794,-Metrics/Training(Step): loss,1610368018317,0.09689825773239136
17796,-Metrics/Training(Step): loss,1610368020280,0.12195475399494171
17798,-Metrics/Training(Step): loss,1610368022287,0.1027337834239006
17800,-Metrics/Training(Step): loss,1610368024311,0.0924607440829277
17802,-Metrics/Training(Step): loss,1610368026370,0.13151821494102478
17804,-Metrics/Training(Step): loss,1610368038436,0.09234856814146042
17806,-Metrics/Training(Step): loss,1610368042319,0.09720644354820251
17808,-Metrics/Training(Step): loss,1610368046020,0.08416669070720673
17810,-Metrics/Training(Step): loss,1610368050316,0.10301195830106735
17812,-Metrics/Training(Step): loss,1610368054443,0.09669665992259979
17814,-Metrics/Training(Step): loss,1610368058743,0.11188577860593796
17816,-Metrics/Training(Step): loss,1610368062820,0.12264091521501541
17818,-Metrics/Training(Step): loss,1610368066831,0.10323735326528549
17820,-Metrics/Training(Step): loss,1610368070521,0.0982397198677063
17822,-Metrics/Training(Step): loss,1610368074109,0.09787008911371231
17824,-Metrics/Training(Step): loss,1610368077320,0.11216763406991959
17826,-Metrics/Training(Step): loss,1610368081195,0.10803894698619843
17828,-Metrics/Training(Step): loss,1610368085351,0.06418488174676895
17830,-Metrics/Training(Step): loss,1610368088810,0.10401161015033722
17832,-Metrics/Training(Step): loss,1610368092324,0.07490145415067673
17834,-Metrics/Training(Step): loss,1610368095838,0.1215633898973465
17836,-Metrics/Training(Step): loss,1610368099123,0.08040432631969452
17838,-Metrics/Training(Step): loss,1610368102449,0.0833759680390358
17840,-Metrics/Training(Step): loss,1610368106046,0.10001107305288315
17842,-Metrics/Training(Step): loss,1610368109368,0.0947771668434143
17844,-Metrics/Training(Step): loss,1610368112619,0.12773960828781128
17846,-Metrics/Training(Step): loss,1610368115746,0.10038655251264572
17848,-Metrics/Training(Step): loss,1610368119167,0.08236273378133774
17850,-Metrics/Training(Step): loss,1610368122025,0.08872794359922409
17852,-Metrics/Training(Step): loss,1610368124357,0.09832233935594559
17854,-Metrics/Training(Step): loss,1610368126355,0.0923590436577797
17856,-Metrics/Training(Step): loss,1610368128348,0.12352952361106873
17858,-Metrics/Training(Step): loss,1610368130349,0.08003471791744232
17860,-Metrics/Training(Step): loss,1610368132451,0.0973711907863617
17862,-Metrics/Training(Step): loss,1610368134561,0.10567726194858551
17864,-Metrics/Training(Step): loss,1610368136584,0.07811728864908218
17866,-Metrics/Training(Step): loss,1610368138571,0.10215996950864792
17868,-Metrics/Training(Step): loss,1610368140633,0.09822959452867508
17870,-Metrics/Training(Step): loss,1610368142620,0.12553875148296356
17872,-Metrics/Training(Step): loss,1610368144661,0.08921335637569427
17874,-Metrics/Training(Step): loss,1610368146740,0.06827002018690109
17876,-Metrics/Training(Step): loss,1610368148812,0.08166896551847458
17878,-Metrics/Training(Step): loss,1610368150624,0.09226958453655243
17880,-Metrics/Training(Step): loss,1610368152610,0.08691372722387314
17882,-Metrics/Training(Step): loss,1610368154672,0.09708192199468613
17884,-Metrics/Training(Step): loss,1610368156764,0.09856726229190826
17886,-Metrics/Training(Step): loss,1610368158712,0.0906483605504036
17888,-Metrics/Training(Step): loss,1610368160744,0.09063735604286194
17890,-Metrics/Training(Step): loss,1610368172637,0.09838687628507614
17892,-Metrics/Training(Step): loss,1610368176847,0.08125084638595581
17894,-Metrics/Training(Step): loss,1610368180719,0.09982279688119888
17896,-Metrics/Training(Step): loss,1610368184722,0.10216045379638672
17898,-Metrics/Training(Step): loss,1610368189338,0.09076643735170364
17900,-Metrics/Training(Step): loss,1610368193919,0.11673511564731598
17902,-Metrics/Training(Step): loss,1610368197917,0.09416419267654419
17904,-Metrics/Training(Step): loss,1610368201920,0.11262251436710358
17906,-Metrics/Training(Step): loss,1610368205819,0.10009534657001495
17908,-Metrics/Training(Step): loss,1610368209115,0.09568363428115845
17910,-Metrics/Training(Step): loss,1610368212419,0.09746086597442627
17912,-Metrics/Training(Step): loss,1610368216192,0.08522409200668335
17914,-Metrics/Training(Step): loss,1610368219406,0.09700127691030502
17916,-Metrics/Training(Step): loss,1610368222827,0.10172273963689804
17918,-Metrics/Training(Step): loss,1610368226297,0.07987185567617416
17920,-Metrics/Training(Step): loss,1610368229341,0.0902111828327179
17922,-Metrics/Training(Step): loss,1610368232520,0.07702203840017319
17924,-Metrics/Training(Step): loss,1610368235819,0.07909214496612549
17926,-Metrics/Training(Step): loss,1610368239533,0.09064919501543045
17928,-Metrics/Training(Step): loss,1610368242675,0.09062942862510681
17930,-Metrics/Training(Step): loss,1610368246534,0.09688112884759903
17932,-Metrics/Training(Step): loss,1610368250041,0.09429928660392761
17934,-Metrics/Training(Step): loss,1610368253342,0.10644195228815079
17936,-Metrics/Training(Step): loss,1610368255992,0.07702884823083878
17938,-Metrics/Training(Step): loss,1610368258319,0.1033872589468956
17940,-Metrics/Training(Step): loss,1610368260418,0.07855144888162613
17942,-Metrics/Training(Step): loss,1610368262501,0.1022639125585556
17944,-Metrics/Training(Step): loss,1610368264582,0.1161058321595192
17946,-Metrics/Training(Step): loss,1610368266680,0.0947333499789238
17948,-Metrics/Training(Step): loss,1610368268757,0.08354617655277252
17950,-Metrics/Training(Step): loss,1610368270694,0.11209557205438614
17952,-Metrics/Training(Step): loss,1610368272813,0.10842493176460266
17954,-Metrics/Training(Step): loss,1610368274894,0.0933631956577301
17956,-Metrics/Training(Step): loss,1610368276981,0.0922570526599884
17958,-Metrics/Training(Step): loss,1610368279054,0.09981463849544525
17960,-Metrics/Training(Step): loss,1610368281088,0.0971844419836998
17962,-Metrics/Training(Step): loss,1610368282970,0.08193724602460861
17964,-Metrics/Training(Step): loss,1610368284979,0.09695024788379669
17966,-Metrics/Training(Step): loss,1610368287073,0.10956647992134094
17968,-Metrics/Training(Step): loss,1610368289106,0.09709994494915009
17970,-Metrics/Training(Step): loss,1610368291111,0.07628654688596725
17972,-Metrics/Training(Step): loss,1610368293146,0.1124037504196167
17974,-Metrics/Training(Step): loss,1610368295152,0.0993061363697052
17976,-Metrics/Training(Step): loss,1610368308024,0.07699110358953476
17978,-Metrics/Training(Step): loss,1610368312141,0.08056670427322388
17980,-Metrics/Training(Step): loss,1610368315924,0.0963568389415741
17982,-Metrics/Training(Step): loss,1610368319721,0.09681907296180725
17984,-Metrics/Training(Step): loss,1610368323644,0.087439626455307
17986,-Metrics/Training(Step): loss,1610368327752,0.10000293701887131
17988,-Metrics/Training(Step): loss,1610368331613,0.0987030416727066
17990,-Metrics/Training(Step): loss,1610368335133,0.1035131886601448
17992,-Metrics/Training(Step): loss,1610368338562,0.09590017795562744
17994,-Metrics/Training(Step): loss,1610368342520,0.09358607232570648
17996,-Metrics/Training(Step): loss,1610368345729,0.08444462716579437
17998,-Metrics/Training(Step): loss,1610368349787,0.07343025505542755
18000,-Metrics/Training(Step): loss,1610368353249,0.09832139313220978
18002,-Metrics/Training(Step): loss,1610368356920,0.08837377279996872
18004,-Metrics/Training(Step): loss,1610368360919,0.07233666628599167
18006,-Metrics/Training(Step): loss,1610368364516,0.07581011205911636
18008,-Metrics/Training(Step): loss,1610368368042,0.10237237811088562
18010,-Metrics/Training(Step): loss,1610368371856,0.07711957395076752
18012,-Metrics/Training(Step): loss,1610368375566,0.08476567268371582
18014,-Metrics/Training(Step): loss,1610368378633,0.08550193905830383
18016,-Metrics/Training(Step): loss,1610368381817,0.0788925439119339
18018,-Metrics/Training(Step): loss,1610368385420,0.08157671988010406
18020,-Metrics/Training(Step): loss,1610368388919,0.10067210346460342
18022,-Metrics/Training(Step): loss,1610368391574,0.09711658209562302
18024,-Metrics/Training(Step): loss,1610368393909,0.0781947523355484
18026,-Metrics/Training(Step): loss,1610368396105,0.10794837772846222
18028,-Metrics/Training(Step): loss,1610368398327,0.09464552998542786
18030,-Metrics/Training(Step): loss,1610368400410,0.11329784244298935
18032,-Metrics/Training(Step): loss,1610368402476,0.09435803443193436
18034,-Metrics/Training(Step): loss,1610368404567,0.08458933979272842
18036,-Metrics/Training(Step): loss,1610368406688,0.10241624712944031
18038,-Metrics/Training(Step): loss,1610368408816,0.08350998163223267
18040,-Metrics/Training(Step): loss,1610368410824,0.1014043316245079
18042,-Metrics/Training(Step): loss,1610368412835,0.09789152443408966
18044,-Metrics/Training(Step): loss,1610368414741,0.08492053300142288
18046,-Metrics/Training(Step): loss,1610368416791,0.08249548077583313
18048,-Metrics/Training(Step): loss,1610368418773,0.07267557829618454
18050,-Metrics/Training(Step): loss,1610368420867,0.10809393227100372
18052,-Metrics/Training(Step): loss,1610368422982,0.09830643981695175
18054,-Metrics/Training(Step): loss,1610368425031,0.10643522441387177
18056,-Metrics/Training(Step): loss,1610368427048,0.0983564555644989
18058,-Metrics/Training(Step): loss,1610368429101,0.08844508975744247
18060,-Metrics/Training(Step): loss,1610368431144,0.08223450183868408
18062,-Metrics/Training(Step): loss,1610368458642,0.07807841151952744
18064,-Metrics/Training(Step): loss,1610368462829,0.09398026019334793
18066,-Metrics/Training(Step): loss,1610368466520,0.0972217321395874
18068,-Metrics/Training(Step): loss,1610368470520,0.08921575546264648
18070,-Metrics/Training(Step): loss,1610368474520,0.1054818257689476
18072,-Metrics/Training(Step): loss,1610368478623,0.0963684692978859
18074,-Metrics/Training(Step): loss,1610368482523,0.1000714972615242
18076,-Metrics/Training(Step): loss,1610368485724,0.09278039634227753
18078,-Metrics/Training(Step): loss,1610368489889,0.09825732558965683
18080,-Metrics/Training(Step): loss,1610368492545,0.0878753811120987
18082,-Metrics/Training(Step): loss,1610368495815,0.07496137917041779
18084,-Metrics/Training(Step): loss,1610368499415,0.11167354881763458
18086,-Metrics/Training(Step): loss,1610368503322,0.08001764863729477
18088,-Metrics/Training(Step): loss,1610368507141,0.08893147110939026
18090,-Metrics/Training(Step): loss,1610368510420,0.08859265595674515
18092,-Metrics/Training(Step): loss,1610368513641,0.09370117634534836
18094,-Metrics/Training(Step): loss,1610368517190,0.09726133942604065
18096,-Metrics/Training(Step): loss,1610368520936,0.11006791889667511
18098,-Metrics/Training(Step): loss,1610368524520,0.10969904065132141
18100,-Metrics/Training(Step): loss,1610368528191,0.0714111477136612
18102,-Metrics/Training(Step): loss,1610368531753,0.11183429509401321
18104,-Metrics/Training(Step): loss,1610368535938,0.08738916367292404
18106,-Metrics/Training(Step): loss,1610368539319,0.10609368979930878
18108,-Metrics/Training(Step): loss,1610368542424,0.11181116849184036
18110,-Metrics/Training(Step): loss,1610368545001,0.11377837508916855
18112,-Metrics/Training(Step): loss,1610368547102,0.11630713194608688
18114,-Metrics/Training(Step): loss,1610368549187,0.0776752308011055
18116,-Metrics/Training(Step): loss,1610368551286,0.11536253988742828
18118,-Metrics/Training(Step): loss,1610368553313,0.09794431924819946
18120,-Metrics/Training(Step): loss,1610368555356,0.06027882918715477
18122,-Metrics/Training(Step): loss,1610368557429,0.10552584379911423
18124,-Metrics/Training(Step): loss,1610368559435,0.08564256131649017
18126,-Metrics/Training(Step): loss,1610368561509,0.08229147642850876
18128,-Metrics/Training(Step): loss,1610368563424,0.09455946087837219
18130,-Metrics/Training(Step): loss,1610368565350,0.09223205596208572
18132,-Metrics/Training(Step): loss,1610368567326,0.07166517525911331
18134,-Metrics/Training(Step): loss,1610368569370,0.11761506646871567
18136,-Metrics/Training(Step): loss,1610368571421,0.08592906594276428
18138,-Metrics/Training(Step): loss,1610368573471,0.10239212960004807
18140,-Metrics/Training(Step): loss,1610368575543,0.08490467816591263
18142,-Metrics/Training(Step): loss,1610368577506,0.10227514803409576
18144,-Metrics/Training(Step): loss,1610368579513,0.0798337310552597
18146,-Metrics/Training(Step): loss,1610368581554,0.0926160216331482
18148,-Metrics/Training(Step): loss,1610368593535,0.08519460260868073
18150,-Metrics/Training(Step): loss,1610368597620,0.08849432319402695
18152,-Metrics/Training(Step): loss,1610368601752,0.11488128453493118
18154,-Metrics/Training(Step): loss,1610368605819,0.10326731204986572
18156,-Metrics/Training(Step): loss,1610368610121,0.104576475918293
18158,-Metrics/Training(Step): loss,1610368613922,0.108240507543087
18160,-Metrics/Training(Step): loss,1610368618220,0.09450320154428482
18162,-Metrics/Training(Step): loss,1610368622219,0.09999164193868637
18164,-Metrics/Training(Step): loss,1610368625819,0.08603576570749283
18166,-Metrics/Training(Step): loss,1610368629423,0.07749509066343307
18168,-Metrics/Training(Step): loss,1610368633119,0.10062791407108307
18170,-Metrics/Training(Step): loss,1610368636522,0.11563878506422043
18172,-Metrics/Training(Step): loss,1610368639920,0.09576772898435593
18174,-Metrics/Training(Step): loss,1610368643520,0.1095196008682251
18176,-Metrics/Training(Step): loss,1610368647308,0.11853395402431488
18178,-Metrics/Training(Step): loss,1610368650765,0.0936540812253952
18180,-Metrics/Training(Step): loss,1610368654321,0.08046025037765503
18182,-Metrics/Training(Step): loss,1610368658220,0.08775360882282257
18184,-Metrics/Training(Step): loss,1610368661419,0.0730528011918068
18186,-Metrics/Training(Step): loss,1610368664262,0.10814115405082703
18188,-Metrics/Training(Step): loss,1610368668121,0.09605474770069122
18190,-Metrics/Training(Step): loss,1610368671328,0.09886225312948227
18192,-Metrics/Training(Step): loss,1610368674788,0.11238063126802444
18194,-Metrics/Training(Step): loss,1610368677154,0.09808312356472015
18196,-Metrics/Training(Step): loss,1610368679401,0.08011093735694885
18198,-Metrics/Training(Step): loss,1610368681599,0.10087858885526657
18200,-Metrics/Training(Step): loss,1610368683656,0.10782734304666519
18202,-Metrics/Training(Step): loss,1610368685738,0.07938018441200256
18204,-Metrics/Training(Step): loss,1610368687836,0.07774434983730316
18206,-Metrics/Training(Step): loss,1610368689846,0.11410784721374512
18208,-Metrics/Training(Step): loss,1610368691805,0.11394991725683212
18210,-Metrics/Training(Step): loss,1610368693672,0.0680152103304863
18212,-Metrics/Training(Step): loss,1610368695696,0.0991106778383255
18214,-Metrics/Training(Step): loss,1610368697571,0.07159852981567383
18216,-Metrics/Training(Step): loss,1610368699410,0.08729919791221619
18218,-Metrics/Training(Step): loss,1610368701370,0.0828084945678711
18220,-Metrics/Training(Step): loss,1610368703456,0.11257616430521011
18222,-Metrics/Training(Step): loss,1610368705521,0.0915621966123581
18224,-Metrics/Training(Step): loss,1610368707469,0.10195846110582352
18226,-Metrics/Training(Step): loss,1610368709427,0.10603661090135574
18228,-Metrics/Training(Step): loss,1610368711403,0.08357403427362442
18230,-Metrics/Training(Step): loss,1610368713414,0.10268542170524597
18232,-Metrics/Training(Step): loss,1610368715422,0.08921544253826141
18234,-Metrics/Training(Step): loss,1610368728825,0.07717909663915634
18236,-Metrics/Training(Step): loss,1610368732919,0.08654989302158356
18238,-Metrics/Training(Step): loss,1610368736929,0.13581161201000214
18240,-Metrics/Training(Step): loss,1610368740720,0.10860420018434525
18242,-Metrics/Training(Step): loss,1610368745020,0.09085456281900406
18244,-Metrics/Training(Step): loss,1610368749064,0.07677362114191055
18246,-Metrics/Training(Step): loss,1610368752531,0.11299098283052444
18248,-Metrics/Training(Step): loss,1610368755999,0.10436150431632996
18250,-Metrics/Training(Step): loss,1610368759721,0.06688179075717926
18252,-Metrics/Training(Step): loss,1610368763320,0.07837946712970734
18254,-Metrics/Training(Step): loss,1610368767146,0.08288852870464325
18256,-Metrics/Training(Step): loss,1610368770790,0.07158549129962921
18258,-Metrics/Training(Step): loss,1610368774485,0.08490268141031265
18260,-Metrics/Training(Step): loss,1610368777816,0.06848200410604477
18262,-Metrics/Training(Step): loss,1610368780624,0.1070311963558197
18264,-Metrics/Training(Step): loss,1610368784012,0.07836733013391495
18266,-Metrics/Training(Step): loss,1610368787620,0.08815577626228333
18268,-Metrics/Training(Step): loss,1610368791123,0.10303284227848053
18270,-Metrics/Training(Step): loss,1610368794524,0.09343810379505157
18272,-Metrics/Training(Step): loss,1610368798147,0.08264607936143875
18274,-Metrics/Training(Step): loss,1610368801592,0.10531137883663177
18276,-Metrics/Training(Step): loss,1610368805320,0.0960058867931366
18278,-Metrics/Training(Step): loss,1610368808887,0.11746087670326233
18280,-Metrics/Training(Step): loss,1610368812024,0.0889524593949318
18282,-Metrics/Training(Step): loss,1610368814605,0.0952143669128418
18284,-Metrics/Training(Step): loss,1610368816640,0.09820697456598282
18286,-Metrics/Training(Step): loss,1610368818737,0.09870453178882599
18288,-Metrics/Training(Step): loss,1610368820809,0.11645682156085968
18290,-Metrics/Training(Step): loss,1610368822874,0.10032177716493607
18292,-Metrics/Training(Step): loss,1610368824939,0.08606117963790894
18294,-Metrics/Training(Step): loss,1610368827009,0.09685087949037552
18296,-Metrics/Training(Step): loss,1610368829053,0.08154413849115372
18298,-Metrics/Training(Step): loss,1610368831122,0.07600649446249008
18300,-Metrics/Training(Step): loss,1610368833219,0.08296333998441696
18302,-Metrics/Training(Step): loss,1610368835212,0.07646339386701584
18304,-Metrics/Training(Step): loss,1610368837180,0.08870969712734222
18306,-Metrics/Training(Step): loss,1610368839104,0.10367781668901443
18308,-Metrics/Training(Step): loss,1610368841135,0.09492556005716324
18310,-Metrics/Training(Step): loss,1610368843198,0.09336866438388824
18312,-Metrics/Training(Step): loss,1610368845200,0.09658391773700714
18314,-Metrics/Training(Step): loss,1610368847244,0.09940213710069656
18316,-Metrics/Training(Step): loss,1610368849322,0.11156711727380753
18318,-Metrics/Training(Step): loss,1610368851368,0.06797704100608826
18320,-Metrics/Training(Step): loss,1610368864432,0.111468106508255
18322,-Metrics/Training(Step): loss,1610368868258,0.10445602238178253
18324,-Metrics/Training(Step): loss,1610368872149,0.09737733006477356
18326,-Metrics/Training(Step): loss,1610368876519,0.0950608104467392
18328,-Metrics/Training(Step): loss,1610368880420,0.08281965553760529
18330,-Metrics/Training(Step): loss,1610368884020,0.09049183875322342
18332,-Metrics/Training(Step): loss,1610368888135,0.08104821294546127
18334,-Metrics/Training(Step): loss,1610368891920,0.09107963740825653
18336,-Metrics/Training(Step): loss,1610368895953,0.08671493828296661
18338,-Metrics/Training(Step): loss,1610368899255,0.08154144883155823
18340,-Metrics/Training(Step): loss,1610368902717,0.1141708567738533
18342,-Metrics/Training(Step): loss,1610368906202,0.08859594911336899
18344,-Metrics/Training(Step): loss,1610368909492,0.07814329862594604
18346,-Metrics/Training(Step): loss,1610368912620,0.11247455328702927
18348,-Metrics/Training(Step): loss,1610368915927,0.09985444694757462
18350,-Metrics/Training(Step): loss,1610368919920,0.09731883555650711
18352,-Metrics/Training(Step): loss,1610368923544,0.08134729415178299
18354,-Metrics/Training(Step): loss,1610368927224,0.11206596344709396
18356,-Metrics/Training(Step): loss,1610368930823,0.08309576660394669
18358,-Metrics/Training(Step): loss,1610368935020,0.09070298075675964
18360,-Metrics/Training(Step): loss,1610368938322,0.08706476539373398
18362,-Metrics/Training(Step): loss,1610368942039,0.10246174782514572
18364,-Metrics/Training(Step): loss,1610368945224,0.10380418598651886
18366,-Metrics/Training(Step): loss,1610368947752,0.10806787014007568
18368,-Metrics/Training(Step): loss,1610368950058,0.07688119262456894
18370,-Metrics/Training(Step): loss,1610368952218,0.09988346695899963
18372,-Metrics/Training(Step): loss,1610368954310,0.07042213529348373
18374,-Metrics/Training(Step): loss,1610368956327,0.09830580651760101
18376,-Metrics/Training(Step): loss,1610368958428,0.09265462309122086
18378,-Metrics/Training(Step): loss,1610368960614,0.10286115854978561
18380,-Metrics/Training(Step): loss,1610368962560,0.10218769311904907
18382,-Metrics/Training(Step): loss,1610368964496,0.08163050562143326
18384,-Metrics/Training(Step): loss,1610368966560,0.08802718669176102
18386,-Metrics/Training(Step): loss,1610368968669,0.10458312928676605
18388,-Metrics/Training(Step): loss,1610368970650,0.07799678295850754
18390,-Metrics/Training(Step): loss,1610368972620,0.07880225777626038
18392,-Metrics/Training(Step): loss,1610368974653,0.10510792583227158
18394,-Metrics/Training(Step): loss,1610368976673,0.12404222786426544
18396,-Metrics/Training(Step): loss,1610368978699,0.1060604527592659
18398,-Metrics/Training(Step): loss,1610368980756,0.08267149329185486
18400,-Metrics/Training(Step): loss,1610368982753,0.0862036645412445
18402,-Metrics/Training(Step): loss,1610368984803,0.09909524023532867
18404,-Metrics/Training(Step): loss,1610368986845,0.09335437417030334
18406,-Metrics/Training(Step): loss,1610368998841,0.09858802706003189
18408,-Metrics/Training(Step): loss,1610369003416,0.11623046547174454
18410,-Metrics/Training(Step): loss,1610369007520,0.10182294249534607
18412,-Metrics/Training(Step): loss,1610369011627,0.07077807188034058
18414,-Metrics/Training(Step): loss,1610369015520,0.0856112539768219
18416,-Metrics/Training(Step): loss,1610369019643,0.11729545891284943
18418,-Metrics/Training(Step): loss,1610369023751,0.10634563863277435
18420,-Metrics/Training(Step): loss,1610369028055,0.10144603252410889
18422,-Metrics/Training(Step): loss,1610369032202,0.06917110085487366
18424,-Metrics/Training(Step): loss,1610369035838,0.0926370769739151
18426,-Metrics/Training(Step): loss,1610369039520,0.06517884880304337
18428,-Metrics/Training(Step): loss,1610369043565,0.08357428759336472
18430,-Metrics/Training(Step): loss,1610369047064,0.09927643090486526
18432,-Metrics/Training(Step): loss,1610369050545,0.08995628356933594
18434,-Metrics/Training(Step): loss,1610369053922,0.09769502282142639
18436,-Metrics/Training(Step): loss,1610369057515,0.10421744734048843
18438,-Metrics/Training(Step): loss,1610369060941,0.10410821437835693
18440,-Metrics/Training(Step): loss,1610369064580,0.11780547350645065
18442,-Metrics/Training(Step): loss,1610369067922,0.09336423873901367
18444,-Metrics/Training(Step): loss,1610369071759,0.09425874054431915
18446,-Metrics/Training(Step): loss,1610369075332,0.10321209579706192
18448,-Metrics/Training(Step): loss,1610369078921,0.0888623520731926
18450,-Metrics/Training(Step): loss,1610369082250,0.10094840079545975
18452,-Metrics/Training(Step): loss,1610369084518,0.09650775790214539
18454,-Metrics/Training(Step): loss,1610369086708,0.09731315821409225
18456,-Metrics/Training(Step): loss,1610369088737,0.09407997131347656
18458,-Metrics/Training(Step): loss,1610369090730,0.09541217982769012
18460,-Metrics/Training(Step): loss,1610369092843,0.08242932707071304
18462,-Metrics/Training(Step): loss,1610369094948,0.10638062655925751
18464,-Metrics/Training(Step): loss,1610369096972,0.09483563154935837
18466,-Metrics/Training(Step): loss,1610369098915,0.07569520175457001
18468,-Metrics/Training(Step): loss,1610369100970,0.08962557464838028
18470,-Metrics/Training(Step): loss,1610369103020,0.10435765236616135
18472,-Metrics/Training(Step): loss,1610369105028,0.07664113491773605
18474,-Metrics/Training(Step): loss,1610369107099,0.09272489696741104
18476,-Metrics/Training(Step): loss,1610369109194,0.08557958155870438
18478,-Metrics/Training(Step): loss,1610369111266,0.08946387469768524
18480,-Metrics/Training(Step): loss,1610369113304,0.11832170188426971
18482,-Metrics/Training(Step): loss,1610369115372,0.09009439498186111
18484,-Metrics/Training(Step): loss,1610369117381,0.08175718784332275
18486,-Metrics/Training(Step): loss,1610369119457,0.10441795736551285
18488,-Metrics/Training(Step): loss,1610369121494,0.10402372479438782
18490,-Metrics/Training(Step): loss,1610369123507,0.0858013704419136
18492,-Metrics/Training(Step): loss,1610369136334,0.09808535873889923
18494,-Metrics/Training(Step): loss,1610369140622,0.09706765413284302
18496,-Metrics/Training(Step): loss,1610369144429,0.07124320417642593
18498,-Metrics/Training(Step): loss,1610369148619,0.06656629592180252
18500,-Metrics/Training(Step): loss,1610369152718,0.09303747117519379
18502,-Metrics/Training(Step): loss,1610369157221,0.08953901380300522
18504,-Metrics/Training(Step): loss,1610369161278,0.07980217784643173
18506,-Metrics/Training(Step): loss,1610369165562,0.11345420032739639
18508,-Metrics/Training(Step): loss,1610369169320,0.09921933710575104
18510,-Metrics/Training(Step): loss,1610369172921,0.09687842428684235
18512,-Metrics/Training(Step): loss,1610369176558,0.08066514879465103
18514,-Metrics/Training(Step): loss,1610369180226,0.0947156473994255
18516,-Metrics/Training(Step): loss,1610369183921,0.0984075739979744
18518,-Metrics/Training(Step): loss,1610369187835,0.12415415048599243
18520,-Metrics/Training(Step): loss,1610369191122,0.10007017850875854
18522,-Metrics/Training(Step): loss,1610369195168,0.0941736102104187
18524,-Metrics/Training(Step): loss,1610369198720,0.09158482402563095
18526,-Metrics/Training(Step): loss,1610369202020,0.09026239812374115
18528,-Metrics/Training(Step): loss,1610369205709,0.0924224704504013
18530,-Metrics/Training(Step): loss,1610369209057,0.07720477133989334
18532,-Metrics/Training(Step): loss,1610369212431,0.08504786342382431
18534,-Metrics/Training(Step): loss,1610369215626,0.09025069326162338
18536,-Metrics/Training(Step): loss,1610369218741,0.11485735327005386
18538,-Metrics/Training(Step): loss,1610369221039,0.09312687069177628
18540,-Metrics/Training(Step): loss,1610369223113,0.08834650367498398
18542,-Metrics/Training(Step): loss,1610369225110,0.091596819460392
18544,-Metrics/Training(Step): loss,1610369227219,0.09053444862365723
18546,-Metrics/Training(Step): loss,1610369229311,0.07256482541561127
18548,-Metrics/Training(Step): loss,1610369231380,0.10233455151319504
18550,-Metrics/Training(Step): loss,1610369233478,0.09485731273889542
18552,-Metrics/Training(Step): loss,1610369235550,0.11977897584438324
18554,-Metrics/Training(Step): loss,1610369237611,0.10736136138439178
18556,-Metrics/Training(Step): loss,1610369239628,0.07043331116437912
18558,-Metrics/Training(Step): loss,1610369241734,0.08161845058202744
18560,-Metrics/Training(Step): loss,1610369243656,0.09566818177700043
18562,-Metrics/Training(Step): loss,1610369245537,0.09607107192277908
18564,-Metrics/Training(Step): loss,1610369247575,0.09758943319320679
18566,-Metrics/Training(Step): loss,1610369249634,0.09463578462600708
18568,-Metrics/Training(Step): loss,1610369251600,0.10611297190189362
18570,-Metrics/Training(Step): loss,1610369253655,0.0993153303861618
18572,-Metrics/Training(Step): loss,1610369255694,0.08992580324411392
18574,-Metrics/Training(Step): loss,1610369257734,0.09745052456855774
18576,-Metrics/Training(Step): loss,1610369259696,0.09473790973424911
18578,-Metrics/Training(Step): loss,1610369271932,0.08441156893968582
18580,-Metrics/Training(Step): loss,1610369276229,0.0733703225851059
18582,-Metrics/Training(Step): loss,1610369280222,0.07837327569723129
18584,-Metrics/Training(Step): loss,1610369284033,0.07502835988998413
18586,-Metrics/Training(Step): loss,1610369288056,0.09173843264579773
18588,-Metrics/Training(Step): loss,1610369292329,0.12309789657592773
18590,-Metrics/Training(Step): loss,1610369296361,0.07607200741767883
18592,-Metrics/Training(Step): loss,1610369299546,0.10011819005012512
18594,-Metrics/Training(Step): loss,1610369302826,0.11725185066461563
18596,-Metrics/Training(Step): loss,1610369306623,0.1199435293674469
18598,-Metrics/Training(Step): loss,1610369310421,0.11333981156349182
18600,-Metrics/Training(Step): loss,1610369314117,0.08003454655408859
18602,-Metrics/Training(Step): loss,1610369317728,0.1165754497051239
18604,-Metrics/Training(Step): loss,1610369321220,0.08691229671239853
18606,-Metrics/Training(Step): loss,1610369324819,0.09001365303993225
18608,-Metrics/Training(Step): loss,1610369328520,0.08823893219232559
18610,-Metrics/Training(Step): loss,1610369332153,0.10396437346935272
18612,-Metrics/Training(Step): loss,1610369335672,0.09323815256357193
18614,-Metrics/Training(Step): loss,1610369338845,0.09846295416355133
18616,-Metrics/Training(Step): loss,1610369342443,0.1179673820734024
18618,-Metrics/Training(Step): loss,1610369345920,0.10768875479698181
18620,-Metrics/Training(Step): loss,1610369349576,0.08997172862291336
18622,-Metrics/Training(Step): loss,1610369352622,0.08962825685739517
18624,-Metrics/Training(Step): loss,1610369355334,0.06619397550821304
18626,-Metrics/Training(Step): loss,1610369357688,0.09689772874116898
18628,-Metrics/Training(Step): loss,1610369359658,0.10633087903261185
18630,-Metrics/Training(Step): loss,1610369361758,0.09182604402303696
18632,-Metrics/Training(Step): loss,1610369363827,0.08387062698602676
18634,-Metrics/Training(Step): loss,1610369365899,0.11497010290622711
18636,-Metrics/Training(Step): loss,1610369367984,0.08723286539316177
18638,-Metrics/Training(Step): loss,1610369370054,0.0870678573846817
18640,-Metrics/Training(Step): loss,1610369372129,0.09057293832302094
18642,-Metrics/Training(Step): loss,1610369374210,0.10355231165885925
18644,-Metrics/Training(Step): loss,1610369376251,0.0799374207854271
18646,-Metrics/Training(Step): loss,1610369378345,0.0984874740242958
18648,-Metrics/Training(Step): loss,1610369380357,0.0866098627448082
18650,-Metrics/Training(Step): loss,1610369382406,0.0802522823214531
18652,-Metrics/Training(Step): loss,1610369384471,0.08370723575353622
18654,-Metrics/Training(Step): loss,1610369386422,0.07885675877332687
18656,-Metrics/Training(Step): loss,1610369388482,0.07844176888465881
18658,-Metrics/Training(Step): loss,1610369390525,0.10445477068424225
18660,-Metrics/Training(Step): loss,1610369392591,0.10399416089057922
18662,-Metrics/Training(Step): loss,1610369394635,0.09643904864788055
18664,-Metrics/Training(Step): loss,1610369407531,0.09476939588785172
18666,-Metrics/Training(Step): loss,1610369411921,0.11246546357870102
18668,-Metrics/Training(Step): loss,1610369415838,0.0910470262169838
18670,-Metrics/Training(Step): loss,1610369419920,0.1214027851819992
18672,-Metrics/Training(Step): loss,1610369423727,0.08343236148357391
18674,-Metrics/Training(Step): loss,1610369427379,0.09188411384820938
18676,-Metrics/Training(Step): loss,1610369431291,0.08780191093683243
18678,-Metrics/Training(Step): loss,1610369434947,0.08857963979244232
18680,-Metrics/Training(Step): loss,1610369439321,0.08151459693908691
18682,-Metrics/Training(Step): loss,1610369443330,0.10713742673397064
18684,-Metrics/Training(Step): loss,1610369446723,0.1134423092007637
18686,-Metrics/Training(Step): loss,1610369450575,0.07220786064863205
18688,-Metrics/Training(Step): loss,1610369454235,0.08592725545167923
18690,-Metrics/Training(Step): loss,1610369457778,0.11503598839044571
18692,-Metrics/Training(Step): loss,1610369461215,0.09530793875455856
18694,-Metrics/Training(Step): loss,1610369464547,0.08947838097810745
18696,-Metrics/Training(Step): loss,1610369467849,0.11990918964147568
18698,-Metrics/Training(Step): loss,1610369471920,0.09323439747095108
18700,-Metrics/Training(Step): loss,1610369475720,0.07696649432182312
18702,-Metrics/Training(Step): loss,1610369479265,0.10622013360261917
18704,-Metrics/Training(Step): loss,1610369482842,0.10457219928503036
18706,-Metrics/Training(Step): loss,1610369485648,0.07616565376520157
18708,-Metrics/Training(Step): loss,1610369488920,0.08975857496261597
18710,-Metrics/Training(Step): loss,1610369491686,0.0960739254951477
18712,-Metrics/Training(Step): loss,1610369494103,0.08920280635356903
18714,-Metrics/Training(Step): loss,1610369496238,0.10460568964481354
18716,-Metrics/Training(Step): loss,1610369498330,0.07161211967468262
18718,-Metrics/Training(Step): loss,1610369500361,0.07769287377595901
18720,-Metrics/Training(Step): loss,1610369502449,0.10331396758556366
18722,-Metrics/Training(Step): loss,1610369504512,0.10409671813249588
18724,-Metrics/Training(Step): loss,1610369506473,0.06706587225198746
18726,-Metrics/Training(Step): loss,1610369508476,0.08775694668292999
18728,-Metrics/Training(Step): loss,1610369510565,0.08530008047819138
18730,-Metrics/Training(Step): loss,1610369512576,0.08093706518411636
18732,-Metrics/Training(Step): loss,1610369514647,0.09199588000774384
18734,-Metrics/Training(Step): loss,1610369516715,0.07843174785375595
18736,-Metrics/Training(Step): loss,1610369518656,0.0853162556886673
18738,-Metrics/Training(Step): loss,1610369520531,0.1110243871808052
18740,-Metrics/Training(Step): loss,1610369522502,0.09841564297676086
18742,-Metrics/Training(Step): loss,1610369524527,0.08028821647167206
18744,-Metrics/Training(Step): loss,1610369526559,0.08871417492628098
18746,-Metrics/Training(Step): loss,1610369528595,0.08614606410264969
18748,-Metrics/Training(Step): loss,1610369530551,0.09294600784778595
18750,-Metrics/Training(Step): loss,1610369543339,0.0987369641661644
18752,-Metrics/Training(Step): loss,1610369547320,0.06929931789636612
18754,-Metrics/Training(Step): loss,1610369551115,0.11047154664993286
18756,-Metrics/Training(Step): loss,1610369555119,0.08544707298278809
18758,-Metrics/Training(Step): loss,1610369559215,0.10450228303670883
18760,-Metrics/Training(Step): loss,1610369563315,0.10169263929128647
18762,-Metrics/Training(Step): loss,1610369567185,0.10480914264917374
18764,-Metrics/Training(Step): loss,1610369570427,0.08449780941009521
18766,-Metrics/Training(Step): loss,1610369574294,0.10269106924533844
18768,-Metrics/Training(Step): loss,1610369577922,0.0982028916478157
18770,-Metrics/Training(Step): loss,1610369581264,0.07440316677093506
18772,-Metrics/Training(Step): loss,1610369585474,0.09326697885990143
18774,-Metrics/Training(Step): loss,1610369588919,0.1283320188522339
18776,-Metrics/Training(Step): loss,1610369592878,0.0911676213145256
18778,-Metrics/Training(Step): loss,1610369596282,0.09583815932273865
18780,-Metrics/Training(Step): loss,1610369599807,0.08930596709251404
18782,-Metrics/Training(Step): loss,1610369603222,0.09941131621599197
18784,-Metrics/Training(Step): loss,1610369606419,0.10145813971757889
18786,-Metrics/Training(Step): loss,1610369609620,0.11013313382863998
18788,-Metrics/Training(Step): loss,1610369613520,0.07784845679998398
18790,-Metrics/Training(Step): loss,1610369617204,0.09095103293657303
18792,-Metrics/Training(Step): loss,1610369620820,0.1000695452094078
18794,-Metrics/Training(Step): loss,1610369623938,0.08714766800403595
18796,-Metrics/Training(Step): loss,1610369626624,0.10862895846366882
18798,-Metrics/Training(Step): loss,1610369628725,0.09430339187383652
18800,-Metrics/Training(Step): loss,1610369630685,0.09711314737796783
18802,-Metrics/Training(Step): loss,1610369632756,0.07801172882318497
18804,-Metrics/Training(Step): loss,1610369634775,0.08929834514856339
18806,-Metrics/Training(Step): loss,1610369636726,0.12359312176704407
18808,-Metrics/Training(Step): loss,1610369638796,0.0939306914806366
18810,-Metrics/Training(Step): loss,1610369640726,0.061510566622018814
18812,-Metrics/Training(Step): loss,1610369642792,0.09326409548521042
18814,-Metrics/Training(Step): loss,1610369644693,0.08708248287439346
18816,-Metrics/Training(Step): loss,1610369646531,0.0881485790014267
18818,-Metrics/Training(Step): loss,1610369648461,0.09127093106508255
18820,-Metrics/Training(Step): loss,1610369650331,0.07298500090837479
18822,-Metrics/Training(Step): loss,1610369652367,0.09314816445112228
18824,-Metrics/Training(Step): loss,1610369654400,0.06703733652830124
18826,-Metrics/Training(Step): loss,1610369656470,0.0812125951051712
18828,-Metrics/Training(Step): loss,1610369658487,0.09759926050901413
18830,-Metrics/Training(Step): loss,1610369660538,0.11202127486467361
18832,-Metrics/Training(Step): loss,1610369662603,0.08509209007024765
18834,-Metrics/Training(Step): loss,1610369664390,0.08835448324680328
18836,-Metrics/Training(Step): loss,1610369676818,0.10093776136636734
18838,-Metrics/Training(Step): loss,1610369680720,0.08999259024858475
18840,-Metrics/Training(Step): loss,1610369684658,0.09909351915121078
18842,-Metrics/Training(Step): loss,1610369688820,0.10477942228317261
18844,-Metrics/Training(Step): loss,1610369693019,0.08432094752788544
18846,-Metrics/Training(Step): loss,1610369696820,0.10298146307468414
18848,-Metrics/Training(Step): loss,1610369700934,0.08601070195436478
18850,-Metrics/Training(Step): loss,1610369705002,0.10247541964054108
18852,-Metrics/Training(Step): loss,1610369708720,0.06468938291072845
18854,-Metrics/Training(Step): loss,1610369712319,0.08047960698604584
18856,-Metrics/Training(Step): loss,1610369715923,0.11466751992702484
18858,-Metrics/Training(Step): loss,1610369719520,0.08929683268070221
18860,-Metrics/Training(Step): loss,1610369722820,0.08390651643276215
18862,-Metrics/Training(Step): loss,1610369726240,0.08913401514291763
18864,-Metrics/Training(Step): loss,1610369729858,0.09325072169303894
18866,-Metrics/Training(Step): loss,1610369733226,0.09881317615509033
18868,-Metrics/Training(Step): loss,1610369736519,0.09342674165964127
18870,-Metrics/Training(Step): loss,1610369740370,0.09075164049863815
18872,-Metrics/Training(Step): loss,1610369743728,0.07448891550302505
18874,-Metrics/Training(Step): loss,1610369747620,0.10053674131631851
18876,-Metrics/Training(Step): loss,1610369751420,0.08352217078208923
18878,-Metrics/Training(Step): loss,1610369755119,0.08252604305744171
18880,-Metrics/Training(Step): loss,1610369758620,0.09947522729635239
18882,-Metrics/Training(Step): loss,1610369761728,0.10001789033412933
18884,-Metrics/Training(Step): loss,1610369763900,0.08347012102603912
18886,-Metrics/Training(Step): loss,1610369765850,0.08621707558631897
18888,-Metrics/Training(Step): loss,1610369767977,0.09828677028417587
18890,-Metrics/Training(Step): loss,1610369770062,0.09484487771987915
18892,-Metrics/Training(Step): loss,1610369772082,0.09434714168310165
18894,-Metrics/Training(Step): loss,1610369773915,0.09639748185873032
18896,-Metrics/Training(Step): loss,1610369775953,0.10204417258501053
18898,-Metrics/Training(Step): loss,1610369778013,0.09753980487585068
18900,-Metrics/Training(Step): loss,1610369780065,0.0824476033449173
18902,-Metrics/Training(Step): loss,1610369782134,0.06023930013179779
18904,-Metrics/Training(Step): loss,1610369784091,0.07231496274471283
18906,-Metrics/Training(Step): loss,1610369786163,0.10138583928346634
18908,-Metrics/Training(Step): loss,1610369787899,0.09627427160739899
18910,-Metrics/Training(Step): loss,1610369789985,0.08758509159088135
18912,-Metrics/Training(Step): loss,1610369792029,0.10871168971061707
18914,-Metrics/Training(Step): loss,1610369793992,0.0735258162021637
18916,-Metrics/Training(Step): loss,1610369796064,0.08640775084495544
18918,-Metrics/Training(Step): loss,1610369798074,0.09506841748952866
18920,-Metrics/Training(Step): loss,1610369800100,0.0873955637216568
18922,-Metrics/Training(Step): loss,1610369812129,0.09734482318162918
18924,-Metrics/Training(Step): loss,1610369816320,0.08839043974876404
18926,-Metrics/Training(Step): loss,1610369820519,0.11606415361166
18928,-Metrics/Training(Step): loss,1610369824421,0.08568520843982697
18930,-Metrics/Training(Step): loss,1610369828620,0.08947820961475372
18932,-Metrics/Training(Step): loss,1610369832721,0.09927938878536224
18934,-Metrics/Training(Step): loss,1610369836747,0.07640550285577774
18936,-Metrics/Training(Step): loss,1610369840284,0.09199784696102142
18938,-Metrics/Training(Step): loss,1610369843899,0.09297435730695724
18940,-Metrics/Training(Step): loss,1610369847923,0.10224664211273193
18942,-Metrics/Training(Step): loss,1610369851325,0.09541221708059311
18944,-Metrics/Training(Step): loss,1610369855156,0.07630417495965958
18946,-Metrics/Training(Step): loss,1610369858721,0.09792892634868622
18948,-Metrics/Training(Step): loss,1610369862050,0.07808762043714523
18950,-Metrics/Training(Step): loss,1610369865815,0.09351763874292374
18952,-Metrics/Training(Step): loss,1610369869316,0.09366324543952942
18954,-Metrics/Training(Step): loss,1610369873126,0.11197879165410995
18956,-Metrics/Training(Step): loss,1610369876353,0.09810405969619751
18958,-Metrics/Training(Step): loss,1610369879535,0.06966660171747208
18960,-Metrics/Training(Step): loss,1610369882969,0.08122368901968002
18962,-Metrics/Training(Step): loss,1610369886832,0.080996073782444
18964,-Metrics/Training(Step): loss,1610369890331,0.10441359877586365
18966,-Metrics/Training(Step): loss,1610369893093,0.09942799061536789
18968,-Metrics/Training(Step): loss,1610369896220,0.08702443540096283
18970,-Metrics/Training(Step): loss,1610369898766,0.09805197268724442
18972,-Metrics/Training(Step): loss,1610369900924,0.08369365334510803
18974,-Metrics/Training(Step): loss,1610369902922,0.07956801354885101
18976,-Metrics/Training(Step): loss,1610369904964,0.09793306142091751
18978,-Metrics/Training(Step): loss,1610369907059,0.10031522065401077
18980,-Metrics/Training(Step): loss,1610369909171,0.06878531724214554
18982,-Metrics/Training(Step): loss,1610369911277,0.09503772854804993
18984,-Metrics/Training(Step): loss,1610369913303,0.09505830705165863
18986,-Metrics/Training(Step): loss,1610369915273,0.06881909817457199
18988,-Metrics/Training(Step): loss,1610369917356,0.0865899845957756
18990,-Metrics/Training(Step): loss,1610369919409,0.07975132763385773
18992,-Metrics/Training(Step): loss,1610369921407,0.10410928726196289
18994,-Metrics/Training(Step): loss,1610369923511,0.08229264616966248
18996,-Metrics/Training(Step): loss,1610369925486,0.10674676299095154
18998,-Metrics/Training(Step): loss,1610369927487,0.08325999230146408
19000,-Metrics/Training(Step): loss,1610369929425,0.09326610714197159
19002,-Metrics/Training(Step): loss,1610369931387,0.0960812196135521
19004,-Metrics/Training(Step): loss,1610369933416,0.09229093790054321
19006,-Metrics/Training(Step): loss,1610369935437,0.0986751988530159
19008,-Metrics/Training(Step): loss,1610369947322,0.0916561409831047
19010,-Metrics/Training(Step): loss,1610369951319,0.07848554849624634
19012,-Metrics/Training(Step): loss,1610369955425,0.07982887327671051
19014,-Metrics/Training(Step): loss,1610369959421,0.09217976778745651
19016,-Metrics/Training(Step): loss,1610369963420,0.09847375005483627
19018,-Metrics/Training(Step): loss,1610369967120,0.11791856586933136
19020,-Metrics/Training(Step): loss,1610369971220,0.06967531889677048
19022,-Metrics/Training(Step): loss,1610369975738,0.07331103831529617
19024,-Metrics/Training(Step): loss,1610369979796,0.10787873715162277
19026,-Metrics/Training(Step): loss,1610369983621,0.07675807923078537
19028,-Metrics/Training(Step): loss,1610369987523,0.09024181962013245
19030,-Metrics/Training(Step): loss,1610369990825,0.10986161977052689
19032,-Metrics/Training(Step): loss,1610369994321,0.10763655602931976
19034,-Metrics/Training(Step): loss,1610369998087,0.07618198543787003
19036,-Metrics/Training(Step): loss,1610370001758,0.10716789215803146
19038,-Metrics/Training(Step): loss,1610370005319,0.08896221220493317
19040,-Metrics/Training(Step): loss,1610370008908,0.07512208074331284
19042,-Metrics/Training(Step): loss,1610370012328,0.10056876391172409
19044,-Metrics/Training(Step): loss,1610370015769,0.09333252161741257
19046,-Metrics/Training(Step): loss,1610370019260,0.08629274368286133
19048,-Metrics/Training(Step): loss,1610370022761,0.08888916671276093
19050,-Metrics/Training(Step): loss,1610370025892,0.09694982320070267
19052,-Metrics/Training(Step): loss,1610370029130,0.0973367840051651
19054,-Metrics/Training(Step): loss,1610370031562,0.08285979926586151
19056,-Metrics/Training(Step): loss,1610370033683,0.11432923376560211
19058,-Metrics/Training(Step): loss,1610370035757,0.08835133910179138
19060,-Metrics/Training(Step): loss,1610370037800,0.08576084673404694
19062,-Metrics/Training(Step): loss,1610370039812,0.09759972244501114
19064,-Metrics/Training(Step): loss,1610370041866,0.08610867708921432
19066,-Metrics/Training(Step): loss,1610370043904,0.09713218361139297
19068,-Metrics/Training(Step): loss,1610370045880,0.0928819477558136
19070,-Metrics/Training(Step): loss,1610370047818,0.091646209359169
19072,-Metrics/Training(Step): loss,1610370049850,0.07564032077789307
19074,-Metrics/Training(Step): loss,1610370051864,0.08245281130075455
19076,-Metrics/Training(Step): loss,1610370053853,0.08895402401685715
19078,-Metrics/Training(Step): loss,1610370055952,0.09776739776134491
19080,-Metrics/Training(Step): loss,1610370057838,0.09842071682214737
19082,-Metrics/Training(Step): loss,1610370059870,0.09355083853006363
19084,-Metrics/Training(Step): loss,1610370061804,0.09365198016166687
19086,-Metrics/Training(Step): loss,1610370063872,0.07507985830307007
19088,-Metrics/Training(Step): loss,1610370065801,0.0933007299900055
19090,-Metrics/Training(Step): loss,1610370067755,0.09598777443170547
19092,-Metrics/Training(Step): loss,1610370069735,0.0991360992193222
19094,-Metrics/Training(Step): loss,1610370082040,0.10410506278276443
19096,-Metrics/Training(Step): loss,1610370086324,0.09939192235469818
19098,-Metrics/Training(Step): loss,1610370090422,0.102562315762043
19100,-Metrics/Training(Step): loss,1610370094226,0.09580597281455994
19102,-Metrics/Training(Step): loss,1610370098152,0.10311567783355713
19104,-Metrics/Training(Step): loss,1610370101827,0.08826959878206253
19106,-Metrics/Training(Step): loss,1610370105421,0.08190173655748367
19108,-Metrics/Training(Step): loss,1610370108920,0.08599786460399628
19110,-Metrics/Training(Step): loss,1610370112964,0.10420782119035721
19112,-Metrics/Training(Step): loss,1610370116120,0.09600556641817093
19114,-Metrics/Training(Step): loss,1610370119527,0.10317130386829376
19116,-Metrics/Training(Step): loss,1610370123450,0.0779314860701561
19118,-Metrics/Training(Step): loss,1610370126849,0.11333030462265015
19120,-Metrics/Training(Step): loss,1610370130314,0.09601667523384094
19122,-Metrics/Training(Step): loss,1610370133527,0.1017499789595604
19124,-Metrics/Training(Step): loss,1610370137017,0.10826797038316727
19126,-Metrics/Training(Step): loss,1610370140620,0.0833587646484375
19128,-Metrics/Training(Step): loss,1610370144493,0.07840847223997116
19130,-Metrics/Training(Step): loss,1610370147868,0.09792503714561462
19132,-Metrics/Training(Step): loss,1610370151619,0.09610536694526672
19134,-Metrics/Training(Step): loss,1610370155020,0.09947395324707031
19136,-Metrics/Training(Step): loss,1610370158516,0.10014520585536957
19138,-Metrics/Training(Step): loss,1610370162276,0.11181579530239105
19140,-Metrics/Training(Step): loss,1610370165957,0.10522503405809402
19142,-Metrics/Training(Step): loss,1610370168512,0.09072107821702957
19144,-Metrics/Training(Step): loss,1610370170575,0.07272215187549591
19146,-Metrics/Training(Step): loss,1610370172614,0.09671592712402344
19148,-Metrics/Training(Step): loss,1610370174583,0.08759669959545135
19150,-Metrics/Training(Step): loss,1610370176618,0.1057993695139885
19152,-Metrics/Training(Step): loss,1610370178727,0.07877815514802933
19154,-Metrics/Training(Step): loss,1610370180774,0.09666340798139572
19156,-Metrics/Training(Step): loss,1610370182773,0.09758447110652924
19158,-Metrics/Training(Step): loss,1610370184587,0.10537678748369217
19160,-Metrics/Training(Step): loss,1610370186678,0.08813026547431946
19162,-Metrics/Training(Step): loss,1610370188643,0.1000375896692276
19164,-Metrics/Training(Step): loss,1610370190580,0.10377030819654465
19166,-Metrics/Training(Step): loss,1610370192601,0.0955970510840416
19168,-Metrics/Training(Step): loss,1610370194403,0.08900591731071472
19170,-Metrics/Training(Step): loss,1610370196444,0.11456801742315292
19172,-Metrics/Training(Step): loss,1610370198415,0.09453845024108887
19174,-Metrics/Training(Step): loss,1610370200449,0.08878827840089798
19176,-Metrics/Training(Step): loss,1610370202439,0.08829636126756668
19178,-Metrics/Training(Step): loss,1610370204500,0.07047285884618759
19180,-Metrics/Training(Step): loss,1610370217126,0.09484367072582245
19182,-Metrics/Training(Step): loss,1610370220993,0.08857646584510803
19184,-Metrics/Training(Step): loss,1610370225119,0.09636849164962769
19186,-Metrics/Training(Step): loss,1610370228915,0.07159176468849182
19188,-Metrics/Training(Step): loss,1610370232920,0.10074548423290253
19190,-Metrics/Training(Step): loss,1610370237359,0.07968605309724808
19192,-Metrics/Training(Step): loss,1610370241625,0.10146868228912354
19194,-Metrics/Training(Step): loss,1610370245486,0.1082938015460968
19196,-Metrics/Training(Step): loss,1610370249053,0.10409882664680481
19198,-Metrics/Training(Step): loss,1610370252728,0.10395429283380508
19200,-Metrics/Training(Step): loss,1610370256320,0.07743429392576218
19202,-Metrics/Training(Step): loss,1610370260066,0.09544746577739716
19204,-Metrics/Training(Step): loss,1610370263595,0.0873083844780922
19206,-Metrics/Training(Step): loss,1610370267068,0.08759234845638275
19208,-Metrics/Training(Step): loss,1610370271119,0.09852654486894608
19210,-Metrics/Training(Step): loss,1610370274719,0.10694520175457001
19212,-Metrics/Training(Step): loss,1610370278120,0.08669434487819672
19214,-Metrics/Training(Step): loss,1610370280920,0.0894443616271019
19216,-Metrics/Training(Step): loss,1610370284509,0.08792988210916519
19218,-Metrics/Training(Step): loss,1610370288415,0.07170888036489487
19220,-Metrics/Training(Step): loss,1610370292035,0.06882990896701813
19222,-Metrics/Training(Step): loss,1610370295739,0.11063586920499802
19224,-Metrics/Training(Step): loss,1610370299420,0.07917319238185883
19226,-Metrics/Training(Step): loss,1610370301922,0.10515877604484558
19228,-Metrics/Training(Step): loss,1610370303962,0.07249641418457031
19230,-Metrics/Training(Step): loss,1610370306136,0.07631076127290726
19232,-Metrics/Training(Step): loss,1610370308209,0.10529109835624695
19234,-Metrics/Training(Step): loss,1610370310283,0.08735531568527222
19236,-Metrics/Training(Step): loss,1610370312284,0.07647929340600967
19238,-Metrics/Training(Step): loss,1610370314357,0.09588360786437988
19240,-Metrics/Training(Step): loss,1610370316351,0.10767515748739243
19242,-Metrics/Training(Step): loss,1610370318112,0.06841367483139038
19244,-Metrics/Training(Step): loss,1610370320203,0.08025467395782471
19246,-Metrics/Training(Step): loss,1610370322061,0.10193261504173279
19248,-Metrics/Training(Step): loss,1610370324149,0.08725439012050629
19250,-Metrics/Training(Step): loss,1610370326251,0.08665212243795395
19252,-Metrics/Training(Step): loss,1610370328326,0.11116404086351395
19254,-Metrics/Training(Step): loss,1610370330403,0.12059007585048676
19256,-Metrics/Training(Step): loss,1610370332349,0.0891425833106041
19258,-Metrics/Training(Step): loss,1610370334373,0.08592983335256577
19260,-Metrics/Training(Step): loss,1610370336384,0.07447230815887451
19262,-Metrics/Training(Step): loss,1610370338430,0.09725470840930939
19264,-Metrics/Training(Step): loss,1610370340495,0.08444415032863617
19266,-Metrics/Training(Step): loss,1610370352850,0.08112835139036179
19268,-Metrics/Training(Step): loss,1610370357031,0.10803356021642685
19270,-Metrics/Training(Step): loss,1610370361320,0.07612922787666321
19272,-Metrics/Training(Step): loss,1610370365248,0.10441675037145615
19274,-Metrics/Training(Step): loss,1610370369520,0.10099761188030243
19276,-Metrics/Training(Step): loss,1610370373722,0.07521101087331772
19278,-Metrics/Training(Step): loss,1610370377832,0.0836101546883583
19280,-Metrics/Training(Step): loss,1610370382122,0.08147554844617844
19282,-Metrics/Training(Step): loss,1610370386320,0.10067532956600189
19284,-Metrics/Training(Step): loss,1610370389920,0.07567958533763885
19286,-Metrics/Training(Step): loss,1610370393258,0.0904313400387764
19288,-Metrics/Training(Step): loss,1610370396844,0.09243614971637726
19290,-Metrics/Training(Step): loss,1610370400222,0.08207978308200836
19292,-Metrics/Training(Step): loss,1610370403420,0.09361545741558075
19294,-Metrics/Training(Step): loss,1610370407115,0.09763794392347336
19296,-Metrics/Training(Step): loss,1610370410216,0.09055151045322418
19298,-Metrics/Training(Step): loss,1610370413651,0.11123808473348618
19300,-Metrics/Training(Step): loss,1610370417724,0.09061584621667862
19302,-Metrics/Training(Step): loss,1610370421004,0.07048910111188889
19304,-Metrics/Training(Step): loss,1610370424720,0.11822272092103958
19306,-Metrics/Training(Step): loss,1610370428559,0.09874074161052704
19308,-Metrics/Training(Step): loss,1610370431643,0.10220808535814285
19310,-Metrics/Training(Step): loss,1610370435134,0.10891330987215042
19312,-Metrics/Training(Step): loss,1610370437620,0.11480505019426346
19314,-Metrics/Training(Step): loss,1610370439789,0.08277279138565063
19316,-Metrics/Training(Step): loss,1610370441935,0.09768858551979065
19318,-Metrics/Training(Step): loss,1610370444005,0.08910905569791794
19320,-Metrics/Training(Step): loss,1610370446098,0.09684118628501892
19322,-Metrics/Training(Step): loss,1610370448133,0.10819412022829056
19324,-Metrics/Training(Step): loss,1610370450141,0.08771991729736328
19326,-Metrics/Training(Step): loss,1610370452120,0.08807061612606049
19328,-Metrics/Training(Step): loss,1610370454194,0.09551751613616943
19330,-Metrics/Training(Step): loss,1610370456261,0.08421068638563156
19332,-Metrics/Training(Step): loss,1610370458265,0.07389245927333832
19334,-Metrics/Training(Step): loss,1610370460185,0.07811901718378067
19336,-Metrics/Training(Step): loss,1610370462253,0.0953667089343071
19338,-Metrics/Training(Step): loss,1610370464160,0.09833957254886627
19340,-Metrics/Training(Step): loss,1610370466093,0.1193966343998909
19342,-Metrics/Training(Step): loss,1610370467977,0.08960550278425217
19344,-Metrics/Training(Step): loss,1610370469925,0.0793692097067833
19346,-Metrics/Training(Step): loss,1610370471885,0.10319143533706665
19348,-Metrics/Training(Step): loss,1610370473877,0.08708442747592926
19350,-Metrics/Training(Step): loss,1610370475926,0.10318663716316223
19352,-Metrics/Training(Step): loss,1610370487627,0.1130756363272667
19354,-Metrics/Training(Step): loss,1610370491622,0.06646587699651718
19356,-Metrics/Training(Step): loss,1610370495820,0.09767960757017136
19358,-Metrics/Training(Step): loss,1610370500027,0.08114899694919586
19360,-Metrics/Training(Step): loss,1610370504258,0.09433070570230484
19362,-Metrics/Training(Step): loss,1610370508723,0.10731307417154312
19364,-Metrics/Training(Step): loss,1610370512520,0.07406443357467651
19366,-Metrics/Training(Step): loss,1610370516220,0.09473522007465363
19368,-Metrics/Training(Step): loss,1610370520128,0.0869893729686737
19370,-Metrics/Training(Step): loss,1610370523970,0.09745226800441742
19372,-Metrics/Training(Step): loss,1610370527332,0.09200730174779892
19374,-Metrics/Training(Step): loss,1610370530733,0.09053866565227509
19376,-Metrics/Training(Step): loss,1610370534419,0.104522205889225
19378,-Metrics/Training(Step): loss,1610370537837,0.08847615122795105
19380,-Metrics/Training(Step): loss,1610370541352,0.08785134553909302
19382,-Metrics/Training(Step): loss,1610370545024,0.09789814054965973
19384,-Metrics/Training(Step): loss,1610370548173,0.09315601736307144
19386,-Metrics/Training(Step): loss,1610370551679,0.09810996800661087
19388,-Metrics/Training(Step): loss,1610370555319,0.09241320937871933
19390,-Metrics/Training(Step): loss,1610370558525,0.07954559475183487
19392,-Metrics/Training(Step): loss,1610370561919,0.10442490875720978
19394,-Metrics/Training(Step): loss,1610370565220,0.09083220362663269
19396,-Metrics/Training(Step): loss,1610370568234,0.0865432620048523
19398,-Metrics/Training(Step): loss,1610370571625,0.08740261197090149
19400,-Metrics/Training(Step): loss,1610370574039,0.12040745466947556
19402,-Metrics/Training(Step): loss,1610370576190,0.08976273983716965
19404,-Metrics/Training(Step): loss,1610370578165,0.10662495344877243
19406,-Metrics/Training(Step): loss,1610370580287,0.08344479650259018
19408,-Metrics/Training(Step): loss,1610370582370,0.0914301797747612
19410,-Metrics/Training(Step): loss,1610370584474,0.08452924340963364
19412,-Metrics/Training(Step): loss,1610370586539,0.07976242899894714
19414,-Metrics/Training(Step): loss,1610370588592,0.10493544489145279
19416,-Metrics/Training(Step): loss,1610370590683,0.09047624468803406
19418,-Metrics/Training(Step): loss,1610370592791,0.08281084895133972
19420,-Metrics/Training(Step): loss,1610370594669,0.0772896334528923
19422,-Metrics/Training(Step): loss,1610370596671,0.07666933536529541
19424,-Metrics/Training(Step): loss,1610370598704,0.07540182024240494
19426,-Metrics/Training(Step): loss,1610370600737,0.09023340791463852
19428,-Metrics/Training(Step): loss,1610370602777,0.07356742024421692
19430,-Metrics/Training(Step): loss,1610370604705,0.09363847970962524
19432,-Metrics/Training(Step): loss,1610370606704,0.09486363083124161
19434,-Metrics/Training(Step): loss,1610370608727,0.08282097429037094
19436,-Metrics/Training(Step): loss,1610370610750,0.08888453990221024
19438,-Metrics/Training(Step): loss,1610370622928,0.10122810304164886
19440,-Metrics/Training(Step): loss,1610370627119,0.09703605622053146
19442,-Metrics/Training(Step): loss,1610370631448,0.07719352841377258
19444,-Metrics/Training(Step): loss,1610370635451,0.0998283326625824
19446,-Metrics/Training(Step): loss,1610370639630,0.09473273158073425
19448,-Metrics/Training(Step): loss,1610370643653,0.11829438805580139
19450,-Metrics/Training(Step): loss,1610370647520,0.10014588385820389
19452,-Metrics/Training(Step): loss,1610370650857,0.08421207964420319
19454,-Metrics/Training(Step): loss,1610370654222,0.0847465917468071
19456,-Metrics/Training(Step): loss,1610370657720,0.10289151966571808
19458,-Metrics/Training(Step): loss,1610370661421,0.09370249509811401
19460,-Metrics/Training(Step): loss,1610370665320,0.10171723365783691
19462,-Metrics/Training(Step): loss,1610370668623,0.07746299356222153
19464,-Metrics/Training(Step): loss,1610370672415,0.10810097306966782
19466,-Metrics/Training(Step): loss,1610370676079,0.08910501003265381
19468,-Metrics/Training(Step): loss,1610370679149,0.09307461231946945
19470,-Metrics/Training(Step): loss,1610370682983,0.06978396326303482
19472,-Metrics/Training(Step): loss,1610370686445,0.09819352626800537
19474,-Metrics/Training(Step): loss,1610370690141,0.09632062911987305
19476,-Metrics/Training(Step): loss,1610370693621,0.08928722143173218
19478,-Metrics/Training(Step): loss,1610370697620,0.1075848713517189
19480,-Metrics/Training(Step): loss,1610370700632,0.08566142618656158
19482,-Metrics/Training(Step): loss,1610370704074,0.11195705831050873
19484,-Metrics/Training(Step): loss,1610370706766,0.08379989862442017
19486,-Metrics/Training(Step): loss,1610370709282,0.10478182137012482
19488,-Metrics/Training(Step): loss,1610370711438,0.07776401191949844
19490,-Metrics/Training(Step): loss,1610370713493,0.09396900236606598
19492,-Metrics/Training(Step): loss,1610370715552,0.09054580330848694
19494,-Metrics/Training(Step): loss,1610370717649,0.0921272486448288
19496,-Metrics/Training(Step): loss,1610370719718,0.08387399464845657
19498,-Metrics/Training(Step): loss,1610370721845,0.09997054189443588
19500,-Metrics/Training(Step): loss,1610370723855,0.07713133841753006
19502,-Metrics/Training(Step): loss,1610370725771,0.08387536555528641
19504,-Metrics/Training(Step): loss,1610370727859,0.08880364894866943
19506,-Metrics/Training(Step): loss,1610370729942,0.09407039731740952
19508,-Metrics/Training(Step): loss,1610370731885,0.084394171833992
19510,-Metrics/Training(Step): loss,1610370733993,0.09862344712018967
19512,-Metrics/Training(Step): loss,1610370736087,0.061598192900419235
19514,-Metrics/Training(Step): loss,1610370738120,0.09601795673370361
19516,-Metrics/Training(Step): loss,1610370740196,0.11729641258716583
19518,-Metrics/Training(Step): loss,1610370742229,0.09637515246868134
19520,-Metrics/Training(Step): loss,1610370744154,0.08967225253582001
19522,-Metrics/Training(Step): loss,1610370746149,0.08549901843070984
19524,-Metrics/Training(Step): loss,1610370757839,0.08049333840608597
19526,-Metrics/Training(Step): loss,1610370762027,0.09787794202566147
19528,-Metrics/Training(Step): loss,1610370766319,0.0685412585735321
19530,-Metrics/Training(Step): loss,1610370770215,0.09393616765737534
19532,-Metrics/Training(Step): loss,1610370774038,0.09920959919691086
19534,-Metrics/Training(Step): loss,1610370778021,0.09613092988729477
19536,-Metrics/Training(Step): loss,1610370781949,0.09861787408590317
19538,-Metrics/Training(Step): loss,1610370786028,0.0917266458272934
19540,-Metrics/Training(Step): loss,1610370790419,0.11055076867341995
19542,-Metrics/Training(Step): loss,1610370793958,0.09413833171129227
19544,-Metrics/Training(Step): loss,1610370797018,0.1055070087313652
19546,-Metrics/Training(Step): loss,1610370800946,0.113953597843647
19548,-Metrics/Training(Step): loss,1610370804519,0.09740063548088074
19550,-Metrics/Training(Step): loss,1610370808219,0.10451427102088928
19552,-Metrics/Training(Step): loss,1610370811781,0.10761108994483948
19554,-Metrics/Training(Step): loss,1610370815819,0.09683781862258911
19556,-Metrics/Training(Step): loss,1610370819220,0.09691035747528076
19558,-Metrics/Training(Step): loss,1610370823070,0.08978753536939621
19560,-Metrics/Training(Step): loss,1610370826044,0.109308622777462
19562,-Metrics/Training(Step): loss,1610370829321,0.09770150482654572
19564,-Metrics/Training(Step): loss,1610370832939,0.09057684987783432
19566,-Metrics/Training(Step): loss,1610370836519,0.09145587682723999
19568,-Metrics/Training(Step): loss,1610370839434,0.09172999113798141
19570,-Metrics/Training(Step): loss,1610370842176,0.08714505285024643
19572,-Metrics/Training(Step): loss,1610370844488,0.09812144935131073
19574,-Metrics/Training(Step): loss,1610370846599,0.09400954097509384
19576,-Metrics/Training(Step): loss,1610370848609,0.09494484961032867
19578,-Metrics/Training(Step): loss,1610370850482,0.09439720958471298
19580,-Metrics/Training(Step): loss,1610370852556,0.0974523052573204
19582,-Metrics/Training(Step): loss,1610370854659,0.10286823660135269
19584,-Metrics/Training(Step): loss,1610370856642,0.10096584260463715
19586,-Metrics/Training(Step): loss,1610370858708,0.08182571828365326
19588,-Metrics/Training(Step): loss,1610370860770,0.0730130523443222
19590,-Metrics/Training(Step): loss,1610370862850,0.08283179998397827
19592,-Metrics/Training(Step): loss,1610370864933,0.08308757841587067
19594,-Metrics/Training(Step): loss,1610370866854,0.07191026955842972
19596,-Metrics/Training(Step): loss,1610370868885,0.10191719233989716
19598,-Metrics/Training(Step): loss,1610370870956,0.10578653961420059
19600,-Metrics/Training(Step): loss,1610370872827,0.09158413112163544
19602,-Metrics/Training(Step): loss,1610370874867,0.083751380443573
19604,-Metrics/Training(Step): loss,1610370876837,0.08632895350456238
19606,-Metrics/Training(Step): loss,1610370878872,0.08106742799282074
19608,-Metrics/Training(Step): loss,1610370880906,0.07171698659658432
19610,-Metrics/Training(Step): loss,1610370893120,0.09812714159488678
19612,-Metrics/Training(Step): loss,1610370896820,0.07335104048252106
19614,-Metrics/Training(Step): loss,1610370900819,0.09542417526245117
19616,-Metrics/Training(Step): loss,1610370904820,0.080210380256176
19618,-Metrics/Training(Step): loss,1610370908921,0.0908409059047699
19620,-Metrics/Training(Step): loss,1610370913029,0.09816555678844452
19622,-Metrics/Training(Step): loss,1610370917160,0.09563792496919632
19624,-Metrics/Training(Step): loss,1610370920802,0.08128616958856583
19626,-Metrics/Training(Step): loss,1610370924616,0.0749576985836029
19628,-Metrics/Training(Step): loss,1610370928120,0.09568724036216736
19630,-Metrics/Training(Step): loss,1610370931419,0.09353344887495041
19632,-Metrics/Training(Step): loss,1610370934526,0.09926748275756836
19634,-Metrics/Training(Step): loss,1610370938019,0.08982166647911072
19636,-Metrics/Training(Step): loss,1610370941720,0.09861287474632263
19638,-Metrics/Training(Step): loss,1610370945496,0.0960623174905777
19640,-Metrics/Training(Step): loss,1610370949282,0.09753914177417755
19642,-Metrics/Training(Step): loss,1610370953063,0.09039895236492157
19644,-Metrics/Training(Step): loss,1610370956419,0.10641492903232574
19646,-Metrics/Training(Step): loss,1610370960121,0.09316113591194153
19648,-Metrics/Training(Step): loss,1610370962803,0.07767187803983688
19650,-Metrics/Training(Step): loss,1610370966619,0.08779723942279816
19652,-Metrics/Training(Step): loss,1610370970121,0.07948212325572968
19654,-Metrics/Training(Step): loss,1610370973962,0.08597778528928757
19656,-Metrics/Training(Step): loss,1610370977104,0.10101969540119171
19658,-Metrics/Training(Step): loss,1610370979663,0.07952284067869186
19660,-Metrics/Training(Step): loss,1610370981804,0.10232531279325485
19662,-Metrics/Training(Step): loss,1610370983873,0.09178782254457474
19664,-Metrics/Training(Step): loss,1610370985986,0.0899224579334259
19666,-Metrics/Training(Step): loss,1610370988090,0.07626667618751526
19668,-Metrics/Training(Step): loss,1610370990178,0.09545931965112686
19670,-Metrics/Training(Step): loss,1610370992281,0.07779673486948013
19672,-Metrics/Training(Step): loss,1610370994306,0.07544174790382385
19674,-Metrics/Training(Step): loss,1610370996249,0.08466560393571854
19676,-Metrics/Training(Step): loss,1610370998224,0.08465296775102615
19678,-Metrics/Training(Step): loss,1610371000286,0.08896630257368088
19680,-Metrics/Training(Step): loss,1610371002370,0.0880286768078804
19682,-Metrics/Training(Step): loss,1610371004437,0.08568514138460159
19684,-Metrics/Training(Step): loss,1610371006478,0.08319640904664993
19686,-Metrics/Training(Step): loss,1610371008566,0.08793531358242035
19688,-Metrics/Training(Step): loss,1610371010585,0.08543111383914948
19690,-Metrics/Training(Step): loss,1610371012647,0.07682881504297256
19692,-Metrics/Training(Step): loss,1610371014672,0.10903679579496384
19694,-Metrics/Training(Step): loss,1610371016630,0.10178977996110916
19696,-Metrics/Training(Step): loss,1610371029029,0.11262711137533188
19698,-Metrics/Training(Step): loss,1610371032922,0.08545642346143723
19700,-Metrics/Training(Step): loss,1610371036920,0.10467080026865005
19702,-Metrics/Training(Step): loss,1610371041127,0.08052290976047516
19704,-Metrics/Training(Step): loss,1610371045218,0.07915572822093964
19706,-Metrics/Training(Step): loss,1610371049244,0.08964497596025467
19708,-Metrics/Training(Step): loss,1610371053216,0.10419594496488571
19710,-Metrics/Training(Step): loss,1610371056735,0.0810709148645401
19712,-Metrics/Training(Step): loss,1610371060749,0.10547344386577606
19714,-Metrics/Training(Step): loss,1610371063949,0.1149248257279396
19716,-Metrics/Training(Step): loss,1610371067228,0.11166256666183472
19718,-Metrics/Training(Step): loss,1610371070899,0.10229115933179855
19720,-Metrics/Training(Step): loss,1610371074300,0.07930200546979904
19722,-Metrics/Training(Step): loss,1610371078148,0.11317823082208633
19724,-Metrics/Training(Step): loss,1610371081798,0.0926714763045311
19726,-Metrics/Training(Step): loss,1610371085558,0.09813760966062546
19728,-Metrics/Training(Step): loss,1610371089219,0.08831186592578888
19730,-Metrics/Training(Step): loss,1610371092720,0.08241819590330124
19732,-Metrics/Training(Step): loss,1610371096474,0.09519177675247192
19734,-Metrics/Training(Step): loss,1610371099643,0.09818336367607117
19736,-Metrics/Training(Step): loss,1610371102626,0.08941680192947388
19738,-Metrics/Training(Step): loss,1610371106122,0.11011630296707153
19740,-Metrics/Training(Step): loss,1610371109883,0.06519123166799545
19742,-Metrics/Training(Step): loss,1610371112555,0.10448294878005981
19744,-Metrics/Training(Step): loss,1610371114721,0.09580063074827194
19746,-Metrics/Training(Step): loss,1610371116910,0.10554081946611404
19748,-Metrics/Training(Step): loss,1610371118892,0.10806196182966232
19750,-Metrics/Training(Step): loss,1610371120867,0.09689407795667648
19752,-Metrics/Training(Step): loss,1610371122899,0.10408305376768112
19754,-Metrics/Training(Step): loss,1610371124972,0.10424482822418213
19756,-Metrics/Training(Step): loss,1610371127071,0.07169137895107269
19758,-Metrics/Training(Step): loss,1610371129136,0.08475469052791595
19760,-Metrics/Training(Step): loss,1610371130952,0.10170306265354156
19762,-Metrics/Training(Step): loss,1610371132718,0.0853264182806015
19764,-Metrics/Training(Step): loss,1610371134507,0.07930973917245865
19766,-Metrics/Training(Step): loss,1610371136292,0.11292234063148499
19768,-Metrics/Training(Step): loss,1610371138320,0.0944492369890213
19770,-Metrics/Training(Step): loss,1610371140384,0.08644605427980423
19772,-Metrics/Training(Step): loss,1610371142441,0.06956347823143005
19774,-Metrics/Training(Step): loss,1610371144512,0.10203973203897476
19776,-Metrics/Training(Step): loss,1610371146541,0.08829657733440399
19778,-Metrics/Training(Step): loss,1610371148568,0.06989184767007828
19780,-Metrics/Training(Step): loss,1610371150603,0.10169748216867447
19782,-Metrics/Training(Step): loss,1610371163042,0.09616196155548096
19784,-Metrics/Training(Step): loss,1610371167420,0.09973901510238647
19786,-Metrics/Training(Step): loss,1610371171522,0.10731087625026703
19788,-Metrics/Training(Step): loss,1610371175748,0.10041943937540054
19790,-Metrics/Training(Step): loss,1610371179945,0.08323857188224792
19792,-Metrics/Training(Step): loss,1610371184326,0.08612840622663498
19794,-Metrics/Training(Step): loss,1610371188221,0.09042491763830185
19796,-Metrics/Training(Step): loss,1610371192125,0.09635179489850998
19798,-Metrics/Training(Step): loss,1610371195620,0.08617493510246277
19800,-Metrics/Training(Step): loss,1610371198947,0.10598884522914886
19802,-Metrics/Training(Step): loss,1610371202378,0.09612094610929489
19804,-Metrics/Training(Step): loss,1610371205775,0.0930033028125763
19806,-Metrics/Training(Step): loss,1610371209358,0.08459358662366867
19808,-Metrics/Training(Step): loss,1610371212768,0.10573301464319229
19810,-Metrics/Training(Step): loss,1610371216234,0.10237491875886917
19812,-Metrics/Training(Step): loss,1610371219703,0.08158743381500244
19814,-Metrics/Training(Step): loss,1610371223053,0.07224924862384796
19816,-Metrics/Training(Step): loss,1610371226539,0.08887297660112381
19818,-Metrics/Training(Step): loss,1610371230453,0.08741564303636551
19820,-Metrics/Training(Step): loss,1610371234558,0.09675709903240204
19822,-Metrics/Training(Step): loss,1610371238720,0.09123169630765915
19824,-Metrics/Training(Step): loss,1610371242499,0.07145343720912933
19826,-Metrics/Training(Step): loss,1610371245489,0.0823826938867569
19828,-Metrics/Training(Step): loss,1610371248221,0.0870518758893013
19830,-Metrics/Training(Step): loss,1610371250583,0.08961934596300125
19832,-Metrics/Training(Step): loss,1610371252769,0.09281124174594879
19834,-Metrics/Training(Step): loss,1610371254881,0.08450016379356384
19836,-Metrics/Training(Step): loss,1610371256932,0.09528236091136932
19838,-Metrics/Training(Step): loss,1610371258923,0.11693374812602997
19840,-Metrics/Training(Step): loss,1610371261011,0.09619947522878647
19842,-Metrics/Training(Step): loss,1610371262942,0.10403726994991302
19844,-Metrics/Training(Step): loss,1610371264989,0.09402962774038315
19846,-Metrics/Training(Step): loss,1610371267053,0.08313004672527313
19848,-Metrics/Training(Step): loss,1610371269067,0.10297324508428574
19850,-Metrics/Training(Step): loss,1610371271139,0.0843333899974823
19852,-Metrics/Training(Step): loss,1610371273129,0.09299077838659286
19854,-Metrics/Training(Step): loss,1610371275146,0.10793478786945343
19856,-Metrics/Training(Step): loss,1610371277223,0.08015971630811691
19858,-Metrics/Training(Step): loss,1610371279137,0.11085619777441025
19860,-Metrics/Training(Step): loss,1610371281014,0.07234349846839905
19862,-Metrics/Training(Step): loss,1610371283078,0.09935339540243149
19864,-Metrics/Training(Step): loss,1610371285115,0.09364820271730423
19866,-Metrics/Training(Step): loss,1610371287149,0.10082520544528961
19868,-Metrics/Training(Step): loss,1610371299221,0.10176558792591095
19870,-Metrics/Training(Step): loss,1610371302820,0.09622248262166977
19872,-Metrics/Training(Step): loss,1610371306819,0.08431931585073471
19874,-Metrics/Training(Step): loss,1610371310820,0.07366392761468887
19876,-Metrics/Training(Step): loss,1610371314937,0.0881555825471878
19878,-Metrics/Training(Step): loss,1610371319119,0.0851607397198677
19880,-Metrics/Training(Step): loss,1610371323820,0.09311189502477646
19882,-Metrics/Training(Step): loss,1610371327922,0.09536120295524597
19884,-Metrics/Training(Step): loss,1610371331737,0.08542957901954651
19886,-Metrics/Training(Step): loss,1610371335126,0.08709514886140823
19888,-Metrics/Training(Step): loss,1610371338919,0.07909634709358215
19890,-Metrics/Training(Step): loss,1610371342646,0.11071153730154037
19892,-Metrics/Training(Step): loss,1610371345840,0.09787687659263611
19894,-Metrics/Training(Step): loss,1610371349226,0.11585109680891037
19896,-Metrics/Training(Step): loss,1610371352915,0.08776938170194626
19898,-Metrics/Training(Step): loss,1610371356620,0.059738337993621826
19900,-Metrics/Training(Step): loss,1610371359637,0.09016141295433044
19902,-Metrics/Training(Step): loss,1610371363219,0.0818457379937172
19904,-Metrics/Training(Step): loss,1610371366620,0.11751408129930496
19906,-Metrics/Training(Step): loss,1610371370031,0.07763583958148956
19908,-Metrics/Training(Step): loss,1610371373847,0.09218469262123108
19910,-Metrics/Training(Step): loss,1610371377043,0.11665742844343185
19912,-Metrics/Training(Step): loss,1610371380319,0.0970526933670044
19914,-Metrics/Training(Step): loss,1610371383180,0.06729206442832947
19916,-Metrics/Training(Step): loss,1610371385347,0.11831264942884445
19918,-Metrics/Training(Step): loss,1610371387255,0.10012169182300568
19920,-Metrics/Training(Step): loss,1610371389350,0.10577407479286194
19922,-Metrics/Training(Step): loss,1610371391456,0.10318383574485779
19924,-Metrics/Training(Step): loss,1610371393518,0.10370433330535889
19926,-Metrics/Training(Step): loss,1610371395581,0.08189638704061508
19928,-Metrics/Training(Step): loss,1610371397564,0.0903005301952362
19930,-Metrics/Training(Step): loss,1610371399603,0.08203902840614319
19932,-Metrics/Training(Step): loss,1610371401589,0.08848684281110764
19934,-Metrics/Training(Step): loss,1610371403419,0.08121257275342941
19936,-Metrics/Training(Step): loss,1610371405494,0.07672250270843506
19938,-Metrics/Training(Step): loss,1610371407577,0.08682842552661896
19940,-Metrics/Training(Step): loss,1610371409573,0.09574947506189346
19942,-Metrics/Training(Step): loss,1610371411539,0.07875766605138779
19944,-Metrics/Training(Step): loss,1610371413512,0.09047238528728485
19946,-Metrics/Training(Step): loss,1610371415550,0.08799408376216888
19948,-Metrics/Training(Step): loss,1610371417574,0.12412332743406296
19950,-Metrics/Training(Step): loss,1610371419433,0.08522867411375046
19952,-Metrics/Training(Step): loss,1610371421463,0.08870462328195572
19954,-Metrics/Training(Step): loss,1610371433728,0.07553529739379883
19956,-Metrics/Training(Step): loss,1610371437921,0.06905162334442139
19958,-Metrics/Training(Step): loss,1610371442121,0.10070325434207916
19960,-Metrics/Training(Step): loss,1610371446520,0.09538304060697556
19962,-Metrics/Training(Step): loss,1610371450620,0.09298538416624069
19964,-Metrics/Training(Step): loss,1610371454327,0.09189552813768387
19966,-Metrics/Training(Step): loss,1610371458220,0.0962304100394249
19968,-Metrics/Training(Step): loss,1610371461816,0.08480588346719742
19970,-Metrics/Training(Step): loss,1610371465496,0.09198113530874252
19972,-Metrics/Training(Step): loss,1610371468666,0.09054932743310928
19974,-Metrics/Training(Step): loss,1610371471066,0.076586052775383
19976,-Metrics/Training(Step): loss,1610371473508,0.08591707795858383
19978,-Metrics/Training(Step): loss,1610371476044,0.10098250210285187
19980,-Metrics/Training(Step): loss,1610371479739,0.09316293895244598
19982,-Metrics/Training(Step): loss,1610371483515,0.0934620052576065
19984,-Metrics/Training(Step): loss,1610371486858,0.10798704624176025
19986,-Metrics/Training(Step): loss,1610371490819,0.07699055224657059
19988,-Metrics/Training(Step): loss,1610371494720,0.09473584592342377
19990,-Metrics/Training(Step): loss,1610371498602,0.0946190133690834
19992,-Metrics/Training(Step): loss,1610371501947,0.10696443915367126
19994,-Metrics/Training(Step): loss,1610371505520,0.09729214757680893
19996,-Metrics/Training(Step): loss,1610371508496,0.09302565455436707
19998,-Metrics/Training(Step): loss,1610371511993,0.10264550149440765
20000,-Metrics/Training(Step): loss,1610371515519,0.07377807050943375
20002,-Metrics/Training(Step): loss,1610371519020,0.11371468752622604
20004,-Metrics/Training(Step): loss,1610371521965,0.07952423393726349
20006,-Metrics/Training(Step): loss,1610371524604,0.07934854179620743
20008,-Metrics/Training(Step): loss,1610371526781,0.07684336602687836
20010,-Metrics/Training(Step): loss,1610371528873,0.09775319695472717
20012,-Metrics/Training(Step): loss,1610371530940,0.07308529317378998
20014,-Metrics/Training(Step): loss,1610371532921,0.07987622171640396
20016,-Metrics/Training(Step): loss,1610371534988,0.10702034831047058
20018,-Metrics/Training(Step): loss,1610371537067,0.11115662008523941
20020,-Metrics/Training(Step): loss,1610371539135,0.10064040869474411
20022,-Metrics/Training(Step): loss,1610371541073,0.06645584106445312
20024,-Metrics/Training(Step): loss,1610371543111,0.08255697786808014
20026,-Metrics/Training(Step): loss,1610371545138,0.07508444786071777
20028,-Metrics/Training(Step): loss,1610371547122,0.08277662098407745
20030,-Metrics/Training(Step): loss,1610371549193,0.11612089723348618
20032,-Metrics/Training(Step): loss,1610371551113,0.10853151977062225
20034,-Metrics/Training(Step): loss,1610371552970,0.08870017528533936
20036,-Metrics/Training(Step): loss,1610371554982,0.10230989754199982
20038,-Metrics/Training(Step): loss,1610371557008,0.08615982532501221
20040,-Metrics/Training(Step): loss,1610371569532,0.08918095380067825
20042,-Metrics/Training(Step): loss,1610371573628,0.10333438962697983
20044,-Metrics/Training(Step): loss,1610371577932,0.08854624629020691
20046,-Metrics/Training(Step): loss,1610371582021,0.09224492311477661
20048,-Metrics/Training(Step): loss,1610371586139,0.08868555724620819
20050,-Metrics/Training(Step): loss,1610371590337,0.09047706425189972
20052,-Metrics/Training(Step): loss,1610371593822,0.10346517711877823
20054,-Metrics/Training(Step): loss,1610371597437,0.09532555937767029
20056,-Metrics/Training(Step): loss,1610371601315,0.09189502894878387
20058,-Metrics/Training(Step): loss,1610371604994,0.08654174953699112
20060,-Metrics/Training(Step): loss,1610371608288,0.075640469789505
20062,-Metrics/Training(Step): loss,1610371611760,0.09340490400791168
20064,-Metrics/Training(Step): loss,1610371615118,0.10284887999296188
20066,-Metrics/Training(Step): loss,1610371618211,0.06060607731342316
20068,-Metrics/Training(Step): loss,1610371621820,0.07559219747781754
20070,-Metrics/Training(Step): loss,1610371625617,0.09277331084012985
20072,-Metrics/Training(Step): loss,1610371628719,0.08805444091558456
20074,-Metrics/Training(Step): loss,1610371632282,0.06586678326129913
20076,-Metrics/Training(Step): loss,1610371635506,0.07490438222885132
20078,-Metrics/Training(Step): loss,1610371638552,0.09431491792201996
20080,-Metrics/Training(Step): loss,1610371641919,0.08600220084190369
20082,-Metrics/Training(Step): loss,1610371645629,0.07234769314527512
20084,-Metrics/Training(Step): loss,1610371649479,0.08138389885425568
20086,-Metrics/Training(Step): loss,1610371652226,0.07927920669317245
20088,-Metrics/Training(Step): loss,1610371654697,0.11380835622549057
20090,-Metrics/Training(Step): loss,1610371656913,0.0704394206404686
20092,-Metrics/Training(Step): loss,1610371658946,0.1043911799788475
20094,-Metrics/Training(Step): loss,1610371661027,0.09593043476343155
20096,-Metrics/Training(Step): loss,1610371663106,0.0881284549832344
20098,-Metrics/Training(Step): loss,1610371665197,0.10011982917785645
20100,-Metrics/Training(Step): loss,1610371667266,0.0875677615404129
20102,-Metrics/Training(Step): loss,1610371669354,0.08672752231359482
20104,-Metrics/Training(Step): loss,1610371671391,0.11433780193328857
20106,-Metrics/Training(Step): loss,1610371673443,0.09521608054637909
20108,-Metrics/Training(Step): loss,1610371675474,0.0933254212141037
20110,-Metrics/Training(Step): loss,1610371677531,0.09453696012496948
20112,-Metrics/Training(Step): loss,1610371679493,0.07952680438756943
20114,-Metrics/Training(Step): loss,1610371681532,0.08462552726268768
20116,-Metrics/Training(Step): loss,1610371683413,0.07313460856676102
20118,-Metrics/Training(Step): loss,1610371685359,0.07817947119474411
20120,-Metrics/Training(Step): loss,1610371687394,0.09616034477949142
20122,-Metrics/Training(Step): loss,1610371689441,0.09368786215782166
20124,-Metrics/Training(Step): loss,1610371691471,0.08316008001565933
20126,-Metrics/Training(Step): loss,1610371703845,0.08291487395763397
20128,-Metrics/Training(Step): loss,1610371707632,0.0804252028465271
20130,-Metrics/Training(Step): loss,1610371711821,0.0844508484005928
20132,-Metrics/Training(Step): loss,1610371715920,0.09950735419988632
20134,-Metrics/Training(Step): loss,1610371719846,0.0920926108956337
20136,-Metrics/Training(Step): loss,1610371724123,0.11415541917085648
20138,-Metrics/Training(Step): loss,1610371727866,0.12093271315097809
20140,-Metrics/Training(Step): loss,1610371731302,0.1139182522892952
20142,-Metrics/Training(Step): loss,1610371734742,0.1094149500131607
20144,-Metrics/Training(Step): loss,1610371738619,0.1017974466085434
20146,-Metrics/Training(Step): loss,1610371741822,0.10474152863025665
20148,-Metrics/Training(Step): loss,1610371745520,0.06645260751247406
20150,-Metrics/Training(Step): loss,1610371748823,0.08369291573762894
20152,-Metrics/Training(Step): loss,1610371752114,0.0786876231431961
20154,-Metrics/Training(Step): loss,1610371755629,0.08399872481822968
20156,-Metrics/Training(Step): loss,1610371759420,0.09438028931617737
20158,-Metrics/Training(Step): loss,1610371762732,0.07119166105985641
20160,-Metrics/Training(Step): loss,1610371766244,0.10253792256116867
20162,-Metrics/Training(Step): loss,1610371770159,0.09705714881420135
20164,-Metrics/Training(Step): loss,1610371773882,0.07471726834774017
20166,-Metrics/Training(Step): loss,1610371777032,0.08816596865653992
20168,-Metrics/Training(Step): loss,1610371781084,0.09879153221845627
20170,-Metrics/Training(Step): loss,1610371784744,0.10506152361631393
20172,-Metrics/Training(Step): loss,1610371788097,0.08197961002588272
20174,-Metrics/Training(Step): loss,1610371790383,0.0902501717209816
20176,-Metrics/Training(Step): loss,1610371792529,0.09896938502788544
20178,-Metrics/Training(Step): loss,1610371794743,0.07915256172418594
20180,-Metrics/Training(Step): loss,1610371796837,0.08869878947734833
20182,-Metrics/Training(Step): loss,1610371798912,0.09229447692632675
20184,-Metrics/Training(Step): loss,1610371801017,0.11319306492805481
20186,-Metrics/Training(Step): loss,1610371802949,0.08571501076221466
20188,-Metrics/Training(Step): loss,1610371804961,0.10301530361175537
20190,-Metrics/Training(Step): loss,1610371806988,0.08441060781478882
20192,-Metrics/Training(Step): loss,1610371808857,0.0801655575633049
20194,-Metrics/Training(Step): loss,1610371810795,0.10120195150375366
20196,-Metrics/Training(Step): loss,1610371812881,0.09209927171468735
20198,-Metrics/Training(Step): loss,1610371814936,0.06251136213541031
20200,-Metrics/Training(Step): loss,1610371816931,0.07433483749628067
20202,-Metrics/Training(Step): loss,1610371818896,0.09243923425674438
20204,-Metrics/Training(Step): loss,1610371820922,0.09955853223800659
20206,-Metrics/Training(Step): loss,1610371822902,0.09955674409866333
20208,-Metrics/Training(Step): loss,1610371824930,0.09100984036922455
20210,-Metrics/Training(Step): loss,1610371826971,0.08055680245161057
20212,-Metrics/Training(Step): loss,1610371839916,0.08593981713056564
20214,-Metrics/Training(Step): loss,1610371843958,0.10044682025909424
20216,-Metrics/Training(Step): loss,1610371848018,0.10565569996833801
20218,-Metrics/Training(Step): loss,1610371852119,0.09248631447553635
20220,-Metrics/Training(Step): loss,1610371856121,0.0991835817694664
20222,-Metrics/Training(Step): loss,1610371860020,0.09017343819141388
20224,-Metrics/Training(Step): loss,1610371863931,0.0727086290717125
20226,-Metrics/Training(Step): loss,1610371867321,0.09373698383569717
20228,-Metrics/Training(Step): loss,1610371870720,0.09807147085666656
20230,-Metrics/Training(Step): loss,1610371874317,0.09572342783212662
20232,-Metrics/Training(Step): loss,1610371878065,0.07983960956335068
20234,-Metrics/Training(Step): loss,1610371881972,0.08174816519021988
20236,-Metrics/Training(Step): loss,1610371885677,0.08575356751680374
20238,-Metrics/Training(Step): loss,1610371889128,0.08273756504058838
20240,-Metrics/Training(Step): loss,1610371892653,0.0948205217719078
20242,-Metrics/Training(Step): loss,1610371895525,0.09835723042488098
20244,-Metrics/Training(Step): loss,1610371898519,0.09115711599588394
20246,-Metrics/Training(Step): loss,1610371902024,0.07743625342845917
20248,-Metrics/Training(Step): loss,1610371905341,0.07100353389978409
20250,-Metrics/Training(Step): loss,1610371908920,0.08783788233995438
20252,-Metrics/Training(Step): loss,1610371912423,0.08344242721796036
20254,-Metrics/Training(Step): loss,1610371916020,0.08934065699577332
20256,-Metrics/Training(Step): loss,1610371919977,0.11959774047136307
20258,-Metrics/Training(Step): loss,1610371923304,0.08680234104394913
20260,-Metrics/Training(Step): loss,1610371925548,0.06840484589338303
20262,-Metrics/Training(Step): loss,1610371927603,0.08730904757976532
20264,-Metrics/Training(Step): loss,1610371929649,0.08684147894382477
20266,-Metrics/Training(Step): loss,1610371931727,0.09272681176662445
20268,-Metrics/Training(Step): loss,1610371933838,0.09608998149633408
20270,-Metrics/Training(Step): loss,1610371935898,0.10997124761343002
20272,-Metrics/Training(Step): loss,1610371938003,0.09606995433568954
20274,-Metrics/Training(Step): loss,1610371940105,0.09119929373264313
20276,-Metrics/Training(Step): loss,1610371942129,0.09485389292240143
20278,-Metrics/Training(Step): loss,1610371944054,0.07275165617465973
20280,-Metrics/Training(Step): loss,1610371945931,0.0865950807929039
20282,-Metrics/Training(Step): loss,1610371948009,0.06620040535926819
20284,-Metrics/Training(Step): loss,1610371950017,0.08745038509368896
20286,-Metrics/Training(Step): loss,1610371951745,0.10037963092327118
20288,-Metrics/Training(Step): loss,1610371953756,0.08512630313634872
20290,-Metrics/Training(Step): loss,1610371955854,0.08124763518571854
20292,-Metrics/Training(Step): loss,1610371957907,0.09046906977891922
20294,-Metrics/Training(Step): loss,1610371959887,0.07528436183929443
20296,-Metrics/Training(Step): loss,1610371961936,0.09488005936145782
20298,-Metrics/Training(Step): loss,1610371973452,0.08893708139657974
20300,-Metrics/Training(Step): loss,1610371977419,0.08475873619318008
20302,-Metrics/Training(Step): loss,1610371981323,0.09291649609804153
20304,-Metrics/Training(Step): loss,1610371985620,0.09217733889818192
20306,-Metrics/Training(Step): loss,1610371989520,0.08536048233509064
20308,-Metrics/Training(Step): loss,1610371993448,0.098704993724823
20310,-Metrics/Training(Step): loss,1610371997621,0.07004987448453903
20312,-Metrics/Training(Step): loss,1610372002034,0.10055480152368546
20314,-Metrics/Training(Step): loss,1610372006058,0.0933801531791687
20316,-Metrics/Training(Step): loss,1610372009873,0.08258485794067383
20318,-Metrics/Training(Step): loss,1610372013732,0.09897202253341675
20320,-Metrics/Training(Step): loss,1610372017321,0.06834232062101364
20322,-Metrics/Training(Step): loss,1610372021951,0.10476498305797577
20324,-Metrics/Training(Step): loss,1610372025620,0.06676266342401505
20326,-Metrics/Training(Step): loss,1610372028831,0.11078028380870819
20328,-Metrics/Training(Step): loss,1610372032122,0.0748090073466301
20330,-Metrics/Training(Step): loss,1610372035520,0.08767475187778473
20332,-Metrics/Training(Step): loss,1610372038820,0.08444593101739883
20334,-Metrics/Training(Step): loss,1610372042321,0.07017072290182114
20336,-Metrics/Training(Step): loss,1610372045420,0.09348743408918381
20338,-Metrics/Training(Step): loss,1610372049024,0.08470018953084946
20340,-Metrics/Training(Step): loss,1610372052771,0.09011247009038925
20342,-Metrics/Training(Step): loss,1610372056101,0.10466375946998596
20344,-Metrics/Training(Step): loss,1610372058369,0.10510151088237762
20346,-Metrics/Training(Step): loss,1610372060685,0.09570043534040451
20348,-Metrics/Training(Step): loss,1610372062800,0.08589158207178116
20350,-Metrics/Training(Step): loss,1610372064849,0.08048582077026367
20352,-Metrics/Training(Step): loss,1610372066860,0.08651287108659744
20354,-Metrics/Training(Step): loss,1610372068962,0.08686748892068863
20356,-Metrics/Training(Step): loss,1610372071056,0.06631570309400558
20358,-Metrics/Training(Step): loss,1610372073129,0.10268077254295349
20360,-Metrics/Training(Step): loss,1610372075111,0.10368437319993973
20362,-Metrics/Training(Step): loss,1610372077191,0.08995898813009262
20364,-Metrics/Training(Step): loss,1610372079271,0.11295425891876221
20366,-Metrics/Training(Step): loss,1610372081347,0.07756031304597855
20368,-Metrics/Training(Step): loss,1610372083451,0.08576267957687378
20370,-Metrics/Training(Step): loss,1610372085458,0.08624547719955444
20372,-Metrics/Training(Step): loss,1610372087517,0.08432447910308838
20374,-Metrics/Training(Step): loss,1610372089483,0.10788139700889587
20376,-Metrics/Training(Step): loss,1610372091536,0.09895974397659302
20378,-Metrics/Training(Step): loss,1610372093560,0.07573732733726501
20380,-Metrics/Training(Step): loss,1610372095574,0.08769083023071289
20382,-Metrics/Training(Step): loss,1610372097565,0.09561479091644287
20384,-Metrics/Training(Step): loss,1610372109725,0.10928814858198166
20386,-Metrics/Training(Step): loss,1610372114027,0.09532070904970169
20388,-Metrics/Training(Step): loss,1610372118124,0.08282176405191422
20390,-Metrics/Training(Step): loss,1610372122145,0.062333155423402786
20392,-Metrics/Training(Step): loss,1610372126217,0.10819914937019348
20394,-Metrics/Training(Step): loss,1610372130329,0.07665602117776871
20396,-Metrics/Training(Step): loss,1610372133961,0.06760119646787643
20398,-Metrics/Training(Step): loss,1610372137520,0.07732629776000977
20400,-Metrics/Training(Step): loss,1610372141020,0.09777975082397461
20402,-Metrics/Training(Step): loss,1610372144783,0.08019524067640305
20404,-Metrics/Training(Step): loss,1610372148280,0.10995322465896606
20406,-Metrics/Training(Step): loss,1610372151784,0.09188298881053925
20408,-Metrics/Training(Step): loss,1610372155611,0.10872229188680649
20410,-Metrics/Training(Step): loss,1610372159258,0.08914969116449356
20412,-Metrics/Training(Step): loss,1610372162820,0.06771731376647949
20414,-Metrics/Training(Step): loss,1610372166081,0.06639161705970764
20416,-Metrics/Training(Step): loss,1610372169442,0.08547400683164597
20418,-Metrics/Training(Step): loss,1610372173222,0.10833471268415451
20420,-Metrics/Training(Step): loss,1610372176646,0.10724873095750809
20422,-Metrics/Training(Step): loss,1610372180241,0.0858757421374321
20424,-Metrics/Training(Step): loss,1610372184019,0.09341683238744736
20426,-Metrics/Training(Step): loss,1610372187671,0.0907953530550003
20428,-Metrics/Training(Step): loss,1610372190861,0.08281966298818588
20430,-Metrics/Training(Step): loss,1610372193714,0.07328245788812637
20432,-Metrics/Training(Step): loss,1610372195914,0.06982554495334625
20434,-Metrics/Training(Step): loss,1610372197856,0.07913999259471893
20436,-Metrics/Training(Step): loss,1610372200001,0.11047933995723724
20438,-Metrics/Training(Step): loss,1610372202005,0.09188870340585709
20440,-Metrics/Training(Step): loss,1610372204042,0.07614397257566452
20442,-Metrics/Training(Step): loss,1610372206129,0.0864066407084465
20444,-Metrics/Training(Step): loss,1610372208198,0.08543545007705688
20446,-Metrics/Training(Step): loss,1610372210230,0.08220244199037552
20448,-Metrics/Training(Step): loss,1610372212300,0.08613271266222
20450,-Metrics/Training(Step): loss,1610372214359,0.08294510096311569
20452,-Metrics/Training(Step): loss,1610372216413,0.08369278907775879
20454,-Metrics/Training(Step): loss,1610372218490,0.10006644576787949
20456,-Metrics/Training(Step): loss,1610372220544,0.07643875479698181
20458,-Metrics/Training(Step): loss,1610372222508,0.1076664924621582
20460,-Metrics/Training(Step): loss,1610372224455,0.07469230890274048
20462,-Metrics/Training(Step): loss,1610372226432,0.0845087319612503
20464,-Metrics/Training(Step): loss,1610372228497,0.08692047744989395
20466,-Metrics/Training(Step): loss,1610372230518,0.10527326166629791
20468,-Metrics/Training(Step): loss,1610372232520,0.11081831902265549
20470,-Metrics/Training(Step): loss,1610372244636,0.0952528566122055
20472,-Metrics/Training(Step): loss,1610372248522,0.07685412466526031
20474,-Metrics/Training(Step): loss,1610372252638,0.09265381842851639
20476,-Metrics/Training(Step): loss,1610372256858,0.09838609397411346
20478,-Metrics/Training(Step): loss,1610372260819,0.09974589198827744
20480,-Metrics/Training(Step): loss,1610372265120,0.08914420753717422
20482,-Metrics/Training(Step): loss,1610372268832,0.08916541934013367
20484,-Metrics/Training(Step): loss,1610372272648,0.10279028117656708
20486,-Metrics/Training(Step): loss,1610372275820,0.10003606975078583
20488,-Metrics/Training(Step): loss,1610372279058,0.10325827449560165
20490,-Metrics/Training(Step): loss,1610372282326,0.10985388606786728
20492,-Metrics/Training(Step): loss,1610372286425,0.0719049721956253
20494,-Metrics/Training(Step): loss,1610372290419,0.0800342783331871
20496,-Metrics/Training(Step): loss,1610372294269,0.08738462626934052
20498,-Metrics/Training(Step): loss,1610372298020,0.08528788387775421
20500,-Metrics/Training(Step): loss,1610372301320,0.07013680785894394
20502,-Metrics/Training(Step): loss,1610372304373,0.0778985396027565
20504,-Metrics/Training(Step): loss,1610372307623,0.08842954784631729
20506,-Metrics/Training(Step): loss,1610372310720,0.06157679855823517
20508,-Metrics/Training(Step): loss,1610372314120,0.08830864727497101
20510,-Metrics/Training(Step): loss,1610372318021,0.09326184540987015
20512,-Metrics/Training(Step): loss,1610372321516,0.1001860722899437
20514,-Metrics/Training(Step): loss,1610372324921,0.10177136212587357
20516,-Metrics/Training(Step): loss,1610372328142,0.09292327612638474
20518,-Metrics/Training(Step): loss,1610372330637,0.08442902565002441
20520,-Metrics/Training(Step): loss,1610372332775,0.08783930540084839
20522,-Metrics/Training(Step): loss,1610372334867,0.0912628322839737
20524,-Metrics/Training(Step): loss,1610372336930,0.08974703401327133
20526,-Metrics/Training(Step): loss,1610372339031,0.09874697774648666
20528,-Metrics/Training(Step): loss,1610372341042,0.09459524601697922
20530,-Metrics/Training(Step): loss,1610372343113,0.0773521140217781
20532,-Metrics/Training(Step): loss,1610372345114,0.08711110800504684
20534,-Metrics/Training(Step): loss,1610372347087,0.08169723302125931
20536,-Metrics/Training(Step): loss,1610372349111,0.08339569717645645
20538,-Metrics/Training(Step): loss,1610372351031,0.08700308948755264
20540,-Metrics/Training(Step): loss,1610372353106,0.07694251835346222
20542,-Metrics/Training(Step): loss,1610372355079,0.09820997714996338
20544,-Metrics/Training(Step): loss,1610372357036,0.09646493941545486
20546,-Metrics/Training(Step): loss,1610372358938,0.05986153334379196
20548,-Metrics/Training(Step): loss,1610372361022,0.07490304112434387
20550,-Metrics/Training(Step): loss,1610372363004,0.10033342242240906
20552,-Metrics/Training(Step): loss,1610372365026,0.08216952532529831
20554,-Metrics/Training(Step): loss,1610372366920,0.10331563651561737
20556,-Metrics/Training(Step): loss,1610372379427,0.07950203120708466
20558,-Metrics/Training(Step): loss,1610372383820,0.0778314471244812
20560,-Metrics/Training(Step): loss,1610372388142,0.08518083393573761
20562,-Metrics/Training(Step): loss,1610372392818,0.07785317301750183
20564,-Metrics/Training(Step): loss,1610372396444,0.08043009787797928
20566,-Metrics/Training(Step): loss,1610372400220,0.08545044809579849
20568,-Metrics/Training(Step): loss,1610372404520,0.10255900025367737
20570,-Metrics/Training(Step): loss,1610372408921,0.09106724709272385
20572,-Metrics/Training(Step): loss,1610372412520,0.0936264842748642
20574,-Metrics/Training(Step): loss,1610372415916,0.0800037682056427
20576,-Metrics/Training(Step): loss,1610372419220,0.08901648223400116
20578,-Metrics/Training(Step): loss,1610372422720,0.07870858907699585
20580,-Metrics/Training(Step): loss,1610372426648,0.09178835898637772
20582,-Metrics/Training(Step): loss,1610372430199,0.09653809666633606
20584,-Metrics/Training(Step): loss,1610372433863,0.08579850196838379
20586,-Metrics/Training(Step): loss,1610372437341,0.0924462154507637
20588,-Metrics/Training(Step): loss,1610372441115,0.08065276592969894
20590,-Metrics/Training(Step): loss,1610372443920,0.09945844113826752
20592,-Metrics/Training(Step): loss,1610372447320,0.08640683442354202
20594,-Metrics/Training(Step): loss,1610372451354,0.09460354596376419
20596,-Metrics/Training(Step): loss,1610372455222,0.09008976072072983
20598,-Metrics/Training(Step): loss,1610372459442,0.0781397596001625
20600,-Metrics/Training(Step): loss,1610372463141,0.10946675390005112
20602,-Metrics/Training(Step): loss,1610372465598,0.09170792996883392
20604,-Metrics/Training(Step): loss,1610372467846,0.09349685907363892
20606,-Metrics/Training(Step): loss,1610372469884,0.08042974770069122
20608,-Metrics/Training(Step): loss,1610372471974,0.09443870186805725
20610,-Metrics/Training(Step): loss,1610372474063,0.11520586907863617
20612,-Metrics/Training(Step): loss,1610372476165,0.10791917890310287
20614,-Metrics/Training(Step): loss,1610372478264,0.07272228598594666
20616,-Metrics/Training(Step): loss,1610372480372,0.08522557467222214
20618,-Metrics/Training(Step): loss,1610372482393,0.08564898371696472
20620,-Metrics/Training(Step): loss,1610372484294,0.0798620730638504
20622,-Metrics/Training(Step): loss,1610372486387,0.09053951501846313
20624,-Metrics/Training(Step): loss,1610372488368,0.0713401585817337
20626,-Metrics/Training(Step): loss,1610372490124,0.06476501375436783
20628,-Metrics/Training(Step): loss,1610372492125,0.07097321003675461
20630,-Metrics/Training(Step): loss,1610372494148,0.10128655284643173
20632,-Metrics/Training(Step): loss,1610372496173,0.1080249771475792
20634,-Metrics/Training(Step): loss,1610372498020,0.08361086249351501
20636,-Metrics/Training(Step): loss,1610372500047,0.09517643600702286
20638,-Metrics/Training(Step): loss,1610372502090,0.11013559997081757
20640,-Metrics/Training(Step): loss,1610372504113,0.08962845057249069
20642,-Metrics/Training(Step): loss,1610372526936,0.08940111845731735
20644,-Metrics/Training(Step): loss,1610372531221,0.09024988859891891
20646,-Metrics/Training(Step): loss,1610372535527,0.07462835311889648
20648,-Metrics/Training(Step): loss,1610372539921,0.06588712334632874
20650,-Metrics/Training(Step): loss,1610372546444,0.10060819238424301
20652,-Metrics/Training(Step): loss,1610372552196,0.06992605328559875
20654,-Metrics/Training(Step): loss,1610372556321,0.06260672211647034
20656,-Metrics/Training(Step): loss,1610372560220,0.09566546976566315
20658,-Metrics/Training(Step): loss,1610372565229,0.08307795226573944
20660,-Metrics/Training(Step): loss,1610372569364,0.09570617973804474
20662,-Metrics/Training(Step): loss,1610372572989,0.10415472835302353
20664,-Metrics/Training(Step): loss,1610372576407,0.07182580232620239
20666,-Metrics/Training(Step): loss,1610372580222,0.07258705049753189
20668,-Metrics/Training(Step): loss,1610372584218,0.09863448888063431
20670,-Metrics/Training(Step): loss,1610372587833,0.0913107842206955
20672,-Metrics/Training(Step): loss,1610372591521,0.11926212906837463
20674,-Metrics/Training(Step): loss,1610372595122,0.08344606310129166
20676,-Metrics/Training(Step): loss,1610372598720,0.10235904157161713
20678,-Metrics/Training(Step): loss,1610372603519,0.10272526741027832
20680,-Metrics/Training(Step): loss,1610372607140,0.11193294078111649
20682,-Metrics/Training(Step): loss,1610372612970,0.08885586261749268
20684,-Metrics/Training(Step): loss,1610372616398,0.09602479636669159
20686,-Metrics/Training(Step): loss,1610372618745,0.12048961967229843
20688,-Metrics/Training(Step): loss,1610372620936,0.09952361136674881
20690,-Metrics/Training(Step): loss,1610372622977,0.08695938438177109
20692,-Metrics/Training(Step): loss,1610372624939,0.07471457868814468
20694,-Metrics/Training(Step): loss,1610372626917,0.10158442705869675
20696,-Metrics/Training(Step): loss,1610372628921,0.10311684757471085
20698,-Metrics/Training(Step): loss,1610372631019,0.09401010721921921
20700,-Metrics/Training(Step): loss,1610372633105,0.07686925679445267
20702,-Metrics/Training(Step): loss,1610372635173,0.0733892172574997
20704,-Metrics/Training(Step): loss,1610372637200,0.08438742905855179
20706,-Metrics/Training(Step): loss,1610372639069,0.08904073387384415
20708,-Metrics/Training(Step): loss,1610372641150,0.06920744478702545
20710,-Metrics/Training(Step): loss,1610372642978,0.08367276936769485
20712,-Metrics/Training(Step): loss,1610372644877,0.08581998944282532
20714,-Metrics/Training(Step): loss,1610372646633,0.09914932399988174
20716,-Metrics/Training(Step): loss,1610372648660,0.10787969827651978
20718,-Metrics/Training(Step): loss,1610372650576,0.08670136332511902
20720,-Metrics/Training(Step): loss,1610372652643,0.08778784424066544
20722,-Metrics/Training(Step): loss,1610372654667,0.09101971238851547
20724,-Metrics/Training(Step): loss,1610372656688,0.11122821271419525
20726,-Metrics/Training(Step): loss,1610372658737,0.08898220211267471
20728,-Metrics/Training(Step): loss,1610372671050,0.07344508916139603
20730,-Metrics/Training(Step): loss,1610372675527,0.09479408711194992
20732,-Metrics/Training(Step): loss,1610372679426,0.08233249187469482
20734,-Metrics/Training(Step): loss,1610372683657,0.08737479150295258
20736,-Metrics/Training(Step): loss,1610372687557,0.09189879149198532
20738,-Metrics/Training(Step): loss,1610372691622,0.0969972237944603
20740,-Metrics/Training(Step): loss,1610372695916,0.08184903115034103
20742,-Metrics/Training(Step): loss,1610372699620,0.07698845863342285
20744,-Metrics/Training(Step): loss,1610372704356,0.08001541346311569
20746,-Metrics/Training(Step): loss,1610372708519,0.11003995686769485
20748,-Metrics/Training(Step): loss,1610372712935,0.08481074124574661
20750,-Metrics/Training(Step): loss,1610372716927,0.095980204641819
20752,-Metrics/Training(Step): loss,1610372720593,0.08684694021940231
20754,-Metrics/Training(Step): loss,1610372724473,0.09749861806631088
20756,-Metrics/Training(Step): loss,1610372728122,0.09741703420877457
20758,-Metrics/Training(Step): loss,1610372731820,0.06977200508117676
20760,-Metrics/Training(Step): loss,1610372735840,0.1088118925690651
20762,-Metrics/Training(Step): loss,1610372740744,0.08615849167108536
20764,-Metrics/Training(Step): loss,1610372744520,0.09428032487630844
20766,-Metrics/Training(Step): loss,1610372748824,0.07813625037670135
20768,-Metrics/Training(Step): loss,1610372752553,0.08327890187501907
20770,-Metrics/Training(Step): loss,1610372755955,0.0827304795384407
20772,-Metrics/Training(Step): loss,1610372759757,0.08928452432155609
20774,-Metrics/Training(Step): loss,1610372762269,0.07857988029718399
20776,-Metrics/Training(Step): loss,1610372764383,0.10040432959794998
20778,-Metrics/Training(Step): loss,1610372766602,0.10044209659099579
20780,-Metrics/Training(Step): loss,1610372768805,0.10238862782716751
20782,-Metrics/Training(Step): loss,1610372770888,0.1004386842250824
20784,-Metrics/Training(Step): loss,1610372772967,0.0703512579202652
20786,-Metrics/Training(Step): loss,1610372775044,0.0886666402220726
20788,-Metrics/Training(Step): loss,1610372777118,0.10375797003507614
20790,-Metrics/Training(Step): loss,1610372779152,0.10807675868272781
20792,-Metrics/Training(Step): loss,1610372781150,0.09307284653186798
20794,-Metrics/Training(Step): loss,1610372783157,0.08024637401103973
20796,-Metrics/Training(Step): loss,1610372785026,0.07486218959093094
20798,-Metrics/Training(Step): loss,1610372787026,0.0773424506187439
20800,-Metrics/Training(Step): loss,1610372789097,0.08988218009471893
20802,-Metrics/Training(Step): loss,1610372791032,0.10564208775758743
20804,-Metrics/Training(Step): loss,1610372793115,0.09194803237915039
20806,-Metrics/Training(Step): loss,1610372795200,0.09920123219490051
20808,-Metrics/Training(Step): loss,1610372797094,0.07834556698799133
20810,-Metrics/Training(Step): loss,1610372799157,0.09679403156042099
20812,-Metrics/Training(Step): loss,1610372801189,0.09101323783397675
20814,-Metrics/Training(Step): loss,1610372813516,0.07790542393922806
20816,-Metrics/Training(Step): loss,1610372817421,0.08347734808921814
20818,-Metrics/Training(Step): loss,1610372821220,0.07817788422107697
20820,-Metrics/Training(Step): loss,1610372825220,0.10031718015670776
20822,-Metrics/Training(Step): loss,1610372829154,0.10225284099578857
20824,-Metrics/Training(Step): loss,1610372833517,0.09240888059139252
20826,-Metrics/Training(Step): loss,1610372841032,0.11620631814002991
20828,-Metrics/Training(Step): loss,1610372847922,0.10563640296459198
20830,-Metrics/Training(Step): loss,1610372852179,0.10261908918619156
20832,-Metrics/Training(Step): loss,1610372856220,0.07441483438014984
20834,-Metrics/Training(Step): loss,1610372860257,0.07838281989097595
20836,-Metrics/Training(Step): loss,1610372863875,0.09991153329610825
20838,-Metrics/Training(Step): loss,1610372867720,0.08802856504917145
20840,-Metrics/Training(Step): loss,1610372871357,0.07876619696617126
20842,-Metrics/Training(Step): loss,1610372875219,0.08187347650527954
20844,-Metrics/Training(Step): loss,1610372879419,0.08547355234622955
20846,-Metrics/Training(Step): loss,1610372883947,0.09724272042512894
20848,-Metrics/Training(Step): loss,1610372887683,0.08388350158929825
20850,-Metrics/Training(Step): loss,1610372891465,0.09741874039173126
20852,-Metrics/Training(Step): loss,1610372895116,0.08911741524934769
20854,-Metrics/Training(Step): loss,1610372898427,0.08586013317108154
20856,-Metrics/Training(Step): loss,1610372901016,0.08757712692022324
20858,-Metrics/Training(Step): loss,1610372903261,0.08684050291776657
20860,-Metrics/Training(Step): loss,1610372905404,0.09364183992147446
20862,-Metrics/Training(Step): loss,1610372907496,0.07379043847322464
20864,-Metrics/Training(Step): loss,1610372909522,0.09131714701652527
20866,-Metrics/Training(Step): loss,1610372911505,0.09562162309885025
20868,-Metrics/Training(Step): loss,1610372913588,0.08090397715568542
20870,-Metrics/Training(Step): loss,1610372915697,0.09712382405996323
20872,-Metrics/Training(Step): loss,1610372917772,0.09812496602535248
20874,-Metrics/Training(Step): loss,1610372919846,0.0747976303100586
20876,-Metrics/Training(Step): loss,1610372921931,0.09284122288227081
20878,-Metrics/Training(Step): loss,1610372923918,0.07685624063014984
20880,-Metrics/Training(Step): loss,1610372925991,0.0993524119257927
20882,-Metrics/Training(Step): loss,1610372927994,0.08716367185115814
20884,-Metrics/Training(Step): loss,1610372930044,0.10032302141189575
20886,-Metrics/Training(Step): loss,1610372932095,0.08462534844875336
20888,-Metrics/Training(Step): loss,1610372934306,0.06971471756696701
20890,-Metrics/Training(Step): loss,1610372936664,0.06127747520804405
20892,-Metrics/Training(Step): loss,1610372938883,0.07629896700382233
20894,-Metrics/Training(Step): loss,1610372941179,0.10240348428487778
20896,-Metrics/Training(Step): loss,1610372943444,0.07399982959032059
20898,-Metrics/Training(Step): loss,1610372945800,0.08202601969242096
20900,-Metrics/Training(Step): loss,1610372957530,0.10349822044372559
20902,-Metrics/Training(Step): loss,1610372961321,0.10072489082813263
20904,-Metrics/Training(Step): loss,1610372965228,0.07042942196130753
20906,-Metrics/Training(Step): loss,1610372969250,0.09071049094200134
20908,-Metrics/Training(Step): loss,1610372973219,0.08223160356283188
20910,-Metrics/Training(Step): loss,1610372977044,0.09752976894378662
20912,-Metrics/Training(Step): loss,1610372981620,0.07763257622718811
20914,-Metrics/Training(Step): loss,1610372986157,0.07659269124269485
20916,-Metrics/Training(Step): loss,1610372989716,0.07771790027618408
20918,-Metrics/Training(Step): loss,1610372993140,0.09522487968206406
20920,-Metrics/Training(Step): loss,1610372996520,0.10458910465240479
20922,-Metrics/Training(Step): loss,1610373000363,0.09162822365760803
20924,-Metrics/Training(Step): loss,1610373003741,0.08300866931676865
20926,-Metrics/Training(Step): loss,1610373006819,0.1041332334280014
20928,-Metrics/Training(Step): loss,1610373010219,0.06825713068246841
20930,-Metrics/Training(Step): loss,1610373013719,0.08517228066921234
20932,-Metrics/Training(Step): loss,1610373017028,0.09414650499820709
20934,-Metrics/Training(Step): loss,1610373020620,0.09910664707422256
20936,-Metrics/Training(Step): loss,1610373023961,0.0994437038898468
20938,-Metrics/Training(Step): loss,1610373027285,0.10068223625421524
20940,-Metrics/Training(Step): loss,1610373030478,0.08562316745519638
20942,-Metrics/Training(Step): loss,1610373034119,0.10738673061132431
20944,-Metrics/Training(Step): loss,1610373037546,0.10434302687644958
20946,-Metrics/Training(Step): loss,1610373041018,0.0844554677605629
20948,-Metrics/Training(Step): loss,1610373043481,0.09084327518939972
20950,-Metrics/Training(Step): loss,1610373045519,0.0976739153265953
20952,-Metrics/Training(Step): loss,1610373047608,0.08615779876708984
20954,-Metrics/Training(Step): loss,1610373049701,0.09481070935726166
20956,-Metrics/Training(Step): loss,1610373051789,0.09903819859027863
20958,-Metrics/Training(Step): loss,1610373053893,0.08961385488510132
20960,-Metrics/Training(Step): loss,1610373055953,0.12250351160764694
20962,-Metrics/Training(Step): loss,1610373058022,0.09353670477867126
20964,-Metrics/Training(Step): loss,1610373059934,0.09112178534269333
20966,-Metrics/Training(Step): loss,1610373061973,0.09140953421592712
20968,-Metrics/Training(Step): loss,1610373064067,0.08422424644231796
20970,-Metrics/Training(Step): loss,1610373066146,0.07431111484766006
20972,-Metrics/Training(Step): loss,1610373068142,0.08387338370084763
20974,-Metrics/Training(Step): loss,1610373070114,0.08163166046142578
20976,-Metrics/Training(Step): loss,1610373072157,0.08504657447338104
20978,-Metrics/Training(Step): loss,1610373074189,0.0838412493467331
20980,-Metrics/Training(Step): loss,1610373076156,0.09846211969852448
20982,-Metrics/Training(Step): loss,1610373078115,0.0919928029179573
20984,-Metrics/Training(Step): loss,1610373080142,0.09203409403562546
20986,-Metrics/Training(Step): loss,1610373092540,0.09846935421228409
20988,-Metrics/Training(Step): loss,1610373096539,0.07320284843444824
20990,-Metrics/Training(Step): loss,1610373100519,0.10151147842407227
20992,-Metrics/Training(Step): loss,1610373104154,0.08184690773487091
20994,-Metrics/Training(Step): loss,1610373108122,0.07215868681669235
20996,-Metrics/Training(Step): loss,1610373112437,0.09837771207094193
20998,-Metrics/Training(Step): loss,1610373116632,0.09977028518915176
21000,-Metrics/Training(Step): loss,1610373120415,0.1010805070400238
21002,-Metrics/Training(Step): loss,1610373123861,0.10812391340732574
21004,-Metrics/Training(Step): loss,1610373127419,0.07581108808517456
21006,-Metrics/Training(Step): loss,1610373131220,0.10493122786283493
21008,-Metrics/Training(Step): loss,1610373135060,0.09318441897630692
21010,-Metrics/Training(Step): loss,1610373138591,0.09689778834581375
21012,-Metrics/Training(Step): loss,1610373142319,0.08947686105966568
21014,-Metrics/Training(Step): loss,1610373145435,0.0924612432718277
21016,-Metrics/Training(Step): loss,1610373149123,0.08248447626829147
21018,-Metrics/Training(Step): loss,1610373152251,0.08959320932626724
21020,-Metrics/Training(Step): loss,1610373155343,0.08713365346193314
21022,-Metrics/Training(Step): loss,1610373158861,0.08449449390172958
21024,-Metrics/Training(Step): loss,1610373162320,0.0782705619931221
21026,-Metrics/Training(Step): loss,1610373165920,0.10620736330747604
21028,-Metrics/Training(Step): loss,1610373169210,0.08273915946483612
21030,-Metrics/Training(Step): loss,1610373172519,0.10157006978988647
21032,-Metrics/Training(Step): loss,1610373175327,0.08975294232368469
21034,-Metrics/Training(Step): loss,1610373177647,0.08411712199449539
21036,-Metrics/Training(Step): loss,1610373179663,0.0719098448753357
21038,-Metrics/Training(Step): loss,1610373181757,0.09539072960615158
21040,-Metrics/Training(Step): loss,1610373183851,0.0850115641951561
21042,-Metrics/Training(Step): loss,1610373185927,0.08688687533140182
21044,-Metrics/Training(Step): loss,1610373188024,0.10290568321943283
21046,-Metrics/Training(Step): loss,1610373190080,0.09170246124267578
21048,-Metrics/Training(Step): loss,1610373192077,0.08256125450134277
21050,-Metrics/Training(Step): loss,1610373194134,0.10844823718070984
21052,-Metrics/Training(Step): loss,1610373196083,0.07316282391548157
21054,-Metrics/Training(Step): loss,1610373198033,0.08242415636777878
21056,-Metrics/Training(Step): loss,1610373200068,0.08689433336257935
21058,-Metrics/Training(Step): loss,1610373202048,0.054926879703998566
21060,-Metrics/Training(Step): loss,1610373204082,0.09985240548849106
21062,-Metrics/Training(Step): loss,1610373206149,0.07494013011455536
21064,-Metrics/Training(Step): loss,1610373208182,0.1113939881324768
21066,-Metrics/Training(Step): loss,1610373210228,0.08666758984327316
21068,-Metrics/Training(Step): loss,1610373212211,0.08681613206863403
21070,-Metrics/Training(Step): loss,1610373214249,0.10876470059156418
21072,-Metrics/Training(Step): loss,1610373226631,0.08465351909399033
21074,-Metrics/Training(Step): loss,1610373230619,0.07286050170660019
21076,-Metrics/Training(Step): loss,1610373234520,0.0972275361418724
21078,-Metrics/Training(Step): loss,1610373238748,0.07822822034358978
21080,-Metrics/Training(Step): loss,1610373242822,0.0984557569026947
21082,-Metrics/Training(Step): loss,1610373246820,0.07833387702703476
21084,-Metrics/Training(Step): loss,1610373250531,0.08780144900083542
21086,-Metrics/Training(Step): loss,1610373254454,0.08047888427972794
21088,-Metrics/Training(Step): loss,1610373258269,0.08840151876211166
21090,-Metrics/Training(Step): loss,1610373262119,0.09639029949903488
21092,-Metrics/Training(Step): loss,1610373265520,0.08685056865215302
21094,-Metrics/Training(Step): loss,1610373268728,0.10081689059734344
21096,-Metrics/Training(Step): loss,1610373272258,0.054275162518024445
21098,-Metrics/Training(Step): loss,1610373275333,0.09045190364122391
21100,-Metrics/Training(Step): loss,1610373279119,0.07513750344514847
21102,-Metrics/Training(Step): loss,1610373282119,0.09724493324756622
21104,-Metrics/Training(Step): loss,1610373285690,0.06720475107431412
21106,-Metrics/Training(Step): loss,1610373289154,0.07717365026473999
21108,-Metrics/Training(Step): loss,1610373293119,0.11424601078033447
21110,-Metrics/Training(Step): loss,1610373296218,0.08749327808618546
21112,-Metrics/Training(Step): loss,1610373299536,0.09097780287265778
21114,-Metrics/Training(Step): loss,1610373303019,0.10064487159252167
21116,-Metrics/Training(Step): loss,1610373306563,0.08676155656576157
21118,-Metrics/Training(Step): loss,1610373309638,0.11557506024837494
21120,-Metrics/Training(Step): loss,1610373312153,0.07704071700572968
21122,-Metrics/Training(Step): loss,1610373314273,0.08650659024715424
21124,-Metrics/Training(Step): loss,1610373316216,0.05555320531129837
21126,-Metrics/Training(Step): loss,1610373318292,0.09672792255878448
21128,-Metrics/Training(Step): loss,1610373320355,0.0859944075345993
21130,-Metrics/Training(Step): loss,1610373322448,0.07703578472137451
21132,-Metrics/Training(Step): loss,1610373324456,0.08645276725292206
21134,-Metrics/Training(Step): loss,1610373326504,0.07789857685565948
21136,-Metrics/Training(Step): loss,1610373328539,0.09082113206386566
21138,-Metrics/Training(Step): loss,1610373330614,0.09825130552053452
21140,-Metrics/Training(Step): loss,1610373332563,0.0707041323184967
21142,-Metrics/Training(Step): loss,1610373334620,0.08669460564851761
21144,-Metrics/Training(Step): loss,1610373336685,0.08017580956220627
21146,-Metrics/Training(Step): loss,1610373338767,0.09279144555330276
21148,-Metrics/Training(Step): loss,1610373340845,0.0910877138376236
21150,-Metrics/Training(Step): loss,1610373342883,0.09352223575115204
21152,-Metrics/Training(Step): loss,1610373344880,0.07121818512678146
21154,-Metrics/Training(Step): loss,1610373346910,0.07043363898992538
21156,-Metrics/Training(Step): loss,1610373348937,0.09683991223573685
21158,-Metrics/Training(Step): loss,1610373361541,0.073292575776577
21160,-Metrics/Training(Step): loss,1610373365719,0.07968494296073914
21162,-Metrics/Training(Step): loss,1610373369722,0.06880926340818405
21164,-Metrics/Training(Step): loss,1610373373620,0.09965935349464417
21166,-Metrics/Training(Step): loss,1610373377619,0.10167375206947327
21168,-Metrics/Training(Step): loss,1610373381615,0.0704892948269844
21170,-Metrics/Training(Step): loss,1610373385924,0.081505186855793
21172,-Metrics/Training(Step): loss,1610373390317,0.09298113733530045
21174,-Metrics/Training(Step): loss,1610373394020,0.08542722463607788
21176,-Metrics/Training(Step): loss,1610373397420,0.0776887759566307
21178,-Metrics/Training(Step): loss,1610373400622,0.10563264042139053
21180,-Metrics/Training(Step): loss,1610373404242,0.08263427019119263
21182,-Metrics/Training(Step): loss,1610373407616,0.0852690041065216
21184,-Metrics/Training(Step): loss,1610373411116,0.08078877627849579
21186,-Metrics/Training(Step): loss,1610373414941,0.08764971047639847
21188,-Metrics/Training(Step): loss,1610373418326,0.11151296645402908
21190,-Metrics/Training(Step): loss,1610373421519,0.08653129637241364
21192,-Metrics/Training(Step): loss,1610373425001,0.08326510339975357
21194,-Metrics/Training(Step): loss,1610373428182,0.08035090565681458
21196,-Metrics/Training(Step): loss,1610373431631,0.0888700857758522
21198,-Metrics/Training(Step): loss,1610373434821,0.08265308290719986
21200,-Metrics/Training(Step): loss,1610373438424,0.09539396315813065
21202,-Metrics/Training(Step): loss,1610373441360,0.06835216283798218
21204,-Metrics/Training(Step): loss,1610373444419,0.10960347205400467
21206,-Metrics/Training(Step): loss,1610373446648,0.09072906523942947
21208,-Metrics/Training(Step): loss,1610373448746,0.0766119509935379
21210,-Metrics/Training(Step): loss,1610373450856,0.09804945439100266
21212,-Metrics/Training(Step): loss,1610373452932,0.09811101853847504
21214,-Metrics/Training(Step): loss,1610373455022,0.09272982180118561
21216,-Metrics/Training(Step): loss,1610373457114,0.10074030607938766
21218,-Metrics/Training(Step): loss,1610373459203,0.08357871323823929
21220,-Metrics/Training(Step): loss,1610373461247,0.07867325842380524
21222,-Metrics/Training(Step): loss,1610373463328,0.08163906633853912
21224,-Metrics/Training(Step): loss,1610373465416,0.1180887371301651
21226,-Metrics/Training(Step): loss,1610373467420,0.07790989428758621
21228,-Metrics/Training(Step): loss,1610373469523,0.09614556282758713
21230,-Metrics/Training(Step): loss,1610373471561,0.059123992919921875
21232,-Metrics/Training(Step): loss,1610373473642,0.08658117800951004
21234,-Metrics/Training(Step): loss,1610373475392,0.07880167663097382
21236,-Metrics/Training(Step): loss,1610373477418,0.09449441730976105
21238,-Metrics/Training(Step): loss,1610373479447,0.09779210388660431
21240,-Metrics/Training(Step): loss,1610373481519,0.0812445804476738
21242,-Metrics/Training(Step): loss,1610373483575,0.07973473519086838
21244,-Metrics/Training(Step): loss,1610373495939,0.09046673774719238
21246,-Metrics/Training(Step): loss,1610373500119,0.11499007791280746
21248,-Metrics/Training(Step): loss,1610373504015,0.12141449749469757
21250,-Metrics/Training(Step): loss,1610373508037,0.08880916237831116
21252,-Metrics/Training(Step): loss,1610373512320,0.06703799217939377
21254,-Metrics/Training(Step): loss,1610373516119,0.09452091157436371
21256,-Metrics/Training(Step): loss,1610373520216,0.0785413533449173
21258,-Metrics/Training(Step): loss,1610373524019,0.10348690301179886
21260,-Metrics/Training(Step): loss,1610373528059,0.07220133394002914
21262,-Metrics/Training(Step): loss,1610373531748,0.06332983821630478
21264,-Metrics/Training(Step): loss,1610373535220,0.10375503450632095
21266,-Metrics/Training(Step): loss,1610373538519,0.08456315100193024
21268,-Metrics/Training(Step): loss,1610373541492,0.08593501895666122
21270,-Metrics/Training(Step): loss,1610373545120,0.09485414624214172
21272,-Metrics/Training(Step): loss,1610373548953,0.09065430611371994
21274,-Metrics/Training(Step): loss,1610373552531,0.09966504573822021
21276,-Metrics/Training(Step): loss,1610373556219,0.09293976426124573
21278,-Metrics/Training(Step): loss,1610373559867,0.07576604932546616
21280,-Metrics/Training(Step): loss,1610373563331,0.09697525948286057
21282,-Metrics/Training(Step): loss,1610373567020,0.08448294550180435
21284,-Metrics/Training(Step): loss,1610373570976,0.09903034567832947
21286,-Metrics/Training(Step): loss,1610373574617,0.08465975522994995
21288,-Metrics/Training(Step): loss,1610373577799,0.09171979874372482
21290,-Metrics/Training(Step): loss,1610373579947,0.08920161426067352
21292,-Metrics/Training(Step): loss,1610373582125,0.08468567579984665
21294,-Metrics/Training(Step): loss,1610373584230,0.10091743618249893
21296,-Metrics/Training(Step): loss,1610373586406,0.07869389653205872
21298,-Metrics/Training(Step): loss,1610373588429,0.10295519977807999
21300,-Metrics/Training(Step): loss,1610373590514,0.08082494139671326
21302,-Metrics/Training(Step): loss,1610373592575,0.0582420714199543
21304,-Metrics/Training(Step): loss,1610373594571,0.08763204514980316
21306,-Metrics/Training(Step): loss,1610373596477,0.07719021290540695
21308,-Metrics/Training(Step): loss,1610373598566,0.08912092447280884
21310,-Metrics/Training(Step): loss,1610373600606,0.09543060511350632
21312,-Metrics/Training(Step): loss,1610373602692,0.10221968591213226
21314,-Metrics/Training(Step): loss,1610373604776,0.08404209464788437
21316,-Metrics/Training(Step): loss,1610373606654,0.10191158205270767
21318,-Metrics/Training(Step): loss,1610373608727,0.08055931329727173
21320,-Metrics/Training(Step): loss,1610373610825,0.06756679713726044
21322,-Metrics/Training(Step): loss,1610373612787,0.0789358988404274
21324,-Metrics/Training(Step): loss,1610373614817,0.09608222544193268
21326,-Metrics/Training(Step): loss,1610373616872,0.09856020659208298
21328,-Metrics/Training(Step): loss,1610373618864,0.0818912461400032
21330,-Metrics/Training(Step): loss,1610373631337,0.10215829312801361
21332,-Metrics/Training(Step): loss,1610373635326,0.08975408226251602
21334,-Metrics/Training(Step): loss,1610373639119,0.07746053487062454
21336,-Metrics/Training(Step): loss,1610373643441,0.10194789618253708
21338,-Metrics/Training(Step): loss,1610373647545,0.05696827918291092
21340,-Metrics/Training(Step): loss,1610373651420,0.07313244044780731
21342,-Metrics/Training(Step): loss,1610373655216,0.09641867876052856
21344,-Metrics/Training(Step): loss,1610373659020,0.10373205691576004
21346,-Metrics/Training(Step): loss,1610373662320,0.0738225132226944
21348,-Metrics/Training(Step): loss,1610373665819,0.09712439030408859
21350,-Metrics/Training(Step): loss,1610373669420,0.07939396053552628
21352,-Metrics/Training(Step): loss,1610373673042,0.08608940988779068
21354,-Metrics/Training(Step): loss,1610373676719,0.07791813462972641
21356,-Metrics/Training(Step): loss,1610373681141,0.09802205115556717
21358,-Metrics/Training(Step): loss,1610373685549,0.08034022152423859
21360,-Metrics/Training(Step): loss,1610373689520,0.08301792293787003
21362,-Metrics/Training(Step): loss,1610373693319,0.07426028698682785
21364,-Metrics/Training(Step): loss,1610373696546,0.0958787277340889
21366,-Metrics/Training(Step): loss,1610373700525,0.10060988366603851
21368,-Metrics/Training(Step): loss,1610373703779,0.06916545331478119
21370,-Metrics/Training(Step): loss,1610373707022,0.07233989983797073
21372,-Metrics/Training(Step): loss,1610373711074,0.0915812999010086
21374,-Metrics/Training(Step): loss,1610373715220,0.08545941859483719
21376,-Metrics/Training(Step): loss,1610373718471,0.06432721763849258
21378,-Metrics/Training(Step): loss,1610373721617,0.07570040971040726
21380,-Metrics/Training(Step): loss,1610373724402,0.09207665175199509
21382,-Metrics/Training(Step): loss,1610373726423,0.0976736769080162
21384,-Metrics/Training(Step): loss,1610373728619,0.10226215422153473
21386,-Metrics/Training(Step): loss,1610373731022,0.09561175107955933
21388,-Metrics/Training(Step): loss,1610373733313,0.08929525315761566
21390,-Metrics/Training(Step): loss,1610373735374,0.09296880662441254
21392,-Metrics/Training(Step): loss,1610373737347,0.0726284608244896
21394,-Metrics/Training(Step): loss,1610373739255,0.08736323565244675
21396,-Metrics/Training(Step): loss,1610373741050,0.08186078816652298
21398,-Metrics/Training(Step): loss,1610373743119,0.1023034080862999
21400,-Metrics/Training(Step): loss,1610373745201,0.08324705809354782
21402,-Metrics/Training(Step): loss,1610373747267,0.07283300906419754
21404,-Metrics/Training(Step): loss,1610373749483,0.08961701393127441
21406,-Metrics/Training(Step): loss,1610373751572,0.09743114560842514
21408,-Metrics/Training(Step): loss,1610373753683,0.08676669001579285
21410,-Metrics/Training(Step): loss,1610373755908,0.09073197841644287
21412,-Metrics/Training(Step): loss,1610373757977,0.0966050773859024
21414,-Metrics/Training(Step): loss,1610373760047,0.09103479236364365
21416,-Metrics/Training(Step): loss,1610373771827,0.06521037220954895
21418,-Metrics/Training(Step): loss,1610373776225,0.08138442039489746
21420,-Metrics/Training(Step): loss,1610373780126,0.09889187663793564
21422,-Metrics/Training(Step): loss,1610373784537,0.06635245680809021
21424,-Metrics/Training(Step): loss,1610373788555,0.08900456875562668
21426,-Metrics/Training(Step): loss,1610373792628,0.10190213471651077
21428,-Metrics/Training(Step): loss,1610373799642,0.07881534844636917
21430,-Metrics/Training(Step): loss,1610373804120,0.07566998898983002
21432,-Metrics/Training(Step): loss,1610373807263,0.1159026175737381
21434,-Metrics/Training(Step): loss,1610373811903,0.08365470916032791
21436,-Metrics/Training(Step): loss,1610373815511,0.08507603406906128
21438,-Metrics/Training(Step): loss,1610373819157,0.09002132713794708
21440,-Metrics/Training(Step): loss,1610373822805,0.10062899440526962
21442,-Metrics/Training(Step): loss,1610373826520,0.09864611178636551
21444,-Metrics/Training(Step): loss,1610373830219,0.08391730487346649
21446,-Metrics/Training(Step): loss,1610373833256,0.08954598009586334
21448,-Metrics/Training(Step): loss,1610373837220,0.08831825852394104
21450,-Metrics/Training(Step): loss,1610373841052,0.083840511739254
21452,-Metrics/Training(Step): loss,1610373844683,0.08312288671731949
21454,-Metrics/Training(Step): loss,1610373848554,0.09868238866329193
21456,-Metrics/Training(Step): loss,1610373852126,0.09643273800611496
21458,-Metrics/Training(Step): loss,1610373855316,0.08552075177431107
21460,-Metrics/Training(Step): loss,1610373858649,0.06837818026542664
21462,-Metrics/Training(Step): loss,1610373861396,0.08593180775642395
21464,-Metrics/Training(Step): loss,1610373864106,0.0818808302283287
21466,-Metrics/Training(Step): loss,1610373866176,0.08879946172237396
21468,-Metrics/Training(Step): loss,1610373868302,0.09169039130210876
21470,-Metrics/Training(Step): loss,1610373870378,0.09373302757740021
21472,-Metrics/Training(Step): loss,1610373872552,0.09093459695577621
21474,-Metrics/Training(Step): loss,1610373874618,0.08682432770729065
21476,-Metrics/Training(Step): loss,1610373876557,0.08278623223304749
21478,-Metrics/Training(Step): loss,1610373878546,0.06759576499462128
21480,-Metrics/Training(Step): loss,1610373880592,0.07778171449899673
21482,-Metrics/Training(Step): loss,1610373882642,0.09271938353776932
21484,-Metrics/Training(Step): loss,1610373884755,0.09113246947526932
21486,-Metrics/Training(Step): loss,1610373886817,0.11488225311040878
21488,-Metrics/Training(Step): loss,1610373888888,0.08341382443904877
21490,-Metrics/Training(Step): loss,1610373890757,0.06355396658182144
21492,-Metrics/Training(Step): loss,1610373892626,0.08136797696352005
21494,-Metrics/Training(Step): loss,1610373894626,0.0727229043841362
21496,-Metrics/Training(Step): loss,1610373896474,0.08913639187812805
21498,-Metrics/Training(Step): loss,1610373898507,0.11121777445077896
21500,-Metrics/Training(Step): loss,1610373900530,0.09284429997205734
21502,-Metrics/Training(Step): loss,1610373912821,0.09859712421894073
21504,-Metrics/Training(Step): loss,1610373917242,0.11620806157588959
21506,-Metrics/Training(Step): loss,1610373921617,0.0831679105758667
21508,-Metrics/Training(Step): loss,1610373928550,0.09142773598432541
21510,-Metrics/Training(Step): loss,1610373932319,0.09739181399345398
21512,-Metrics/Training(Step): loss,1610373936491,0.06735484302043915
21514,-Metrics/Training(Step): loss,1610373940064,0.10252782702445984
21516,-Metrics/Training(Step): loss,1610373943558,0.10060111433267593
21518,-Metrics/Training(Step): loss,1610373946520,0.09612668305635452
21520,-Metrics/Training(Step): loss,1610373949735,0.09443144500255585
21522,-Metrics/Training(Step): loss,1610373953226,0.10279574245214462
21524,-Metrics/Training(Step): loss,1610373957014,0.0840517207980156
21526,-Metrics/Training(Step): loss,1610373961658,0.09310449659824371
21528,-Metrics/Training(Step): loss,1610373965352,0.08344624936580658
21530,-Metrics/Training(Step): loss,1610373969065,0.09033616632223129
21532,-Metrics/Training(Step): loss,1610373972778,0.08877313137054443
21534,-Metrics/Training(Step): loss,1610373976519,0.09193295985460281
21536,-Metrics/Training(Step): loss,1610373979752,0.09332673251628876
21538,-Metrics/Training(Step): loss,1610373983720,0.09387223422527313
21540,-Metrics/Training(Step): loss,1610373987226,0.09581247717142105
21542,-Metrics/Training(Step): loss,1610373991119,0.07508917897939682
21544,-Metrics/Training(Step): loss,1610373994697,0.10678303986787796
21546,-Metrics/Training(Step): loss,1610373998119,0.08136046677827835
21548,-Metrics/Training(Step): loss,1610374001838,0.0954560711979866
21550,-Metrics/Training(Step): loss,1610374004319,0.09648480266332626
21552,-Metrics/Training(Step): loss,1610374006552,0.06704702228307724
21554,-Metrics/Training(Step): loss,1610374008638,0.08402164280414581
21556,-Metrics/Training(Step): loss,1610374010723,0.10766714811325073
21558,-Metrics/Training(Step): loss,1610374012821,0.0833473801612854
21560,-Metrics/Training(Step): loss,1610374014874,0.08327595889568329
21562,-Metrics/Training(Step): loss,1610374016842,0.07906121760606766
21564,-Metrics/Training(Step): loss,1610374018934,0.09662732481956482
21566,-Metrics/Training(Step): loss,1610374021017,0.08476397395133972
21568,-Metrics/Training(Step): loss,1610374022914,0.07230120152235031
21570,-Metrics/Training(Step): loss,1610374025031,0.08716864883899689
21572,-Metrics/Training(Step): loss,1610374026920,0.08765939623117447
21574,-Metrics/Training(Step): loss,1610374028995,0.09279706329107285
21576,-Metrics/Training(Step): loss,1610374031067,0.06552701443433762
21578,-Metrics/Training(Step): loss,1610374033093,0.0851932018995285
21580,-Metrics/Training(Step): loss,1610374035247,0.07332269847393036
21582,-Metrics/Training(Step): loss,1610374037445,0.07096955180168152
21584,-Metrics/Training(Step): loss,1610374039453,0.11050740629434586
21586,-Metrics/Training(Step): loss,1610374041448,0.08193991333246231
21588,-Metrics/Training(Step): loss,1610374054629,0.09630487114191055
21590,-Metrics/Training(Step): loss,1610374058719,0.08937244862318039
21592,-Metrics/Training(Step): loss,1610374062915,0.06850335747003555
21594,-Metrics/Training(Step): loss,1610374067357,0.09369251877069473
21596,-Metrics/Training(Step): loss,1610374071720,0.09090262651443481
21598,-Metrics/Training(Step): loss,1610374075754,0.08430439233779907
21600,-Metrics/Training(Step): loss,1610374079946,0.09514788538217545
21602,-Metrics/Training(Step): loss,1610374083720,0.08805131912231445
21604,-Metrics/Training(Step): loss,1610374087944,0.07983359694480896
21606,-Metrics/Training(Step): loss,1610374092043,0.10538606345653534
21608,-Metrics/Training(Step): loss,1610374095955,0.07263003289699554
21610,-Metrics/Training(Step): loss,1610374099499,0.09408242255449295
21612,-Metrics/Training(Step): loss,1610374102969,0.08193673938512802
21614,-Metrics/Training(Step): loss,1610374108321,0.0899713784456253
21616,-Metrics/Training(Step): loss,1610374111220,0.10660702735185623
21618,-Metrics/Training(Step): loss,1610374114515,0.0897274762392044
21620,-Metrics/Training(Step): loss,1610374118039,0.10052869468927383
21622,-Metrics/Training(Step): loss,1610374121845,0.08815543353557587
21624,-Metrics/Training(Step): loss,1610374124154,0.10367688536643982
21626,-Metrics/Training(Step): loss,1610374126944,0.07639748603105545
21628,-Metrics/Training(Step): loss,1610374129843,0.08537977933883667
21630,-Metrics/Training(Step): loss,1610374133451,0.07322453707456589
21632,-Metrics/Training(Step): loss,1610374136954,0.08546505123376846
21634,-Metrics/Training(Step): loss,1610374140158,0.08504718542098999
21636,-Metrics/Training(Step): loss,1610374143563,0.11833306401968002
21638,-Metrics/Training(Step): loss,1610374146101,0.08851802349090576
21640,-Metrics/Training(Step): loss,1610374148374,0.10008678585290909
21642,-Metrics/Training(Step): loss,1610374150596,0.08654561638832092
21644,-Metrics/Training(Step): loss,1610374152899,0.07657566666603088
21646,-Metrics/Training(Step): loss,1610374155436,0.08744756132364273
21648,-Metrics/Training(Step): loss,1610374157870,0.10333305597305298
21650,-Metrics/Training(Step): loss,1610374160190,0.08987438678741455
21652,-Metrics/Training(Step): loss,1610374162528,0.0689711719751358
21654,-Metrics/Training(Step): loss,1610374164585,0.09022752940654755
21656,-Metrics/Training(Step): loss,1610374166692,0.06131301075220108
21658,-Metrics/Training(Step): loss,1610374169017,0.08013483881950378
21660,-Metrics/Training(Step): loss,1610374171029,0.08144679665565491
21662,-Metrics/Training(Step): loss,1610374173234,0.08638379722833633
21664,-Metrics/Training(Step): loss,1610374175186,0.10356836766004562
21666,-Metrics/Training(Step): loss,1610374177133,0.09000132232904434
21668,-Metrics/Training(Step): loss,1610374179135,0.08004206418991089
21670,-Metrics/Training(Step): loss,1610374181113,0.07141026109457016
21672,-Metrics/Training(Step): loss,1610374183150,0.10938651859760284
21674,-Metrics/Training(Step): loss,1610374195944,0.09695900976657867
21676,-Metrics/Training(Step): loss,1610374200619,0.07451805472373962
21678,-Metrics/Training(Step): loss,1610374204715,0.11674490571022034
21680,-Metrics/Training(Step): loss,1610374208919,0.09050232172012329
21682,-Metrics/Training(Step): loss,1610374213791,0.0803539976477623
21684,-Metrics/Training(Step): loss,1610374218476,0.0695817768573761
21686,-Metrics/Training(Step): loss,1610374222120,0.06802213191986084
21688,-Metrics/Training(Step): loss,1610374226335,0.08295653760433197
21690,-Metrics/Training(Step): loss,1610374229987,0.08901690691709518
21692,-Metrics/Training(Step): loss,1610374233619,0.09431609511375427
21694,-Metrics/Training(Step): loss,1610374237219,0.0896536335349083
21696,-Metrics/Training(Step): loss,1610374240501,0.0887494906783104
21698,-Metrics/Training(Step): loss,1610374244319,0.09495014697313309
21700,-Metrics/Training(Step): loss,1610374248027,0.09605813026428223
21702,-Metrics/Training(Step): loss,1610374251986,0.11440611630678177
21704,-Metrics/Training(Step): loss,1610374255562,0.09995900094509125
21706,-Metrics/Training(Step): loss,1610374259220,0.08550142496824265
21708,-Metrics/Training(Step): loss,1610374262744,0.07895537465810776
21710,-Metrics/Training(Step): loss,1610374266519,0.09347556531429291
21712,-Metrics/Training(Step): loss,1610374270619,0.07397785037755966
21714,-Metrics/Training(Step): loss,1610374274639,0.08523131161928177
21716,-Metrics/Training(Step): loss,1610374277751,0.08034498244524002
21718,-Metrics/Training(Step): loss,1610374280921,0.08753260970115662
21720,-Metrics/Training(Step): loss,1610374283529,0.09040271490812302
21722,-Metrics/Training(Step): loss,1610374285892,0.10139297693967819
21724,-Metrics/Training(Step): loss,1610374288043,0.09806427359580994
21726,-Metrics/Training(Step): loss,1610374290136,0.07243356108665466
21728,-Metrics/Training(Step): loss,1610374292204,0.0857483446598053
21730,-Metrics/Training(Step): loss,1610374294314,0.10817495733499527
21732,-Metrics/Training(Step): loss,1610374296381,0.08592434227466583
21734,-Metrics/Training(Step): loss,1610374298596,0.06434067338705063
21736,-Metrics/Training(Step): loss,1610374300752,0.09000321477651596
21738,-Metrics/Training(Step): loss,1610374302940,0.08770149946212769
21740,-Metrics/Training(Step): loss,1610374305307,0.08054649084806442
21742,-Metrics/Training(Step): loss,1610374307707,0.08500088006258011
21744,-Metrics/Training(Step): loss,1610374309685,0.09232667833566666
21746,-Metrics/Training(Step): loss,1610374312012,0.09779098629951477
21748,-Metrics/Training(Step): loss,1610374314103,0.09578932076692581
21750,-Metrics/Training(Step): loss,1610374316193,0.07680268585681915
21752,-Metrics/Training(Step): loss,1610374318243,0.08116772770881653
21754,-Metrics/Training(Step): loss,1610374320262,0.07366794347763062
21756,-Metrics/Training(Step): loss,1610374322188,0.09070273488759995
21758,-Metrics/Training(Step): loss,1610374324245,0.08673033863306046
21760,-Metrics/Training(Step): loss,1610374337118,0.09110867232084274
21762,-Metrics/Training(Step): loss,1610374341319,0.08717215806245804
21764,-Metrics/Training(Step): loss,1610374345628,0.08946438878774643
21766,-Metrics/Training(Step): loss,1610374349522,0.10972049832344055
21768,-Metrics/Training(Step): loss,1610374353720,0.08729029446840286
21770,-Metrics/Training(Step): loss,1610374357719,0.10204676538705826
21772,-Metrics/Training(Step): loss,1610374361338,0.0860048308968544
21774,-Metrics/Training(Step): loss,1610374365019,0.08613120764493942
21776,-Metrics/Training(Step): loss,1610374368327,0.09826929867267609
21778,-Metrics/Training(Step): loss,1610374372019,0.08968396484851837
21780,-Metrics/Training(Step): loss,1610374375920,0.0646832138299942
21782,-Metrics/Training(Step): loss,1610374380835,0.11820807307958603
21784,-Metrics/Training(Step): loss,1610374384483,0.09392338991165161
21786,-Metrics/Training(Step): loss,1610374388421,0.07869518548250198
21788,-Metrics/Training(Step): loss,1610374391620,0.09104680269956589
21790,-Metrics/Training(Step): loss,1610374395420,0.07765315473079681
21792,-Metrics/Training(Step): loss,1610374399019,0.10089625418186188
21794,-Metrics/Training(Step): loss,1610374402519,0.084808349609375
21796,-Metrics/Training(Step): loss,1610374406319,0.08920415490865707
21798,-Metrics/Training(Step): loss,1610374409706,0.07802736759185791
21800,-Metrics/Training(Step): loss,1610374413829,0.08595079928636551
21802,-Metrics/Training(Step): loss,1610374416862,0.09756170958280563
21804,-Metrics/Training(Step): loss,1610374419180,0.08491629362106323
21806,-Metrics/Training(Step): loss,1610374422819,0.08293642848730087
21808,-Metrics/Training(Step): loss,1610374425703,0.10462566465139389
21810,-Metrics/Training(Step): loss,1610374428293,0.08997467905282974
21812,-Metrics/Training(Step): loss,1610374430459,0.09475622326135635
21814,-Metrics/Training(Step): loss,1610374432564,0.0853504166007042
21816,-Metrics/Training(Step): loss,1610374434674,0.09658703953027725
21818,-Metrics/Training(Step): loss,1610374436686,0.08867442607879639
21820,-Metrics/Training(Step): loss,1610374438668,0.10000914335250854
21822,-Metrics/Training(Step): loss,1610374440707,0.07139448821544647
21824,-Metrics/Training(Step): loss,1610374442773,0.08676990866661072
21826,-Metrics/Training(Step): loss,1610374444810,0.07997103780508041
21828,-Metrics/Training(Step): loss,1610374446933,0.10086850821971893
21830,-Metrics/Training(Step): loss,1610374448979,0.08255568146705627
21832,-Metrics/Training(Step): loss,1610374450960,0.07062958925962448
21834,-Metrics/Training(Step): loss,1610374453041,0.10384136438369751
21836,-Metrics/Training(Step): loss,1610374455070,0.10390292853116989
21838,-Metrics/Training(Step): loss,1610374457171,0.07865701615810394
21840,-Metrics/Training(Step): loss,1610374459245,0.09039931744337082
21842,-Metrics/Training(Step): loss,1610374461281,0.09407634288072586
21844,-Metrics/Training(Step): loss,1610374463307,0.10694161802530289
21846,-Metrics/Training(Step): loss,1610374475826,0.07003215700387955
21848,-Metrics/Training(Step): loss,1610374480120,0.0768727958202362
21850,-Metrics/Training(Step): loss,1610374484420,0.07761503010988235
21852,-Metrics/Training(Step): loss,1610374488625,0.08850951492786407
21854,-Metrics/Training(Step): loss,1610374492819,0.06776471436023712
21856,-Metrics/Training(Step): loss,1610374496958,0.10183469206094742
21858,-Metrics/Training(Step): loss,1610374501022,0.1043088361620903
21860,-Metrics/Training(Step): loss,1610374505220,0.08013207465410233
21862,-Metrics/Training(Step): loss,1610374509330,0.07074256241321564
21864,-Metrics/Training(Step): loss,1610374513919,0.08646557480096817
21866,-Metrics/Training(Step): loss,1610374517370,0.0931425616145134
21868,-Metrics/Training(Step): loss,1610374520937,0.09608586132526398
21870,-Metrics/Training(Step): loss,1610374524521,0.07769206911325455
21872,-Metrics/Training(Step): loss,1610374528119,0.07167652994394302
21874,-Metrics/Training(Step): loss,1610374531319,0.09440769255161285
21876,-Metrics/Training(Step): loss,1610374535019,0.08338470757007599
21878,-Metrics/Training(Step): loss,1610374538220,0.08560282737016678
21880,-Metrics/Training(Step): loss,1610374542170,0.09082598239183426
21882,-Metrics/Training(Step): loss,1610374545525,0.07388085126876831
21884,-Metrics/Training(Step): loss,1610374549618,0.08257512748241425
21886,-Metrics/Training(Step): loss,1610374552930,0.0903153195977211
21888,-Metrics/Training(Step): loss,1610374556465,0.0823824554681778
21890,-Metrics/Training(Step): loss,1610374560662,0.07844249904155731
21892,-Metrics/Training(Step): loss,1610374563134,0.09529970586299896
21894,-Metrics/Training(Step): loss,1610374565455,0.07654792815446854
21896,-Metrics/Training(Step): loss,1610374567674,0.09308548271656036
21898,-Metrics/Training(Step): loss,1610374569626,0.07159523665904999
21900,-Metrics/Training(Step): loss,1610374571716,0.06685003638267517
21902,-Metrics/Training(Step): loss,1610374573756,0.07184458523988724
21904,-Metrics/Training(Step): loss,1610374575724,0.10695786029100418
21906,-Metrics/Training(Step): loss,1610374577813,0.09661369025707245
21908,-Metrics/Training(Step): loss,1610374579804,0.077204130589962
21910,-Metrics/Training(Step): loss,1610374581918,0.10619416832923889
21912,-Metrics/Training(Step): loss,1610374583909,0.0770457461476326
21914,-Metrics/Training(Step): loss,1610374585843,0.08043797314167023
21916,-Metrics/Training(Step): loss,1610374587906,0.100715272128582
21918,-Metrics/Training(Step): loss,1610374589938,0.10267798602581024
21920,-Metrics/Training(Step): loss,1610374592042,0.08290199935436249
21922,-Metrics/Training(Step): loss,1610374594037,0.07764218002557755
21924,-Metrics/Training(Step): loss,1610374596043,0.09133642166852951
21926,-Metrics/Training(Step): loss,1610374598074,0.0769285336136818
21928,-Metrics/Training(Step): loss,1610374600129,0.08987429738044739
21930,-Metrics/Training(Step): loss,1610374601921,0.08855067938566208
21932,-Metrics/Training(Step): loss,1610374615626,0.10015407204627991
21934,-Metrics/Training(Step): loss,1610374619539,0.09677669405937195
21936,-Metrics/Training(Step): loss,1610374623419,0.06885097175836563
21938,-Metrics/Training(Step): loss,1610374627919,0.07027946412563324
21940,-Metrics/Training(Step): loss,1610374632019,0.07982975989580154
21942,-Metrics/Training(Step): loss,1610374636420,0.09177808463573456
21944,-Metrics/Training(Step): loss,1610374640612,0.09722255170345306
21946,-Metrics/Training(Step): loss,1610374644725,0.08387827128171921
21948,-Metrics/Training(Step): loss,1610374648919,0.09207897633314133
21950,-Metrics/Training(Step): loss,1610374652119,0.09222915023565292
21952,-Metrics/Training(Step): loss,1610374656350,0.10633530467748642
21954,-Metrics/Training(Step): loss,1610374660039,0.09754079580307007
21956,-Metrics/Training(Step): loss,1610374663829,0.06592819839715958
21958,-Metrics/Training(Step): loss,1610374667320,0.079618439078331
21960,-Metrics/Training(Step): loss,1610374670836,0.07998710125684738
21962,-Metrics/Training(Step): loss,1610374673620,0.06407149136066437
21964,-Metrics/Training(Step): loss,1610374676725,0.0833873301744461
21966,-Metrics/Training(Step): loss,1610374680320,0.09692151099443436
21968,-Metrics/Training(Step): loss,1610374684575,0.06930895149707794
21970,-Metrics/Training(Step): loss,1610374689948,0.09757903963327408
21972,-Metrics/Training(Step): loss,1610374693280,0.08565879613161087
21974,-Metrics/Training(Step): loss,1610374696594,0.0714707151055336
21976,-Metrics/Training(Step): loss,1610374699969,0.08248967677354813
21978,-Metrics/Training(Step): loss,1610374702948,0.08545945584774017
21980,-Metrics/Training(Step): loss,1610374705442,0.07942953705787659
21982,-Metrics/Training(Step): loss,1610374707537,0.0927053764462471
21984,-Metrics/Training(Step): loss,1610374709613,0.07862312346696854
21986,-Metrics/Training(Step): loss,1610374711683,0.09591752290725708
21988,-Metrics/Training(Step): loss,1610374713784,0.07085330784320831
21990,-Metrics/Training(Step): loss,1610374715840,0.08454149961471558
21992,-Metrics/Training(Step): loss,1610374717748,0.10444343090057373
21994,-Metrics/Training(Step): loss,1610374719598,0.10069089382886887
21996,-Metrics/Training(Step): loss,1610374721447,0.08066835254430771
21998,-Metrics/Training(Step): loss,1610374723538,0.0865604430437088
22000,-Metrics/Training(Step): loss,1610374725436,0.07902567088603973
22002,-Metrics/Training(Step): loss,1610374727407,0.07548501342535019
22004,-Metrics/Training(Step): loss,1610374729404,0.1080828458070755
22006,-Metrics/Training(Step): loss,1610374731358,0.0735149011015892
22008,-Metrics/Training(Step): loss,1610374733658,0.09201237559318542
22010,-Metrics/Training(Step): loss,1610374735946,0.08800850063562393
22012,-Metrics/Training(Step): loss,1610374738346,0.07974176108837128
22014,-Metrics/Training(Step): loss,1610374740647,0.0896148830652237
22016,-Metrics/Training(Step): loss,1610374742819,0.07979107648134232
22018,-Metrics/Training(Step): loss,1610374756132,0.09504658728837967
22020,-Metrics/Training(Step): loss,1610374760419,0.10775832086801529
22022,-Metrics/Training(Step): loss,1610374764120,0.07455738633871078
22024,-Metrics/Training(Step): loss,1610374768220,0.08496841043233871
22026,-Metrics/Training(Step): loss,1610374772519,0.08626870065927505
22028,-Metrics/Training(Step): loss,1610374776438,0.09702235460281372
22030,-Metrics/Training(Step): loss,1610374781019,0.08844921737909317
22032,-Metrics/Training(Step): loss,1610374784956,0.08172295987606049
22034,-Metrics/Training(Step): loss,1610374788676,0.10582701861858368
22036,-Metrics/Training(Step): loss,1610374792385,0.11405754089355469
22038,-Metrics/Training(Step): loss,1610374797866,0.062478769570589066
22040,-Metrics/Training(Step): loss,1610374801227,0.09109910577535629
22042,-Metrics/Training(Step): loss,1610374804764,0.06982894986867905
22044,-Metrics/Training(Step): loss,1610374808831,0.09031561762094498
22046,-Metrics/Training(Step): loss,1610374813242,0.08455844968557358
22048,-Metrics/Training(Step): loss,1610374816822,0.08208481967449188
22050,-Metrics/Training(Step): loss,1610374820800,0.09573439508676529
22052,-Metrics/Training(Step): loss,1610374823919,0.07114232331514359
22054,-Metrics/Training(Step): loss,1610374828423,0.0889839231967926
22056,-Metrics/Training(Step): loss,1610374834039,0.09735652059316635
22058,-Metrics/Training(Step): loss,1610374837761,0.09061843156814575
22060,-Metrics/Training(Step): loss,1610374840620,0.0888652503490448
22062,-Metrics/Training(Step): loss,1610374843554,0.10345800220966339
22064,-Metrics/Training(Step): loss,1610374845874,0.09529493004083633
22066,-Metrics/Training(Step): loss,1610374848046,0.09840939939022064
22068,-Metrics/Training(Step): loss,1610374850116,0.07439836859703064
22070,-Metrics/Training(Step): loss,1610374852196,0.09259121865034103
22072,-Metrics/Training(Step): loss,1610374854481,0.07754385471343994
22074,-Metrics/Training(Step): loss,1610374856835,0.08102702349424362
22076,-Metrics/Training(Step): loss,1610374859252,0.09973348677158356
22078,-Metrics/Training(Step): loss,1610374861347,0.060869697481393814
22080,-Metrics/Training(Step): loss,1610374863551,0.11263102293014526
22082,-Metrics/Training(Step): loss,1610374865871,0.09772489219903946
22084,-Metrics/Training(Step): loss,1610374868146,0.07757559418678284
22086,-Metrics/Training(Step): loss,1610374870291,0.08740147203207016
22088,-Metrics/Training(Step): loss,1610374872642,0.08610989153385162
22090,-Metrics/Training(Step): loss,1610374874698,0.09988114982843399
22092,-Metrics/Training(Step): loss,1610374876610,0.09011896699666977
22094,-Metrics/Training(Step): loss,1610374878521,0.0795641541481018
22096,-Metrics/Training(Step): loss,1610374880496,0.08831827342510223
22098,-Metrics/Training(Step): loss,1610374882508,0.1029329001903534
22100,-Metrics/Training(Step): loss,1610374884557,0.0896294042468071
22102,-Metrics/Training(Step): loss,1610374886409,0.0949474573135376
22104,-Metrics/Training(Step): loss,1610374898932,0.09043949097394943
22106,-Metrics/Training(Step): loss,1610374902819,0.08082873374223709
22108,-Metrics/Training(Step): loss,1610374906619,0.07060268521308899
22110,-Metrics/Training(Step): loss,1610374912120,0.09767432510852814
22112,-Metrics/Training(Step): loss,1610374916252,0.09872407466173172
22114,-Metrics/Training(Step): loss,1610374920524,0.09506173431873322
22116,-Metrics/Training(Step): loss,1610374925519,0.07144425064325333
22118,-Metrics/Training(Step): loss,1610374929952,0.09397812932729721
22120,-Metrics/Training(Step): loss,1610374933319,0.08313880115747452
22122,-Metrics/Training(Step): loss,1610374937320,0.08095801621675491
22124,-Metrics/Training(Step): loss,1610374941349,0.07718896120786667
22126,-Metrics/Training(Step): loss,1610374944920,0.0848960354924202
22128,-Metrics/Training(Step): loss,1610374948453,0.07868530601263046
22130,-Metrics/Training(Step): loss,1610374952046,0.10284850001335144
22132,-Metrics/Training(Step): loss,1610374955532,0.07953827828168869
22134,-Metrics/Training(Step): loss,1610374958956,0.08903895318508148
22136,-Metrics/Training(Step): loss,1610374963308,0.08504541218280792
22138,-Metrics/Training(Step): loss,1610374966782,0.0934319719672203
22140,-Metrics/Training(Step): loss,1610374970348,0.09773703664541245
22142,-Metrics/Training(Step): loss,1610374973823,0.10106265544891357
22144,-Metrics/Training(Step): loss,1610374978140,0.09222622215747833
22146,-Metrics/Training(Step): loss,1610374981920,0.09565567970275879
22148,-Metrics/Training(Step): loss,1610374986935,0.0911957323551178
22150,-Metrics/Training(Step): loss,1610374989638,0.06535295397043228
22152,-Metrics/Training(Step): loss,1610374992141,0.09193099290132523
22154,-Metrics/Training(Step): loss,1610374994550,0.07403779029846191
22156,-Metrics/Training(Step): loss,1610374996880,0.07480872422456741
22158,-Metrics/Training(Step): loss,1610374999281,0.10845822095870972
22160,-Metrics/Training(Step): loss,1610375001573,0.10657640546560287
22162,-Metrics/Training(Step): loss,1610375004030,0.06972693651914597
22164,-Metrics/Training(Step): loss,1610375006505,0.09512442350387573
22166,-Metrics/Training(Step): loss,1610375008630,0.08798018097877502
22168,-Metrics/Training(Step): loss,1610375010802,0.08482759445905685
22170,-Metrics/Training(Step): loss,1610375012837,0.11130382865667343
22172,-Metrics/Training(Step): loss,1610375015057,0.08004205673933029
22174,-Metrics/Training(Step): loss,1610375017330,0.08841980248689651
22176,-Metrics/Training(Step): loss,1610375019428,0.08370425552129745
22178,-Metrics/Training(Step): loss,1610375021491,0.092778779566288
22180,-Metrics/Training(Step): loss,1610375023457,0.09673129767179489
22182,-Metrics/Training(Step): loss,1610375025400,0.09621364623308182
22184,-Metrics/Training(Step): loss,1610375027394,0.08359836786985397
22186,-Metrics/Training(Step): loss,1610375029418,0.07955484837293625
22188,-Metrics/Training(Step): loss,1610375031461,0.0793011337518692
22190,-Metrics/Training(Step): loss,1610375044442,0.09077572077512741
22192,-Metrics/Training(Step): loss,1610375048820,0.07408053427934647
22194,-Metrics/Training(Step): loss,1610375052857,0.06290395557880402
22196,-Metrics/Training(Step): loss,1610375057015,0.0801684707403183
22198,-Metrics/Training(Step): loss,1610375060643,0.08443176001310349
22200,-Metrics/Training(Step): loss,1610375064427,0.09680113941431046
22202,-Metrics/Training(Step): loss,1610375068623,0.10560588538646698
22204,-Metrics/Training(Step): loss,1610375072736,0.11002115160226822
22206,-Metrics/Training(Step): loss,1610375076560,0.07546503096818924
22208,-Metrics/Training(Step): loss,1610375079977,0.08308187127113342
22210,-Metrics/Training(Step): loss,1610375082350,0.056839197874069214
22212,-Metrics/Training(Step): loss,1610375085501,0.10598454624414444
22214,-Metrics/Training(Step): loss,1610375089020,0.08010606467723846
22216,-Metrics/Training(Step): loss,1610375092820,0.061665814369916916
22218,-Metrics/Training(Step): loss,1610375096819,0.08314346522092819
22220,-Metrics/Training(Step): loss,1610375100466,0.07657402753829956
22222,-Metrics/Training(Step): loss,1610375104158,0.097294382750988
22224,-Metrics/Training(Step): loss,1610375107884,0.0723048746585846
22226,-Metrics/Training(Step): loss,1610375111372,0.09211350977420807
22228,-Metrics/Training(Step): loss,1610375115092,0.10053010284900665
22230,-Metrics/Training(Step): loss,1610375119015,0.0904998779296875
22232,-Metrics/Training(Step): loss,1610375121819,0.07388168573379517
22234,-Metrics/Training(Step): loss,1610375125092,0.09145752340555191
22236,-Metrics/Training(Step): loss,1610375127584,0.08223665505647659
22238,-Metrics/Training(Step): loss,1610375129924,0.09647363424301147
22240,-Metrics/Training(Step): loss,1610375132105,0.1018216609954834
22242,-Metrics/Training(Step): loss,1610375134135,0.0781942829489708
22244,-Metrics/Training(Step): loss,1610375136166,0.08149416744709015
22246,-Metrics/Training(Step): loss,1610375138279,0.09175723046064377
22248,-Metrics/Training(Step): loss,1610375140282,0.08887740969657898
22250,-Metrics/Training(Step): loss,1610375142123,0.1084810122847557
22252,-Metrics/Training(Step): loss,1610375144189,0.08248686045408249
22254,-Metrics/Training(Step): loss,1610375146185,0.09067748486995697
22256,-Metrics/Training(Step): loss,1610375148257,0.07953618466854095
22258,-Metrics/Training(Step): loss,1610375150334,0.09304805845022202
22260,-Metrics/Training(Step): loss,1610375152432,0.09793554991483688
22262,-Metrics/Training(Step): loss,1610375154405,0.09669508785009384
22264,-Metrics/Training(Step): loss,1610375156365,0.09470906853675842
22266,-Metrics/Training(Step): loss,1610375158428,0.07085039466619492
22268,-Metrics/Training(Step): loss,1610375160395,0.09293685108423233
22270,-Metrics/Training(Step): loss,1610375162434,0.08771760016679764
22272,-Metrics/Training(Step): loss,1610375164500,0.09220050275325775
22274,-Metrics/Training(Step): loss,1610375166284,0.0679101049900055
22276,-Metrics/Training(Step): loss,1610375178826,0.0844535157084465
22278,-Metrics/Training(Step): loss,1610375183119,0.07396873086690903
22280,-Metrics/Training(Step): loss,1610375187243,0.08495420217514038
22282,-Metrics/Training(Step): loss,1610375191326,0.0897660180926323
22284,-Metrics/Training(Step): loss,1610375195015,0.09560397267341614
22286,-Metrics/Training(Step): loss,1610375199020,0.09196408092975616
22288,-Metrics/Training(Step): loss,1610375203319,0.08198326081037521
22290,-Metrics/Training(Step): loss,1610375207321,0.07003617286682129
22292,-Metrics/Training(Step): loss,1610375211221,0.08682646602392197
22294,-Metrics/Training(Step): loss,1610375214722,0.09140221774578094
22296,-Metrics/Training(Step): loss,1610375218474,0.07058516889810562
22298,-Metrics/Training(Step): loss,1610375221993,0.08885540068149567
22300,-Metrics/Training(Step): loss,1610375225820,0.07380969077348709
22302,-Metrics/Training(Step): loss,1610375229140,0.07499252259731293
22304,-Metrics/Training(Step): loss,1610375233120,0.09412286430597305
22306,-Metrics/Training(Step): loss,1610375236634,0.08376378566026688
22308,-Metrics/Training(Step): loss,1610375240525,0.09143996238708496
22310,-Metrics/Training(Step): loss,1610375244219,0.09049561619758606
22312,-Metrics/Training(Step): loss,1610375247991,0.07527108490467072
22314,-Metrics/Training(Step): loss,1610375251625,0.08538004755973816
22316,-Metrics/Training(Step): loss,1610375254819,0.07159912586212158
22318,-Metrics/Training(Step): loss,1610375258066,0.09509696811437607
22320,-Metrics/Training(Step): loss,1610375261880,0.12685027718544006
22322,-Metrics/Training(Step): loss,1610375264194,0.059309862554073334
22324,-Metrics/Training(Step): loss,1610375266369,0.0871746763586998
22326,-Metrics/Training(Step): loss,1610375268501,0.10850679129362106
22328,-Metrics/Training(Step): loss,1610375270602,0.09194853901863098
22330,-Metrics/Training(Step): loss,1610375272690,0.09796003997325897
22332,-Metrics/Training(Step): loss,1610375274677,0.06924772262573242
22334,-Metrics/Training(Step): loss,1610375276602,0.0955422893166542
22336,-Metrics/Training(Step): loss,1610375278640,0.09012597054243088
22338,-Metrics/Training(Step): loss,1610375280654,0.10222690552473068
22340,-Metrics/Training(Step): loss,1610375282534,0.0775805190205574
22342,-Metrics/Training(Step): loss,1610375284522,0.08265901356935501
22344,-Metrics/Training(Step): loss,1610375286392,0.08383841067552567
22346,-Metrics/Training(Step): loss,1610375288465,0.08410182595252991
22348,-Metrics/Training(Step): loss,1610375290509,0.0782494992017746
22350,-Metrics/Training(Step): loss,1610375292618,0.07925672829151154
22352,-Metrics/Training(Step): loss,1610375294580,0.09242001175880432
22354,-Metrics/Training(Step): loss,1610375296616,0.0818227082490921
22356,-Metrics/Training(Step): loss,1610375298576,0.07597754895687103
22358,-Metrics/Training(Step): loss,1610375300561,0.07883051782846451
22360,-Metrics/Training(Step): loss,1610375302533,0.07969743758440018
22362,-Metrics/Training(Step): loss,1610375314732,0.09078296273946762
22364,-Metrics/Training(Step): loss,1610375318919,0.08345147967338562
22366,-Metrics/Training(Step): loss,1610375322819,0.08708266168832779
22368,-Metrics/Training(Step): loss,1610375326619,0.10831062495708466
22370,-Metrics/Training(Step): loss,1610375330419,0.10071391612291336
22372,-Metrics/Training(Step): loss,1610375334426,0.0884849801659584
22374,-Metrics/Training(Step): loss,1610375337918,0.08468708395957947
22376,-Metrics/Training(Step): loss,1610375341718,0.11060656607151031
22378,-Metrics/Training(Step): loss,1610375345628,0.08020278066396713
22380,-Metrics/Training(Step): loss,1610375349159,0.10776852816343307
22382,-Metrics/Training(Step): loss,1610375353146,0.0737159326672554
22384,-Metrics/Training(Step): loss,1610375356919,0.09679414331912994
22386,-Metrics/Training(Step): loss,1610375360321,0.09170491248369217
22388,-Metrics/Training(Step): loss,1610375363619,0.07976926863193512
22390,-Metrics/Training(Step): loss,1610375366750,0.08599855750799179
22392,-Metrics/Training(Step): loss,1610375370319,0.09297513216733932
22394,-Metrics/Training(Step): loss,1610375373620,0.0902300477027893
22396,-Metrics/Training(Step): loss,1610375377145,0.07373219728469849
22398,-Metrics/Training(Step): loss,1610375381120,0.09594908356666565
22400,-Metrics/Training(Step): loss,1610375385219,0.0777416080236435
22402,-Metrics/Training(Step): loss,1610375388720,0.09297607839107513
22404,-Metrics/Training(Step): loss,1610375391841,0.10576708614826202
22406,-Metrics/Training(Step): loss,1610375395353,0.09586521238088608
22408,-Metrics/Training(Step): loss,1610375398537,0.08065664768218994
22410,-Metrics/Training(Step): loss,1610375400900,0.08126943558454514
22412,-Metrics/Training(Step): loss,1610375402901,0.08486514538526535
22414,-Metrics/Training(Step): loss,1610375405034,0.10617397725582123
22416,-Metrics/Training(Step): loss,1610375407116,0.08818433433771133
22418,-Metrics/Training(Step): loss,1610375409208,0.09450294822454453
22420,-Metrics/Training(Step): loss,1610375411310,0.0856219008564949
22422,-Metrics/Training(Step): loss,1610375413380,0.09571550786495209
22424,-Metrics/Training(Step): loss,1610375415447,0.061204783618450165
22426,-Metrics/Training(Step): loss,1610375417486,0.10592391341924667
22428,-Metrics/Training(Step): loss,1610375419586,0.08931273967027664
22430,-Metrics/Training(Step): loss,1610375421680,0.07610782980918884
22432,-Metrics/Training(Step): loss,1610375423769,0.08541801571846008
22434,-Metrics/Training(Step): loss,1610375425761,0.08135396987199783
22436,-Metrics/Training(Step): loss,1610375427804,0.0822511538863182
22438,-Metrics/Training(Step): loss,1610375429825,0.08854334056377411
22440,-Metrics/Training(Step): loss,1610375431844,0.06578981131315231
22442,-Metrics/Training(Step): loss,1610375433859,0.09453840553760529
22444,-Metrics/Training(Step): loss,1610375435830,0.0874059870839119
22446,-Metrics/Training(Step): loss,1610375437868,0.07093003392219543
22448,-Metrics/Training(Step): loss,1610375450830,0.09588485956192017
22450,-Metrics/Training(Step): loss,1610375454920,0.09218164533376694
22452,-Metrics/Training(Step): loss,1610375458947,0.07388369739055634
22454,-Metrics/Training(Step): loss,1610375462720,0.07474382221698761
22456,-Metrics/Training(Step): loss,1610375466856,0.09350172430276871
22458,-Metrics/Training(Step): loss,1610375470927,0.0959065780043602
22460,-Metrics/Training(Step): loss,1610375474635,0.08209314197301865
22462,-Metrics/Training(Step): loss,1610375478229,0.12181676924228668
22464,-Metrics/Training(Step): loss,1610375482269,0.103788360953331
22466,-Metrics/Training(Step): loss,1610375485619,0.10284711420536041
22468,-Metrics/Training(Step): loss,1610375489021,0.05843223258852959
22470,-Metrics/Training(Step): loss,1610375492698,0.0858132615685463
22472,-Metrics/Training(Step): loss,1610375496719,0.0816517025232315
22474,-Metrics/Training(Step): loss,1610375499619,0.07538456469774246
22476,-Metrics/Training(Step): loss,1610375503452,0.06863275170326233
22478,-Metrics/Training(Step): loss,1610375506830,0.10301729291677475
22480,-Metrics/Training(Step): loss,1610375510420,0.08168274164199829
22482,-Metrics/Training(Step): loss,1610375513819,0.08936481177806854
22484,-Metrics/Training(Step): loss,1610375517319,0.09921266883611679
22486,-Metrics/Training(Step): loss,1610375521178,0.059584833681583405
22488,-Metrics/Training(Step): loss,1610375524530,0.08792397379875183
22490,-Metrics/Training(Step): loss,1610375528180,0.08515702188014984
22492,-Metrics/Training(Step): loss,1610375531152,0.08083787560462952
22494,-Metrics/Training(Step): loss,1610375533966,0.08269795775413513
22496,-Metrics/Training(Step): loss,1610375536265,0.07301732152700424
22498,-Metrics/Training(Step): loss,1610375538286,0.10250262916088104
22500,-Metrics/Training(Step): loss,1610375540394,0.09530407190322876
22502,-Metrics/Training(Step): loss,1610375542498,0.0849226862192154
22504,-Metrics/Training(Step): loss,1610375544554,0.10048038512468338
22506,-Metrics/Training(Step): loss,1610375546656,0.07514757663011551
22508,-Metrics/Training(Step): loss,1610375548713,0.08900316804647446
22510,-Metrics/Training(Step): loss,1610375550802,0.09100185334682465
22512,-Metrics/Training(Step): loss,1610375552868,0.08262796700000763
22514,-Metrics/Training(Step): loss,1610375554886,0.08867522329092026
22516,-Metrics/Training(Step): loss,1610375556728,0.08170616626739502
22518,-Metrics/Training(Step): loss,1610375558812,0.08749070018529892
22520,-Metrics/Training(Step): loss,1610375560865,0.08930417895317078
22522,-Metrics/Training(Step): loss,1610375562950,0.09029606729745865
22524,-Metrics/Training(Step): loss,1610375565020,0.08549483865499496
22526,-Metrics/Training(Step): loss,1610375567052,0.08037620782852173
22528,-Metrics/Training(Step): loss,1610375569067,0.08046058565378189
22530,-Metrics/Training(Step): loss,1610375571154,0.0719708800315857
22532,-Metrics/Training(Step): loss,1610375573112,0.11213556677103043
22534,-Metrics/Training(Step): loss,1610375585818,0.0959618091583252
22536,-Metrics/Training(Step): loss,1610375589925,0.07571780681610107
22538,-Metrics/Training(Step): loss,1610375594019,0.08413312584161758
22540,-Metrics/Training(Step): loss,1610375598035,0.08830548822879791
22542,-Metrics/Training(Step): loss,1610375602032,0.08538772910833359
22544,-Metrics/Training(Step): loss,1610375605919,0.0608515739440918
22546,-Metrics/Training(Step): loss,1610375609222,0.0810265764594078
22548,-Metrics/Training(Step): loss,1610375613387,0.10061691701412201
22550,-Metrics/Training(Step): loss,1610375617028,0.07852120697498322
22552,-Metrics/Training(Step): loss,1610375620737,0.1038181334733963
22554,-Metrics/Training(Step): loss,1610375624152,0.08611124753952026
22556,-Metrics/Training(Step): loss,1610375628120,0.08013813197612762
22558,-Metrics/Training(Step): loss,1610375631519,0.07365274429321289
22560,-Metrics/Training(Step): loss,1610375635275,0.07048678398132324
22562,-Metrics/Training(Step): loss,1610375638746,0.07021195441484451
22564,-Metrics/Training(Step): loss,1610375642089,0.08855659514665604
22566,-Metrics/Training(Step): loss,1610375645449,0.09652791917324066
22568,-Metrics/Training(Step): loss,1610375648842,0.0828218162059784
22570,-Metrics/Training(Step): loss,1610375652019,0.0738971084356308
22572,-Metrics/Training(Step): loss,1610375655819,0.08427897840738297
22574,-Metrics/Training(Step): loss,1610375659768,0.08810816705226898
22576,-Metrics/Training(Step): loss,1610375663320,0.08407959342002869
22578,-Metrics/Training(Step): loss,1610375666943,0.08036541193723679
22580,-Metrics/Training(Step): loss,1610375669815,0.08636578917503357
22582,-Metrics/Training(Step): loss,1610375672023,0.09478654712438583
22584,-Metrics/Training(Step): loss,1610375674048,0.09787460416555405
22586,-Metrics/Training(Step): loss,1610375675958,0.09424161165952682
22588,-Metrics/Training(Step): loss,1610375678056,0.06908180564641953
22590,-Metrics/Training(Step): loss,1610375680111,0.08099698275327682
22592,-Metrics/Training(Step): loss,1610375682272,0.0914958119392395
22594,-Metrics/Training(Step): loss,1610375684439,0.1015736535191536
22596,-Metrics/Training(Step): loss,1610375686411,0.07811552286148071
22598,-Metrics/Training(Step): loss,1610375688477,0.09498851746320724
22600,-Metrics/Training(Step): loss,1610375690327,0.09560544788837433
22602,-Metrics/Training(Step): loss,1610375692239,0.08482521027326584
22604,-Metrics/Training(Step): loss,1610375694195,0.08784151822328568
22606,-Metrics/Training(Step): loss,1610375696170,0.06758751720190048
22608,-Metrics/Training(Step): loss,1610375698275,0.11610601842403412
22610,-Metrics/Training(Step): loss,1610375700322,0.1056736633181572
22612,-Metrics/Training(Step): loss,1610375702326,0.07022210210561752
22614,-Metrics/Training(Step): loss,1610375704290,0.07058094441890717
22616,-Metrics/Training(Step): loss,1610375706396,0.09599979966878891
22618,-Metrics/Training(Step): loss,1610375708454,0.08250351250171661
22620,-Metrics/Training(Step): loss,1610375721649,0.06928621232509613
22622,-Metrics/Training(Step): loss,1610375725320,0.08675218373537064
22624,-Metrics/Training(Step): loss,1610375729528,0.08764252811670303
22626,-Metrics/Training(Step): loss,1610375733217,0.08376464247703552
22628,-Metrics/Training(Step): loss,1610375737058,0.07319191098213196
22630,-Metrics/Training(Step): loss,1610375741158,0.07065264135599136
22632,-Metrics/Training(Step): loss,1610375745120,0.09153179079294205
22634,-Metrics/Training(Step): loss,1610375749118,0.08741164207458496
22636,-Metrics/Training(Step): loss,1610375752987,0.07945846021175385
22638,-Metrics/Training(Step): loss,1610375756821,0.07424560189247131
22640,-Metrics/Training(Step): loss,1610375760122,0.09343796968460083
22642,-Metrics/Training(Step): loss,1610375763820,0.10641512274742126
22644,-Metrics/Training(Step): loss,1610375767758,0.08045365661382675
22646,-Metrics/Training(Step): loss,1610375771222,0.09600546956062317
22648,-Metrics/Training(Step): loss,1610375775104,0.09427886456251144
22650,-Metrics/Training(Step): loss,1610375778472,0.08946920186281204
22652,-Metrics/Training(Step): loss,1610375782522,0.08170714974403381
22654,-Metrics/Training(Step): loss,1610375785918,0.09255478531122208
22656,-Metrics/Training(Step): loss,1610375789392,0.10930702090263367
22658,-Metrics/Training(Step): loss,1610375793219,0.07452557981014252
22660,-Metrics/Training(Step): loss,1610375796709,0.0696936547756195
22662,-Metrics/Training(Step): loss,1610375800138,0.09512633085250854
22664,-Metrics/Training(Step): loss,1610375803452,0.08663434535264969
22666,-Metrics/Training(Step): loss,1610375805991,0.08328401297330856
22668,-Metrics/Training(Step): loss,1610375808083,0.09275166690349579
22670,-Metrics/Training(Step): loss,1610375810100,0.08490137755870819
22672,-Metrics/Training(Step): loss,1610375812149,0.10721169412136078
22674,-Metrics/Training(Step): loss,1610375814249,0.0904398262500763
22676,-Metrics/Training(Step): loss,1610375816316,0.10541078448295593
22678,-Metrics/Training(Step): loss,1610375818347,0.08680195361375809
22680,-Metrics/Training(Step): loss,1610375820412,0.08318191021680832
22682,-Metrics/Training(Step): loss,1610375822508,0.1041596457362175
22684,-Metrics/Training(Step): loss,1610375824382,0.09238516539335251
22686,-Metrics/Training(Step): loss,1610375826485,0.07665026187896729
22688,-Metrics/Training(Step): loss,1610375828577,0.08624367415904999
22690,-Metrics/Training(Step): loss,1610375830573,0.09176954627037048
22692,-Metrics/Training(Step): loss,1610375832654,0.06952882558107376
22694,-Metrics/Training(Step): loss,1610375834730,0.08708609640598297
22696,-Metrics/Training(Step): loss,1610375836753,0.09908848255872726
22698,-Metrics/Training(Step): loss,1610375838848,0.07550331950187683
22700,-Metrics/Training(Step): loss,1610375840861,0.09725546091794968
22702,-Metrics/Training(Step): loss,1610375842942,0.08290980756282806
22704,-Metrics/Training(Step): loss,1610375845001,0.07962973415851593
22706,-Metrics/Training(Step): loss,1610375857535,0.08756937086582184
22708,-Metrics/Training(Step): loss,1610375862336,0.09403504431247711
22710,-Metrics/Training(Step): loss,1610375866019,0.07505118101835251
22712,-Metrics/Training(Step): loss,1610375870033,0.09383083879947662
22714,-Metrics/Training(Step): loss,1610375874245,0.08055152744054794
22716,-Metrics/Training(Step): loss,1610375878178,0.10156133025884628
22718,-Metrics/Training(Step): loss,1610375881679,0.09258786588907242
22720,-Metrics/Training(Step): loss,1610375885519,0.10057755559682846
22722,-Metrics/Training(Step): loss,1610375888682,0.09463392198085785
22724,-Metrics/Training(Step): loss,1610375892719,0.08679842948913574
22726,-Metrics/Training(Step): loss,1610375895843,0.07200992852449417
22728,-Metrics/Training(Step): loss,1610375899238,0.07806294411420822
22730,-Metrics/Training(Step): loss,1610375902705,0.08145301043987274
22732,-Metrics/Training(Step): loss,1610375906028,0.08761235326528549
22734,-Metrics/Training(Step): loss,1610375909720,0.100972980260849
22736,-Metrics/Training(Step): loss,1610375913039,0.10380257666110992
22738,-Metrics/Training(Step): loss,1610375916519,0.1025964692234993
22740,-Metrics/Training(Step): loss,1610375919953,0.09485263377428055
22742,-Metrics/Training(Step): loss,1610375924119,0.08095609396696091
22744,-Metrics/Training(Step): loss,1610375927442,0.0805727019906044
22746,-Metrics/Training(Step): loss,1610375930720,0.10411433130502701
22748,-Metrics/Training(Step): loss,1610375934451,0.07414441555738449
22750,-Metrics/Training(Step): loss,1610375938228,0.08896397799253464
22752,-Metrics/Training(Step): loss,1610375941235,0.09680923819541931
22754,-Metrics/Training(Step): loss,1610375943737,0.08008866757154465
22756,-Metrics/Training(Step): loss,1610375945703,0.08267178386449814
22758,-Metrics/Training(Step): loss,1610375947825,0.08643548935651779
22760,-Metrics/Training(Step): loss,1610375949819,0.08411732316017151
22762,-Metrics/Training(Step): loss,1610375951914,0.09001920372247696
22764,-Metrics/Training(Step): loss,1610375953964,0.09104956686496735
22766,-Metrics/Training(Step): loss,1610375956071,0.10157763957977295
22768,-Metrics/Training(Step): loss,1610375958141,0.07369741052389145
22770,-Metrics/Training(Step): loss,1610375960203,0.0937013104557991
22772,-Metrics/Training(Step): loss,1610375962209,0.06638026237487793
22774,-Metrics/Training(Step): loss,1610375964121,0.07703392952680588
22776,-Metrics/Training(Step): loss,1610375966196,0.058608297258615494
22778,-Metrics/Training(Step): loss,1610375968055,0.07444385439157486
22780,-Metrics/Training(Step): loss,1610375970113,0.06793000549077988
22782,-Metrics/Training(Step): loss,1610375972082,0.09604069590568542
22784,-Metrics/Training(Step): loss,1610375974051,0.0728970319032669
22786,-Metrics/Training(Step): loss,1610375976123,0.0913441851735115
22788,-Metrics/Training(Step): loss,1610375978149,0.08991555869579315
22790,-Metrics/Training(Step): loss,1610375980207,0.07373356819152832
22792,-Metrics/Training(Step): loss,1610375992546,0.07981082051992416
22794,-Metrics/Training(Step): loss,1610375996819,0.09093564003705978
22796,-Metrics/Training(Step): loss,1610376000619,0.09496870636940002
22798,-Metrics/Training(Step): loss,1610376005321,0.07356201112270355
22800,-Metrics/Training(Step): loss,1610376009419,0.09652914851903915
22802,-Metrics/Training(Step): loss,1610376013229,0.061545539647340775
22804,-Metrics/Training(Step): loss,1610376017826,0.08222281187772751
22806,-Metrics/Training(Step): loss,1610376021916,0.10434836894273758
22808,-Metrics/Training(Step): loss,1610376025321,0.0900450199842453
22810,-Metrics/Training(Step): loss,1610376028419,0.07161717116832733
22812,-Metrics/Training(Step): loss,1610376031847,0.08471516519784927
22814,-Metrics/Training(Step): loss,1610376035635,0.09256749600172043
22816,-Metrics/Training(Step): loss,1610376039374,0.0998513400554657
22818,-Metrics/Training(Step): loss,1610376043217,0.08447687327861786
22820,-Metrics/Training(Step): loss,1610376046340,0.10268068313598633
22822,-Metrics/Training(Step): loss,1610376049717,0.0788331851363182
22824,-Metrics/Training(Step): loss,1610376053205,0.06177001819014549
22826,-Metrics/Training(Step): loss,1610376056843,0.07929875701665878
22828,-Metrics/Training(Step): loss,1610376060120,0.07479314506053925
22830,-Metrics/Training(Step): loss,1610376063919,0.09554138034582138
22832,-Metrics/Training(Step): loss,1610376067020,0.07927880436182022
22834,-Metrics/Training(Step): loss,1610376070420,0.09194642305374146
22836,-Metrics/Training(Step): loss,1610376073719,0.0940946638584137
22838,-Metrics/Training(Step): loss,1610376076426,0.10481879115104675
22840,-Metrics/Training(Step): loss,1610376078425,0.08820763230323792
22842,-Metrics/Training(Step): loss,1610376080528,0.08559087663888931
22844,-Metrics/Training(Step): loss,1610376082606,0.07201977074146271
22846,-Metrics/Training(Step): loss,1610376084694,0.09691310673952103
22848,-Metrics/Training(Step): loss,1610376086795,0.08653248101472855
22850,-Metrics/Training(Step): loss,1610376088903,0.07719120383262634
22852,-Metrics/Training(Step): loss,1610376090975,0.08655930310487747
22854,-Metrics/Training(Step): loss,1610376093046,0.09237265586853027
22856,-Metrics/Training(Step): loss,1610376095085,0.0634663850069046
22858,-Metrics/Training(Step): loss,1610376097100,0.09187078475952148
22860,-Metrics/Training(Step): loss,1610376099036,0.0807931125164032
22862,-Metrics/Training(Step): loss,1610376101009,0.06704389303922653
22864,-Metrics/Training(Step): loss,1610376103109,0.09272748231887817
22866,-Metrics/Training(Step): loss,1610376105114,0.08189965039491653
22868,-Metrics/Training(Step): loss,1610376107161,0.08589968085289001
22870,-Metrics/Training(Step): loss,1610376109161,0.06183389201760292
22872,-Metrics/Training(Step): loss,1610376111214,0.09975355118513107
22874,-Metrics/Training(Step): loss,1610376113100,0.08643436431884766
22876,-Metrics/Training(Step): loss,1610376115112,0.07857748121023178
22878,-Metrics/Training(Step): loss,1610376127238,0.07258273661136627
22880,-Metrics/Training(Step): loss,1610376131354,0.09282958507537842
22882,-Metrics/Training(Step): loss,1610376135322,0.09473883360624313
22884,-Metrics/Training(Step): loss,1610376139519,0.08509945869445801
22886,-Metrics/Training(Step): loss,1610376143629,0.09576968103647232
22888,-Metrics/Training(Step): loss,1610376147820,0.10048249363899231
22890,-Metrics/Training(Step): loss,1610376151919,0.0882941260933876
22892,-Metrics/Training(Step): loss,1610376155813,0.09123998135328293
22894,-Metrics/Training(Step): loss,1610376159194,0.08901207149028778
22896,-Metrics/Training(Step): loss,1610376162527,0.071650430560112
22898,-Metrics/Training(Step): loss,1610376166237,0.07130873948335648
22900,-Metrics/Training(Step): loss,1610376169821,0.07783389091491699
22902,-Metrics/Training(Step): loss,1610376173321,0.1000489592552185
22904,-Metrics/Training(Step): loss,1610376177031,0.1111501082777977
22906,-Metrics/Training(Step): loss,1610376180698,0.07044373452663422
22908,-Metrics/Training(Step): loss,1610376184027,0.09502798318862915
22910,-Metrics/Training(Step): loss,1610376187721,0.09127750992774963
22912,-Metrics/Training(Step): loss,1610376190944,0.07519756257534027
22914,-Metrics/Training(Step): loss,1610376194619,0.09272957593202591
22916,-Metrics/Training(Step): loss,1610376198246,0.0629151239991188
22918,-Metrics/Training(Step): loss,1610376201926,0.09041483700275421
22920,-Metrics/Training(Step): loss,1610376206034,0.09942903369665146
22922,-Metrics/Training(Step): loss,1610376209565,0.0998309776186943
22924,-Metrics/Training(Step): loss,1610376212514,0.06733566522598267
22926,-Metrics/Training(Step): loss,1610376214580,0.09175977855920792
22928,-Metrics/Training(Step): loss,1610376216607,0.10416334122419357
22930,-Metrics/Training(Step): loss,1610376218718,0.07171463966369629
22932,-Metrics/Training(Step): loss,1610376220911,0.0861867219209671
22934,-Metrics/Training(Step): loss,1610376223081,0.08702755719423294
22936,-Metrics/Training(Step): loss,1610376225165,0.0835963562130928
22938,-Metrics/Training(Step): loss,1610376227245,0.07198067009449005
22940,-Metrics/Training(Step): loss,1610376229313,0.08241870254278183
22942,-Metrics/Training(Step): loss,1610376231213,0.09719710052013397
22944,-Metrics/Training(Step): loss,1610376233213,0.08786743879318237
22946,-Metrics/Training(Step): loss,1610376235283,0.08480934053659439
22948,-Metrics/Training(Step): loss,1610376237256,0.10357396304607391
22950,-Metrics/Training(Step): loss,1610376239339,0.08504989743232727
22952,-Metrics/Training(Step): loss,1610376241407,0.08036725968122482
22954,-Metrics/Training(Step): loss,1610376243428,0.08640230447053909
22956,-Metrics/Training(Step): loss,1610376245406,0.08283506333827972
22958,-Metrics/Training(Step): loss,1610376247344,0.07143726944923401
22960,-Metrics/Training(Step): loss,1610376249411,0.08223535120487213
22962,-Metrics/Training(Step): loss,1610376251409,0.10938162356615067
22964,-Metrics/Training(Step): loss,1610376263322,0.07884712517261505
22966,-Metrics/Training(Step): loss,1610376267419,0.09260906279087067
22968,-Metrics/Training(Step): loss,1610376271547,0.0755029171705246
22970,-Metrics/Training(Step): loss,1610376275244,0.06564350426197052
22972,-Metrics/Training(Step): loss,1610376279120,0.08961743116378784
22974,-Metrics/Training(Step): loss,1610376283231,0.07920966297388077
22976,-Metrics/Training(Step): loss,1610376287345,0.08397304266691208
22978,-Metrics/Training(Step): loss,1610376291420,0.07501120865345001
22980,-Metrics/Training(Step): loss,1610376295454,0.07361432909965515
22982,-Metrics/Training(Step): loss,1610376299321,0.07288973033428192
22984,-Metrics/Training(Step): loss,1610376302505,0.08966896682977676
22986,-Metrics/Training(Step): loss,1610376305534,0.10135549306869507
22988,-Metrics/Training(Step): loss,1610376309725,0.08284017443656921
22990,-Metrics/Training(Step): loss,1610376313615,0.07239081710577011
22992,-Metrics/Training(Step): loss,1610376317619,0.0847816988825798
22994,-Metrics/Training(Step): loss,1610376321041,0.08819866180419922
22996,-Metrics/Training(Step): loss,1610376324849,0.0638953298330307
22998,-Metrics/Training(Step): loss,1610376328246,0.08958202600479126
23000,-Metrics/Training(Step): loss,1610376332219,0.09838531166315079
23002,-Metrics/Training(Step): loss,1610376336004,0.09229745715856552
23004,-Metrics/Training(Step): loss,1610376339319,0.08551845699548721
23006,-Metrics/Training(Step): loss,1610376343193,0.10147423297166824
23008,-Metrics/Training(Step): loss,1610376346172,0.09442374110221863
23010,-Metrics/Training(Step): loss,1610376348898,0.08853976428508759
23012,-Metrics/Training(Step): loss,1610376351118,0.08885707706212997
23014,-Metrics/Training(Step): loss,1610376352921,0.09909607470035553
23016,-Metrics/Training(Step): loss,1610376354886,0.08143573999404907
23018,-Metrics/Training(Step): loss,1610376356958,0.10988452285528183
23020,-Metrics/Training(Step): loss,1610376359000,0.06552354991436005
23022,-Metrics/Training(Step): loss,1610376361091,0.06929821521043777
23024,-Metrics/Training(Step): loss,1610376363196,0.0840817540884018
23026,-Metrics/Training(Step): loss,1610376365271,0.08798757940530777
23028,-Metrics/Training(Step): loss,1610376367121,0.08340886980295181
23030,-Metrics/Training(Step): loss,1610376369019,0.08942148834466934
23032,-Metrics/Training(Step): loss,1610376371039,0.0879153162240982
23034,-Metrics/Training(Step): loss,1610376373066,0.10069891065359116
23036,-Metrics/Training(Step): loss,1610376375062,0.09574802964925766
23038,-Metrics/Training(Step): loss,1610376377137,0.08560758829116821
23040,-Metrics/Training(Step): loss,1610376379118,0.08310328423976898
23042,-Metrics/Training(Step): loss,1610376381122,0.08971269428730011
23044,-Metrics/Training(Step): loss,1610376383150,0.08340610563755035
23046,-Metrics/Training(Step): loss,1610376385122,0.07039914280176163
23048,-Metrics/Training(Step): loss,1610376387146,0.08700881898403168
23050,-Metrics/Training(Step): loss,1610376400320,0.0924249142408371
23052,-Metrics/Training(Step): loss,1610376404258,0.077406145632267
23054,-Metrics/Training(Step): loss,1610376408252,0.09365826100111008
23056,-Metrics/Training(Step): loss,1610376412619,0.09302686899900436
23058,-Metrics/Training(Step): loss,1610376416619,0.06153528019785881
23060,-Metrics/Training(Step): loss,1610376420350,0.09189752489328384
23062,-Metrics/Training(Step): loss,1610376424144,0.06427080184221268
23064,-Metrics/Training(Step): loss,1610376427919,0.1087290570139885
23066,-Metrics/Training(Step): loss,1610376431420,0.08302170038223267
23068,-Metrics/Training(Step): loss,1610376435098,0.07600128650665283
23070,-Metrics/Training(Step): loss,1610376438485,0.08535776287317276
23072,-Metrics/Training(Step): loss,1610376441989,0.08877270668745041
23074,-Metrics/Training(Step): loss,1610376445480,0.0971590057015419
23076,-Metrics/Training(Step): loss,1610376449127,0.08695275336503983
23078,-Metrics/Training(Step): loss,1610376452822,0.07110463082790375
23080,-Metrics/Training(Step): loss,1610376456585,0.08507562428712845
23082,-Metrics/Training(Step): loss,1610376460091,0.0969734638929367
23084,-Metrics/Training(Step): loss,1610376464043,0.09925520420074463
23086,-Metrics/Training(Step): loss,1610376467675,0.09071067720651627
23088,-Metrics/Training(Step): loss,1610376470920,0.07881006598472595
23090,-Metrics/Training(Step): loss,1610376474116,0.0765802338719368
23092,-Metrics/Training(Step): loss,1610376477878,0.08141148090362549
23094,-Metrics/Training(Step): loss,1610376481456,0.06983635574579239
23096,-Metrics/Training(Step): loss,1610376483696,0.08646312355995178
23098,-Metrics/Training(Step): loss,1610376485938,0.08389817923307419
23100,-Metrics/Training(Step): loss,1610376488156,0.08328080177307129
23102,-Metrics/Training(Step): loss,1610376490214,0.07337083667516708
23104,-Metrics/Training(Step): loss,1610376492256,0.09022180736064911
23106,-Metrics/Training(Step): loss,1610376494349,0.06951145827770233
23108,-Metrics/Training(Step): loss,1610376496460,0.0882660374045372
23110,-Metrics/Training(Step): loss,1610376498537,0.08004272729158401
23112,-Metrics/Training(Step): loss,1610376500546,0.08024921268224716
23114,-Metrics/Training(Step): loss,1610376502556,0.0762549638748169
23116,-Metrics/Training(Step): loss,1610376504515,0.09453632682561874
23118,-Metrics/Training(Step): loss,1610376506477,0.0822407677769661
23120,-Metrics/Training(Step): loss,1610376508550,0.1002771332859993
23122,-Metrics/Training(Step): loss,1610376510638,0.10452166199684143
23124,-Metrics/Training(Step): loss,1610376512504,0.09364918619394302
23126,-Metrics/Training(Step): loss,1610376514494,0.08932667225599289
23128,-Metrics/Training(Step): loss,1610376516489,0.09737519174814224
23130,-Metrics/Training(Step): loss,1610376518491,0.07778998464345932
23132,-Metrics/Training(Step): loss,1610376520539,0.10864536464214325
23134,-Metrics/Training(Step): loss,1610376522560,0.0801788717508316
23136,-Metrics/Training(Step): loss,1610376535740,0.08087032288312912
23138,-Metrics/Training(Step): loss,1610376539919,0.1037355363368988
23140,-Metrics/Training(Step): loss,1610376544144,0.08240453153848648
23142,-Metrics/Training(Step): loss,1610376548147,0.08337398618459702
23144,-Metrics/Training(Step): loss,1610376551919,0.07355736196041107
23146,-Metrics/Training(Step): loss,1610376555723,0.07102108001708984
23148,-Metrics/Training(Step): loss,1610376559419,0.0871129259467125
23150,-Metrics/Training(Step): loss,1610376563135,0.10601997375488281
23152,-Metrics/Training(Step): loss,1610376566019,0.08624874800443649
23154,-Metrics/Training(Step): loss,1610376570136,0.08894547075033188
23156,-Metrics/Training(Step): loss,1610376573751,0.09222473204135895
23158,-Metrics/Training(Step): loss,1610376578382,0.06955607980489731
23160,-Metrics/Training(Step): loss,1610376582320,0.06592168658971786
23162,-Metrics/Training(Step): loss,1610376586142,0.08728690445423126
23164,-Metrics/Training(Step): loss,1610376589765,0.08122098445892334
23166,-Metrics/Training(Step): loss,1610376593218,0.10340876132249832
23168,-Metrics/Training(Step): loss,1610376596792,0.08186202496290207
23170,-Metrics/Training(Step): loss,1610376600620,0.10438910126686096
23172,-Metrics/Training(Step): loss,1610376603919,0.08508705347776413
23174,-Metrics/Training(Step): loss,1610376607048,0.08129476010799408
23176,-Metrics/Training(Step): loss,1610376611121,0.09447334706783295
23178,-Metrics/Training(Step): loss,1610376614419,0.08061912655830383
23180,-Metrics/Training(Step): loss,1610376617617,0.08731850981712341
23182,-Metrics/Training(Step): loss,1610376620217,0.09097681939601898
23184,-Metrics/Training(Step): loss,1610376622339,0.12077893316745758
23186,-Metrics/Training(Step): loss,1610376624405,0.11139678955078125
23188,-Metrics/Training(Step): loss,1610376626479,0.10046270489692688
23190,-Metrics/Training(Step): loss,1610376628527,0.07268960028886795
23192,-Metrics/Training(Step): loss,1610376630623,0.08209957182407379
23194,-Metrics/Training(Step): loss,1610376632701,0.08429514616727829
23196,-Metrics/Training(Step): loss,1610376634785,0.0914628729224205
23198,-Metrics/Training(Step): loss,1610376636894,0.09678307920694351
23200,-Metrics/Training(Step): loss,1610376638934,0.0759790763258934
23202,-Metrics/Training(Step): loss,1610376641052,0.08647977560758591
23204,-Metrics/Training(Step): loss,1610376643054,0.08426599204540253
23206,-Metrics/Training(Step): loss,1610376645031,0.0722033903002739
23208,-Metrics/Training(Step): loss,1610376647091,0.09494400769472122
23210,-Metrics/Training(Step): loss,1610376649177,0.0682351216673851
23212,-Metrics/Training(Step): loss,1610376651204,0.08688631653785706
23214,-Metrics/Training(Step): loss,1610376653248,0.09386526793241501
23216,-Metrics/Training(Step): loss,1610376655270,0.07504590600728989
23218,-Metrics/Training(Step): loss,1610376657280,0.09163616597652435
23220,-Metrics/Training(Step): loss,1610376659297,0.09825606644153595
23222,-Metrics/Training(Step): loss,1610376682220,0.08846323937177658
23224,-Metrics/Training(Step): loss,1610376686119,0.08511970192193985
23226,-Metrics/Training(Step): loss,1610376689819,0.08347329497337341
23228,-Metrics/Training(Step): loss,1610376693919,0.09665223211050034
23230,-Metrics/Training(Step): loss,1610376698123,0.08326876163482666
23232,-Metrics/Training(Step): loss,1610376702527,0.091437928378582
23234,-Metrics/Training(Step): loss,1610376706119,0.09336526691913605
23236,-Metrics/Training(Step): loss,1610376709750,0.08078233152627945
23238,-Metrics/Training(Step): loss,1610376713600,0.08290078490972519
23240,-Metrics/Training(Step): loss,1610376717219,0.09197907149791718
23242,-Metrics/Training(Step): loss,1610376720358,0.07622401416301727
23244,-Metrics/Training(Step): loss,1610376724069,0.09959671646356583
23246,-Metrics/Training(Step): loss,1610376727721,0.0864955484867096
23248,-Metrics/Training(Step): loss,1610376731347,0.1007075160741806
23250,-Metrics/Training(Step): loss,1610376735023,0.1133359894156456
23252,-Metrics/Training(Step): loss,1610376738623,0.07443185150623322
23254,-Metrics/Training(Step): loss,1610376741794,0.09133662283420563
23256,-Metrics/Training(Step): loss,1610376745115,0.08740095794200897
23258,-Metrics/Training(Step): loss,1610376748620,0.12554451823234558
23260,-Metrics/Training(Step): loss,1610376751858,0.08937445282936096
23262,-Metrics/Training(Step): loss,1610376755494,0.08709026128053665
23264,-Metrics/Training(Step): loss,1610376758844,0.06593497842550278
23266,-Metrics/Training(Step): loss,1610376762020,0.08404979854822159
23268,-Metrics/Training(Step): loss,1610376765190,0.07014936208724976
23270,-Metrics/Training(Step): loss,1610376767475,0.11426086723804474
23272,-Metrics/Training(Step): loss,1610376769606,0.07641437649726868
23274,-Metrics/Training(Step): loss,1610376771635,0.08419734984636307
23276,-Metrics/Training(Step): loss,1610376773723,0.0784875825047493
23278,-Metrics/Training(Step): loss,1610376775784,0.10495147854089737
23280,-Metrics/Training(Step): loss,1610376777821,0.09901891648769379
23282,-Metrics/Training(Step): loss,1610376779899,0.10142545402050018
23284,-Metrics/Training(Step): loss,1610376781877,0.07484965026378632
23286,-Metrics/Training(Step): loss,1610376783880,0.06707466393709183
23288,-Metrics/Training(Step): loss,1610376785920,0.06857364624738693
23290,-Metrics/Training(Step): loss,1610376787992,0.0795244425535202
23292,-Metrics/Training(Step): loss,1610376789610,0.09727984666824341
23294,-Metrics/Training(Step): loss,1610376791593,0.08286526054143906
23296,-Metrics/Training(Step): loss,1610376793498,0.0683678686618805
23298,-Metrics/Training(Step): loss,1610376795545,0.08509116619825363
23300,-Metrics/Training(Step): loss,1610376797566,0.07757536321878433
23302,-Metrics/Training(Step): loss,1610376799573,0.09168078005313873
23304,-Metrics/Training(Step): loss,1610376801614,0.09913178533315659
23306,-Metrics/Training(Step): loss,1610376803566,0.08615820854902267
23308,-Metrics/Training(Step): loss,1610376815428,0.06525179743766785
23310,-Metrics/Training(Step): loss,1610376819530,0.0940125361084938
23312,-Metrics/Training(Step): loss,1610376823519,0.10825403034687042
23314,-Metrics/Training(Step): loss,1610376827838,0.09076829254627228
23316,-Metrics/Training(Step): loss,1610376832020,0.10211727023124695
23318,-Metrics/Training(Step): loss,1610376835844,0.07684018462896347
23320,-Metrics/Training(Step): loss,1610376839840,0.09232467412948608
23322,-Metrics/Training(Step): loss,1610376843323,0.09416496008634567
23324,-Metrics/Training(Step): loss,1610376847120,0.0923454761505127
23326,-Metrics/Training(Step): loss,1610376850869,0.07081909477710724
23328,-Metrics/Training(Step): loss,1610376854319,0.08484427630901337
23330,-Metrics/Training(Step): loss,1610376858123,0.08659477531909943
23332,-Metrics/Training(Step): loss,1610376862021,0.11008750647306442
23334,-Metrics/Training(Step): loss,1610376865719,0.08156639337539673
23336,-Metrics/Training(Step): loss,1610376869516,0.09054320305585861
23338,-Metrics/Training(Step): loss,1610376873032,0.08584659546613693
23340,-Metrics/Training(Step): loss,1610376876242,0.07499019056558609
23342,-Metrics/Training(Step): loss,1610376879643,0.0945802554488182
23344,-Metrics/Training(Step): loss,1610376883221,0.09203112870454788
23346,-Metrics/Training(Step): loss,1610376886576,0.06948535889387131
23348,-Metrics/Training(Step): loss,1610376890020,0.08491707593202591
23350,-Metrics/Training(Step): loss,1610376893578,0.07673169672489166
23352,-Metrics/Training(Step): loss,1610376896887,0.08732139319181442
23354,-Metrics/Training(Step): loss,1610376899326,0.09393483400344849
23356,-Metrics/Training(Step): loss,1610376901516,0.10234573483467102
23358,-Metrics/Training(Step): loss,1610376903579,0.07450219988822937
23360,-Metrics/Training(Step): loss,1610376905651,0.07865062355995178
23362,-Metrics/Training(Step): loss,1610376907728,0.08510331809520721
23364,-Metrics/Training(Step): loss,1610376909796,0.06827434152364731
23366,-Metrics/Training(Step): loss,1610376911906,0.07937485724687576
23368,-Metrics/Training(Step): loss,1610376913960,0.07638584822416306
23370,-Metrics/Training(Step): loss,1610376916061,0.07279019802808762
23372,-Metrics/Training(Step): loss,1610376918089,0.0812588557600975
23374,-Metrics/Training(Step): loss,1610376920140,0.09532991051673889
23376,-Metrics/Training(Step): loss,1610376922216,0.0918918251991272
23378,-Metrics/Training(Step): loss,1610376924238,0.08505284041166306
23380,-Metrics/Training(Step): loss,1610376926219,0.08949422091245651
23382,-Metrics/Training(Step): loss,1610376928230,0.06781399995088577
23384,-Metrics/Training(Step): loss,1610376930280,0.07920946925878525
23386,-Metrics/Training(Step): loss,1610376932233,0.09858594834804535
23388,-Metrics/Training(Step): loss,1610376934306,0.10031203925609589
23390,-Metrics/Training(Step): loss,1610376936329,0.08806757628917694
23392,-Metrics/Training(Step): loss,1610376938209,0.08660010248422623
23394,-Metrics/Training(Step): loss,1610376950518,0.06614241749048233
23396,-Metrics/Training(Step): loss,1610376954619,0.07791107147932053
23398,-Metrics/Training(Step): loss,1610376958215,0.07430712133646011
23400,-Metrics/Training(Step): loss,1610376961820,0.07095575332641602
23402,-Metrics/Training(Step): loss,1610376965621,0.08571531623601913
23404,-Metrics/Training(Step): loss,1610376969420,0.09132424741983414
23406,-Metrics/Training(Step): loss,1610376973044,0.09223059564828873
23408,-Metrics/Training(Step): loss,1610376977219,0.07890360802412033
23410,-Metrics/Training(Step): loss,1610376980920,0.08479167520999908
23412,-Metrics/Training(Step): loss,1610376984020,0.08389835804700851
23414,-Metrics/Training(Step): loss,1610376987720,0.07991629838943481
23416,-Metrics/Training(Step): loss,1610376991222,0.0906400978565216
23418,-Metrics/Training(Step): loss,1610376995258,0.06734960526227951
23420,-Metrics/Training(Step): loss,1610376998994,0.06492448598146439
23422,-Metrics/Training(Step): loss,1610377002440,0.07903443276882172
23424,-Metrics/Training(Step): loss,1610377006561,0.09317608922719955
23426,-Metrics/Training(Step): loss,1610377010021,0.0821678563952446
23428,-Metrics/Training(Step): loss,1610377013994,0.0936993733048439
23430,-Metrics/Training(Step): loss,1610377016920,0.09912880510091782
23432,-Metrics/Training(Step): loss,1610377020548,0.0795062780380249
23434,-Metrics/Training(Step): loss,1610377023920,0.07513521611690521
23436,-Metrics/Training(Step): loss,1610377027031,0.08063587546348572
23438,-Metrics/Training(Step): loss,1610377030653,0.08544404804706573
23440,-Metrics/Training(Step): loss,1610377033275,0.07987167686223984
23442,-Metrics/Training(Step): loss,1610377035521,0.096220001578331
23444,-Metrics/Training(Step): loss,1610377037543,0.08458355814218521
23446,-Metrics/Training(Step): loss,1610377039577,0.07200559973716736
23448,-Metrics/Training(Step): loss,1610377041666,0.08382825553417206
23450,-Metrics/Training(Step): loss,1610377043772,0.0845915675163269
23452,-Metrics/Training(Step): loss,1610377045624,0.08875348418951035
23454,-Metrics/Training(Step): loss,1610377047603,0.07788744568824768
23456,-Metrics/Training(Step): loss,1610377049635,0.07802610099315643
23458,-Metrics/Training(Step): loss,1610377051677,0.09194019436836243
23460,-Metrics/Training(Step): loss,1610377053666,0.09316644072532654
23462,-Metrics/Training(Step): loss,1610377055740,0.05645063892006874
23464,-Metrics/Training(Step): loss,1610377057816,0.08388575911521912
23466,-Metrics/Training(Step): loss,1610377059866,0.09529498219490051
23468,-Metrics/Training(Step): loss,1610377061913,0.08374550938606262
23470,-Metrics/Training(Step): loss,1610377063857,0.09638229012489319
23472,-Metrics/Training(Step): loss,1610377065836,0.09995247423648834
23474,-Metrics/Training(Step): loss,1610377067785,0.09041299670934677
23476,-Metrics/Training(Step): loss,1610377069848,0.07581508159637451
23478,-Metrics/Training(Step): loss,1610377071893,0.0788290947675705
23480,-Metrics/Training(Step): loss,1610377084125,0.09705526381731033
23482,-Metrics/Training(Step): loss,1610377088116,0.08108591288328171
23484,-Metrics/Training(Step): loss,1610377092020,0.09179109334945679
23486,-Metrics/Training(Step): loss,1610377096219,0.0907604917883873
23488,-Metrics/Training(Step): loss,1610377100320,0.08752362430095673
23490,-Metrics/Training(Step): loss,1610377104428,0.08485925942659378
23492,-Metrics/Training(Step): loss,1610377108429,0.09189073741436005
23494,-Metrics/Training(Step): loss,1610377112226,0.07752085477113724
23496,-Metrics/Training(Step): loss,1610377115781,0.0750720202922821
23498,-Metrics/Training(Step): loss,1610377119229,0.07917766273021698
23500,-Metrics/Training(Step): loss,1610377123238,0.08219847828149796
23502,-Metrics/Training(Step): loss,1610377126832,0.09139757603406906
23504,-Metrics/Training(Step): loss,1610377130378,0.08384911715984344
23506,-Metrics/Training(Step): loss,1610377133920,0.07045534253120422
23508,-Metrics/Training(Step): loss,1610377137320,0.08870772272348404
23510,-Metrics/Training(Step): loss,1610377140898,0.09636957198381424
23512,-Metrics/Training(Step): loss,1610377144220,0.08726781606674194
23514,-Metrics/Training(Step): loss,1610377147720,0.07605990022420883
23516,-Metrics/Training(Step): loss,1610377150919,0.06370965391397476
23518,-Metrics/Training(Step): loss,1610377154597,0.08407007902860641
23520,-Metrics/Training(Step): loss,1610377158037,0.10514438152313232
23522,-Metrics/Training(Step): loss,1610377161539,0.08981902152299881
23524,-Metrics/Training(Step): loss,1610377164370,0.10613308846950531
23526,-Metrics/Training(Step): loss,1610377167131,0.0806296095252037
23528,-Metrics/Training(Step): loss,1610377169398,0.08893287926912308
23530,-Metrics/Training(Step): loss,1610377171430,0.07109557092189789
23532,-Metrics/Training(Step): loss,1610377173518,0.06586389243602753
23534,-Metrics/Training(Step): loss,1610377175521,0.11163220554590225
23536,-Metrics/Training(Step): loss,1610377177592,0.07758712023496628
23538,-Metrics/Training(Step): loss,1610377179677,0.08562880009412766
23540,-Metrics/Training(Step): loss,1610377181770,0.08403924107551575
23542,-Metrics/Training(Step): loss,1610377183857,0.0641426369547844
23544,-Metrics/Training(Step): loss,1610377185543,0.0943245142698288
23546,-Metrics/Training(Step): loss,1610377187565,0.06908602267503738
23548,-Metrics/Training(Step): loss,1610377189653,0.08194910734891891
23550,-Metrics/Training(Step): loss,1610377191445,0.06553922593593597
23552,-Metrics/Training(Step): loss,1610377193519,0.06973234564065933
23554,-Metrics/Training(Step): loss,1610377195463,0.0768350139260292
23556,-Metrics/Training(Step): loss,1610377197465,0.09725847840309143
23558,-Metrics/Training(Step): loss,1610377199543,0.07690783590078354
23560,-Metrics/Training(Step): loss,1610377201447,0.08168023824691772
23562,-Metrics/Training(Step): loss,1610377203520,0.08677700161933899
23564,-Metrics/Training(Step): loss,1610377205548,0.07584462314844131
23566,-Metrics/Training(Step): loss,1610377217635,0.07214754819869995
23568,-Metrics/Training(Step): loss,1610377221519,0.07476378232240677
23570,-Metrics/Training(Step): loss,1610377225617,0.07585448026657104
23572,-Metrics/Training(Step): loss,1610377229747,0.08429192006587982
23574,-Metrics/Training(Step): loss,1610377233619,0.07383322715759277
23576,-Metrics/Training(Step): loss,1610377237647,0.07033023238182068
23578,-Metrics/Training(Step): loss,1610377241487,0.09422431886196136
23580,-Metrics/Training(Step): loss,1610377245481,0.1125251054763794
23582,-Metrics/Training(Step): loss,1610377249515,0.1079539805650711
23584,-Metrics/Training(Step): loss,1610377252831,0.07624375075101852
23586,-Metrics/Training(Step): loss,1610377256621,0.08474822342395782
23588,-Metrics/Training(Step): loss,1610377260220,0.09141205251216888
23590,-Metrics/Training(Step): loss,1610377263517,0.08428087830543518
23592,-Metrics/Training(Step): loss,1610377267120,0.10212384909391403
23594,-Metrics/Training(Step): loss,1610377270428,0.08288593590259552
23596,-Metrics/Training(Step): loss,1610377273821,0.08073452860116959
23598,-Metrics/Training(Step): loss,1610377277041,0.08751390874385834
23600,-Metrics/Training(Step): loss,1610377280119,0.0931510329246521
23602,-Metrics/Training(Step): loss,1610377283850,0.08933320641517639
23604,-Metrics/Training(Step): loss,1610377287531,0.11158029735088348
23606,-Metrics/Training(Step): loss,1610377291129,0.06646821647882462
23608,-Metrics/Training(Step): loss,1610377294398,0.08123350143432617
23610,-Metrics/Training(Step): loss,1610377297647,0.08491627871990204
23612,-Metrics/Training(Step): loss,1610377301019,0.067914679646492
23614,-Metrics/Training(Step): loss,1610377303422,0.10629953444004059
23616,-Metrics/Training(Step): loss,1610377305666,0.06908531486988068
23618,-Metrics/Training(Step): loss,1610377307859,0.10509876161813736
23620,-Metrics/Training(Step): loss,1610377309947,0.08937807381153107
23622,-Metrics/Training(Step): loss,1610377312014,0.09441355615854263
23624,-Metrics/Training(Step): loss,1610377314106,0.07853376120328903
23626,-Metrics/Training(Step): loss,1610377316165,0.09909354150295258
23628,-Metrics/Training(Step): loss,1610377318237,0.08736807852983475
23630,-Metrics/Training(Step): loss,1610377320351,0.09080371260643005
23632,-Metrics/Training(Step): loss,1610377322419,0.06476999074220657
23634,-Metrics/Training(Step): loss,1610377324331,0.10144142806529999
23636,-Metrics/Training(Step): loss,1610377326346,0.08216524124145508
23638,-Metrics/Training(Step): loss,1610377328321,0.07307665795087814
23640,-Metrics/Training(Step): loss,1610377330261,0.08308883756399155
23642,-Metrics/Training(Step): loss,1610377332315,0.07995044440031052
23644,-Metrics/Training(Step): loss,1610377334371,0.08331586420536041
23646,-Metrics/Training(Step): loss,1610377336405,0.09009372442960739
23648,-Metrics/Training(Step): loss,1610377338463,0.0857996940612793
23650,-Metrics/Training(Step): loss,1610377340508,0.0766199380159378
23652,-Metrics/Training(Step): loss,1610377352952,0.09427661448717117
23654,-Metrics/Training(Step): loss,1610377357123,0.08267712593078613
23656,-Metrics/Training(Step): loss,1610377360958,0.07001689821481705
23658,-Metrics/Training(Step): loss,1610377364858,0.08379185199737549
23660,-Metrics/Training(Step): loss,1610377369049,0.08262418210506439
23662,-Metrics/Training(Step): loss,1610377373053,0.08815591037273407
23664,-Metrics/Training(Step): loss,1610377376916,0.09598635882139206
23666,-Metrics/Training(Step): loss,1610377380869,0.06495612114667892
23668,-Metrics/Training(Step): loss,1610377384923,0.10396380722522736
23670,-Metrics/Training(Step): loss,1610377388821,0.07402980327606201
23672,-Metrics/Training(Step): loss,1610377392944,0.09640109539031982
23674,-Metrics/Training(Step): loss,1610377396126,0.07922295480966568
23676,-Metrics/Training(Step): loss,1610377399519,0.10467913001775742
23678,-Metrics/Training(Step): loss,1610377403052,0.06555872410535812
23680,-Metrics/Training(Step): loss,1610377406325,0.09224459528923035
23682,-Metrics/Training(Step): loss,1610377409909,0.0792873203754425
23684,-Metrics/Training(Step): loss,1610377413847,0.08772069960832596
23686,-Metrics/Training(Step): loss,1610377417245,0.10702598839998245
23688,-Metrics/Training(Step): loss,1610377420633,0.07750265300273895
23690,-Metrics/Training(Step): loss,1610377424024,0.10192922502756119
23692,-Metrics/Training(Step): loss,1610377427868,0.10569237172603607
23694,-Metrics/Training(Step): loss,1610377431283,0.08359125256538391
23696,-Metrics/Training(Step): loss,1610377434833,0.10350816696882248
23698,-Metrics/Training(Step): loss,1610377437290,0.08555995672941208
23700,-Metrics/Training(Step): loss,1610377439252,0.07211434841156006
23702,-Metrics/Training(Step): loss,1610377441323,0.09411193430423737
23704,-Metrics/Training(Step): loss,1610377443410,0.10512175410985947
23706,-Metrics/Training(Step): loss,1610377445438,0.07855062931776047
23708,-Metrics/Training(Step): loss,1610377447491,0.08624201267957687
23710,-Metrics/Training(Step): loss,1610377449594,0.07846524566411972
23712,-Metrics/Training(Step): loss,1610377451669,0.09174167364835739
23714,-Metrics/Training(Step): loss,1610377453599,0.07649626582860947
23716,-Metrics/Training(Step): loss,1610377455501,0.08278626948595047
23718,-Metrics/Training(Step): loss,1610377457359,0.08191721141338348
23720,-Metrics/Training(Step): loss,1610377459421,0.0762331411242485
23722,-Metrics/Training(Step): loss,1610377461369,0.07285673916339874
23724,-Metrics/Training(Step): loss,1610377463424,0.10602240264415741
23726,-Metrics/Training(Step): loss,1610377465490,0.06382151693105698
23728,-Metrics/Training(Step): loss,1610377467564,0.09407773613929749
23730,-Metrics/Training(Step): loss,1610377469509,0.08358952403068542
23732,-Metrics/Training(Step): loss,1610377471567,0.06859150528907776
23734,-Metrics/Training(Step): loss,1610377473623,0.08323109894990921
23736,-Metrics/Training(Step): loss,1610377475650,0.07723663002252579
23738,-Metrics/Training(Step): loss,1610377487533,0.09433534741401672
23740,-Metrics/Training(Step): loss,1610377491930,0.0842798501253128
23742,-Metrics/Training(Step): loss,1610377496344,0.058657266199588776
23744,-Metrics/Training(Step): loss,1610377500150,0.0767119824886322
23746,-Metrics/Training(Step): loss,1610377504335,0.06803281605243683
23748,-Metrics/Training(Step): loss,1610377508453,0.07945221662521362
23750,-Metrics/Training(Step): loss,1610377512420,0.08355341106653214
23752,-Metrics/Training(Step): loss,1610377515921,0.07820522785186768
23754,-Metrics/Training(Step): loss,1610377519619,0.0942789688706398
23756,-Metrics/Training(Step): loss,1610377523278,0.09653080254793167
23758,-Metrics/Training(Step): loss,1610377526520,0.08819720894098282
23760,-Metrics/Training(Step): loss,1610377530220,0.09079387038946152
23762,-Metrics/Training(Step): loss,1610377533647,0.05503459274768829
23764,-Metrics/Training(Step): loss,1610377536835,0.08881807327270508
23766,-Metrics/Training(Step): loss,1610377539920,0.08613097667694092
23768,-Metrics/Training(Step): loss,1610377543019,0.08144266158342361
23770,-Metrics/Training(Step): loss,1610377546319,0.08960376679897308
23772,-Metrics/Training(Step): loss,1610377549965,0.08904001861810684
23774,-Metrics/Training(Step): loss,1610377553419,0.10383065044879913
23776,-Metrics/Training(Step): loss,1610377556919,0.0725504532456398
23778,-Metrics/Training(Step): loss,1610377560419,0.0827673152089119
23780,-Metrics/Training(Step): loss,1610377564069,0.0648714229464531
23782,-Metrics/Training(Step): loss,1610377567300,0.08373800665140152
23784,-Metrics/Training(Step): loss,1610377570641,0.10348972678184509
23786,-Metrics/Training(Step): loss,1610377573038,0.08499220013618469
23788,-Metrics/Training(Step): loss,1610377575089,0.10131488740444183
23790,-Metrics/Training(Step): loss,1610377577182,0.10320503264665604
23792,-Metrics/Training(Step): loss,1610377579289,0.09156898409128189
23794,-Metrics/Training(Step): loss,1610377581380,0.06648208200931549
23796,-Metrics/Training(Step): loss,1610377583483,0.0825580283999443
23798,-Metrics/Training(Step): loss,1610377585522,0.09376751631498337
23800,-Metrics/Training(Step): loss,1610377587372,0.07706263661384583
23802,-Metrics/Training(Step): loss,1610377589441,0.09212695062160492
23804,-Metrics/Training(Step): loss,1610377591485,0.08364720642566681
23806,-Metrics/Training(Step): loss,1610377593457,0.10174734145402908
23808,-Metrics/Training(Step): loss,1610377595562,0.08134709298610687
23810,-Metrics/Training(Step): loss,1610377597614,0.05980799347162247
23812,-Metrics/Training(Step): loss,1610377599635,0.07416708022356033
23814,-Metrics/Training(Step): loss,1610377601581,0.09223949909210205
23816,-Metrics/Training(Step): loss,1610377603611,0.08941839635372162
23818,-Metrics/Training(Step): loss,1610377605669,0.088758684694767
23820,-Metrics/Training(Step): loss,1610377607736,0.07127998024225235
23822,-Metrics/Training(Step): loss,1610377609763,0.0745173841714859
23824,-Metrics/Training(Step): loss,1610377622143,0.08453305065631866
23826,-Metrics/Training(Step): loss,1610377626635,0.07711545377969742
23828,-Metrics/Training(Step): loss,1610377630720,0.10017337650060654
23830,-Metrics/Training(Step): loss,1610377634919,0.09109923988580704
23832,-Metrics/Training(Step): loss,1610377638920,0.10312670469284058
23834,-Metrics/Training(Step): loss,1610377642720,0.08642759174108505
23836,-Metrics/Training(Step): loss,1610377646421,0.07179964333772659
23838,-Metrics/Training(Step): loss,1610377650188,0.08736138790845871
23840,-Metrics/Training(Step): loss,1610377653422,0.07956068217754364
23842,-Metrics/Training(Step): loss,1610377656821,0.07680843770503998
23844,-Metrics/Training(Step): loss,1610377660519,0.08887354284524918
23846,-Metrics/Training(Step): loss,1610377664492,0.09737861901521683
23848,-Metrics/Training(Step): loss,1610377667658,0.09967083483934402
23850,-Metrics/Training(Step): loss,1610377671443,0.10235098749399185
23852,-Metrics/Training(Step): loss,1610377675215,0.08849389851093292
23854,-Metrics/Training(Step): loss,1610377678818,0.0799311175942421
23856,-Metrics/Training(Step): loss,1610377682247,0.0837654173374176
23858,-Metrics/Training(Step): loss,1610377685780,0.06918061524629593
23860,-Metrics/Training(Step): loss,1610377689202,0.08011380583047867
23862,-Metrics/Training(Step): loss,1610377692933,0.09298700094223022
23864,-Metrics/Training(Step): loss,1610377696854,0.08785273134708405
23866,-Metrics/Training(Step): loss,1610377699720,0.06902745366096497
23868,-Metrics/Training(Step): loss,1610377702919,0.0838845893740654
23870,-Metrics/Training(Step): loss,1610377706140,0.07509563118219376
23872,-Metrics/Training(Step): loss,1610377708501,0.09105578064918518
23874,-Metrics/Training(Step): loss,1610377710670,0.0792108103632927
23876,-Metrics/Training(Step): loss,1610377712862,0.08404086530208588
23878,-Metrics/Training(Step): loss,1610377714870,0.07395314425230026
23880,-Metrics/Training(Step): loss,1610377716971,0.0925728902220726
23882,-Metrics/Training(Step): loss,1610377718907,0.09318786859512329
23884,-Metrics/Training(Step): loss,1610377721016,0.0812259092926979
23886,-Metrics/Training(Step): loss,1610377723073,0.06982875615358353
23888,-Metrics/Training(Step): loss,1610377725173,0.09192605316638947
23890,-Metrics/Training(Step): loss,1610377727256,0.0927080512046814
23892,-Metrics/Training(Step): loss,1610377729319,0.09341201186180115
23894,-Metrics/Training(Step): loss,1610377731077,0.07642797380685806
23896,-Metrics/Training(Step): loss,1610377733151,0.09406497329473495
23898,-Metrics/Training(Step): loss,1610377735228,0.0946890339255333
23900,-Metrics/Training(Step): loss,1610377737133,0.07528971880674362
23902,-Metrics/Training(Step): loss,1610377739113,0.07514319568872452
23904,-Metrics/Training(Step): loss,1610377741104,0.09294284880161285
23906,-Metrics/Training(Step): loss,1610377743107,0.08075965195894241
23908,-Metrics/Training(Step): loss,1610377745154,0.09134902060031891
23910,-Metrics/Training(Step): loss,1610377757419,0.08523564040660858
23912,-Metrics/Training(Step): loss,1610377761720,0.08416260778903961
23914,-Metrics/Training(Step): loss,1610377765524,0.08093932271003723
23916,-Metrics/Training(Step): loss,1610377769622,0.08270401507616043
23918,-Metrics/Training(Step): loss,1610377773519,0.07675448060035706
23920,-Metrics/Training(Step): loss,1610377777719,0.08238014578819275
23922,-Metrics/Training(Step): loss,1610377781956,0.0785355195403099
23924,-Metrics/Training(Step): loss,1610377785543,0.07751781493425369
23926,-Metrics/Training(Step): loss,1610377789320,0.08852982521057129
23928,-Metrics/Training(Step): loss,1610377792928,0.09532974660396576
23930,-Metrics/Training(Step): loss,1610377797320,0.08351285010576248
23932,-Metrics/Training(Step): loss,1610377801256,0.07316932082176208
23934,-Metrics/Training(Step): loss,1610377804686,0.067281574010849
23936,-Metrics/Training(Step): loss,1610377808620,0.09970831125974655
23938,-Metrics/Training(Step): loss,1610377812020,0.07359591871500015
23940,-Metrics/Training(Step): loss,1610377815543,0.09868527948856354
23942,-Metrics/Training(Step): loss,1610377819320,0.07618691772222519
23944,-Metrics/Training(Step): loss,1610377822347,0.09064413607120514
23946,-Metrics/Training(Step): loss,1610377826119,0.08922147005796432
23948,-Metrics/Training(Step): loss,1610377830119,0.08181601762771606
23950,-Metrics/Training(Step): loss,1610377833619,0.06986173242330551
23952,-Metrics/Training(Step): loss,1610377837278,0.07830362766981125
23954,-Metrics/Training(Step): loss,1610377840633,0.0825619027018547
23956,-Metrics/Training(Step): loss,1610377843058,0.07203167676925659
23958,-Metrics/Training(Step): loss,1610377845172,0.08285512775182724
23960,-Metrics/Training(Step): loss,1610377847217,0.0781717598438263
23962,-Metrics/Training(Step): loss,1610377849212,0.08895358443260193
23964,-Metrics/Training(Step): loss,1610377851282,0.10966525971889496
23966,-Metrics/Training(Step): loss,1610377853356,0.09333963692188263
23968,-Metrics/Training(Step): loss,1610377855441,0.09434272348880768
23970,-Metrics/Training(Step): loss,1610377857536,0.0680706724524498
23972,-Metrics/Training(Step): loss,1610377859600,0.0831296518445015
23974,-Metrics/Training(Step): loss,1610377861662,0.09446098655462265
23976,-Metrics/Training(Step): loss,1610377863692,0.09942653030157089
23978,-Metrics/Training(Step): loss,1610377865626,0.0882265493273735
23980,-Metrics/Training(Step): loss,1610377867567,0.08737654238939285
23982,-Metrics/Training(Step): loss,1610377869561,0.09521415084600449
23984,-Metrics/Training(Step): loss,1610377871609,0.09036203473806381
23986,-Metrics/Training(Step): loss,1610377873691,0.11689352989196777
23988,-Metrics/Training(Step): loss,1610377875757,0.06976916640996933
23990,-Metrics/Training(Step): loss,1610377877815,0.08388689160346985
23992,-Metrics/Training(Step): loss,1610377879759,0.06988189369440079
23994,-Metrics/Training(Step): loss,1610377881816,0.08036018908023834
23996,-Metrics/Training(Step): loss,1610377894020,0.0830950066447258
23998,-Metrics/Training(Step): loss,1610377898227,0.0716630220413208
24000,-Metrics/Training(Step): loss,1610377902023,0.10505449771881104
24002,-Metrics/Training(Step): loss,1610377906034,0.08309219777584076
24004,-Metrics/Training(Step): loss,1610377910049,0.07346341758966446
24006,-Metrics/Training(Step): loss,1610377914377,0.09160166233778
24008,-Metrics/Training(Step): loss,1610377918147,0.07902011275291443
24010,-Metrics/Training(Step): loss,1610377921422,0.06723897159099579
24012,-Metrics/Training(Step): loss,1610377925241,0.08290300518274307
24014,-Metrics/Training(Step): loss,1610377928669,0.0892183780670166
24016,-Metrics/Training(Step): loss,1610377932523,0.09276992082595825
24018,-Metrics/Training(Step): loss,1610377935819,0.07005003839731216
24020,-Metrics/Training(Step): loss,1610377939123,0.08829246461391449
24022,-Metrics/Training(Step): loss,1610377942726,0.10242177546024323
24024,-Metrics/Training(Step): loss,1610377946420,0.08955782651901245
24026,-Metrics/Training(Step): loss,1610377950297,0.11477368324995041
24028,-Metrics/Training(Step): loss,1610377953846,0.07727047055959702
24030,-Metrics/Training(Step): loss,1610377957257,0.08723127841949463
24032,-Metrics/Training(Step): loss,1610377961222,0.11041387170553207
24034,-Metrics/Training(Step): loss,1610377964820,0.0787820890545845
24036,-Metrics/Training(Step): loss,1610377968105,0.09247571974992752
24038,-Metrics/Training(Step): loss,1610377971819,0.07059332728385925
24040,-Metrics/Training(Step): loss,1610377975212,0.09200243651866913
24042,-Metrics/Training(Step): loss,1610377977404,0.06474727392196655
24044,-Metrics/Training(Step): loss,1610377979660,0.0966121107339859
24046,-Metrics/Training(Step): loss,1610377981883,0.08191625028848648
24048,-Metrics/Training(Step): loss,1610377983760,0.07392924278974533
24050,-Metrics/Training(Step): loss,1610377985834,0.08375208079814911
24052,-Metrics/Training(Step): loss,1610377987920,0.10154515504837036
24054,-Metrics/Training(Step): loss,1610377990009,0.08966483920812607
24056,-Metrics/Training(Step): loss,1610377992106,0.0868382528424263
24058,-Metrics/Training(Step): loss,1610377994186,0.10193056613206863
24060,-Metrics/Training(Step): loss,1610377996151,0.08872760087251663
24062,-Metrics/Training(Step): loss,1610377998178,0.08630488812923431
24064,-Metrics/Training(Step): loss,1610378000143,0.07236417382955551
24066,-Metrics/Training(Step): loss,1610378002092,0.08373837172985077
24068,-Metrics/Training(Step): loss,1610378004114,0.08943722397089005
24070,-Metrics/Training(Step): loss,1610378005974,0.0944114476442337
24072,-Metrics/Training(Step): loss,1610378007965,0.07298991829156876
24074,-Metrics/Training(Step): loss,1610378010018,0.07924024760723114
24076,-Metrics/Training(Step): loss,1610378012040,0.10880409181118011
24078,-Metrics/Training(Step): loss,1610378014077,0.0775645524263382
24080,-Metrics/Training(Step): loss,1610378016086,0.0867072343826294
24082,-Metrics/Training(Step): loss,1610378028132,0.11322367936372757
24084,-Metrics/Training(Step): loss,1610378032420,0.06793027371168137
24086,-Metrics/Training(Step): loss,1610378036521,0.0642673596739769
24088,-Metrics/Training(Step): loss,1610378040546,0.08779910951852798
24090,-Metrics/Training(Step): loss,1610378044623,0.07981491088867188
24092,-Metrics/Training(Step): loss,1610378048421,0.0839511901140213
24094,-Metrics/Training(Step): loss,1610378051919,0.08653388917446136
24096,-Metrics/Training(Step): loss,1610378056074,0.07499291747808456
24098,-Metrics/Training(Step): loss,1610378059420,0.09116941690444946
24100,-Metrics/Training(Step): loss,1610378062420,0.11880673468112946
24102,-Metrics/Training(Step): loss,1610378066214,0.07866829633712769
24104,-Metrics/Training(Step): loss,1610378069835,0.06909330934286118
24106,-Metrics/Training(Step): loss,1610378073621,0.07569028437137604
24108,-Metrics/Training(Step): loss,1610378077366,0.07667727023363113
24110,-Metrics/Training(Step): loss,1610378081319,0.07076199352741241
24112,-Metrics/Training(Step): loss,1610378084847,0.09898388385772705
24114,-Metrics/Training(Step): loss,1610378088322,0.09771037101745605
24116,-Metrics/Training(Step): loss,1610378092188,0.07454977184534073
24118,-Metrics/Training(Step): loss,1610378095615,0.0800120159983635
24120,-Metrics/Training(Step): loss,1610378099622,0.06941135972738266
24122,-Metrics/Training(Step): loss,1610378102919,0.0751953274011612
24124,-Metrics/Training(Step): loss,1610378106448,0.0877956822514534
24126,-Metrics/Training(Step): loss,1610378109964,0.08086047321557999
24128,-Metrics/Training(Step): loss,1610378112332,0.0879930853843689
24130,-Metrics/Training(Step): loss,1610378114504,0.08057674020528793
24132,-Metrics/Training(Step): loss,1610378116436,0.059699226170778275
24134,-Metrics/Training(Step): loss,1610378118432,0.08119695633649826
24136,-Metrics/Training(Step): loss,1610378120492,0.08600500226020813
24138,-Metrics/Training(Step): loss,1610378122569,0.08938399702310562
24140,-Metrics/Training(Step): loss,1610378124630,0.09161882102489471
24142,-Metrics/Training(Step): loss,1610378126704,0.09098681807518005
24144,-Metrics/Training(Step): loss,1610378128584,0.1081443727016449
24146,-Metrics/Training(Step): loss,1610378130585,0.06792232394218445
24148,-Metrics/Training(Step): loss,1610378132596,0.08904006332159042
24150,-Metrics/Training(Step): loss,1610378134629,0.08651432394981384
24152,-Metrics/Training(Step): loss,1610378136650,0.09049726277589798
24154,-Metrics/Training(Step): loss,1610378138623,0.0867915153503418
24156,-Metrics/Training(Step): loss,1610378140645,0.08046763390302658
24158,-Metrics/Training(Step): loss,1610378142732,0.0629618912935257
24160,-Metrics/Training(Step): loss,1610378144721,0.09199920296669006
24162,-Metrics/Training(Step): loss,1610378146683,0.09542360156774521
24164,-Metrics/Training(Step): loss,1610378148755,0.08294758945703506
24166,-Metrics/Training(Step): loss,1610378150776,0.1043219342827797
24168,-Metrics/Training(Step): loss,1610378162426,0.08369676768779755
24170,-Metrics/Training(Step): loss,1610378166531,0.07572951167821884
24172,-Metrics/Training(Step): loss,1610378170820,0.09552815556526184
24174,-Metrics/Training(Step): loss,1610378174837,0.07319741696119308
24176,-Metrics/Training(Step): loss,1610378179025,0.08923536539077759
24178,-Metrics/Training(Step): loss,1610378183215,0.08103381097316742
24180,-Metrics/Training(Step): loss,1610378187217,0.08155501633882523
24182,-Metrics/Training(Step): loss,1610378191031,0.06593769788742065
24184,-Metrics/Training(Step): loss,1610378194420,0.07177641242742538
24186,-Metrics/Training(Step): loss,1610378198320,0.0835760161280632
24188,-Metrics/Training(Step): loss,1610378201685,0.09767933189868927
24190,-Metrics/Training(Step): loss,1610378205238,0.09396948665380478
24192,-Metrics/Training(Step): loss,1610378208820,0.09322936832904816
24194,-Metrics/Training(Step): loss,1610378212163,0.08797670900821686
24196,-Metrics/Training(Step): loss,1610378215920,0.09990683197975159
24198,-Metrics/Training(Step): loss,1610378219444,0.08243342489004135
24200,-Metrics/Training(Step): loss,1610378222820,0.07424197345972061
24202,-Metrics/Training(Step): loss,1610378226119,0.07546663284301758
24204,-Metrics/Training(Step): loss,1610378229746,0.0841880589723587
24206,-Metrics/Training(Step): loss,1610378233121,0.08135630190372467
24208,-Metrics/Training(Step): loss,1610378237480,0.09006903320550919
24210,-Metrics/Training(Step): loss,1610378240819,0.08296176791191101
24212,-Metrics/Training(Step): loss,1610378244285,0.10035751014947891
24214,-Metrics/Training(Step): loss,1610378246870,0.08768045902252197
24216,-Metrics/Training(Step): loss,1610378249032,0.09144040942192078
24218,-Metrics/Training(Step): loss,1610378251100,0.108428955078125
24220,-Metrics/Training(Step): loss,1610378253188,0.07088904082775116
24222,-Metrics/Training(Step): loss,1610378255282,0.08571840077638626
24224,-Metrics/Training(Step): loss,1610378257346,0.10385783761739731
24226,-Metrics/Training(Step): loss,1610378259428,0.06980214267969131
24228,-Metrics/Training(Step): loss,1610378261299,0.08976195752620697
24230,-Metrics/Training(Step): loss,1610378263294,0.1042814701795578
24232,-Metrics/Training(Step): loss,1610378265358,0.05221032351255417
24234,-Metrics/Training(Step): loss,1610378267358,0.09142663329839706
24236,-Metrics/Training(Step): loss,1610378269275,0.09008512645959854
24238,-Metrics/Training(Step): loss,1610378271275,0.07942372560501099
24240,-Metrics/Training(Step): loss,1610378273291,0.07741756737232208
24242,-Metrics/Training(Step): loss,1610378275293,0.0953359305858612
24244,-Metrics/Training(Step): loss,1610378277338,0.1009560078382492
24246,-Metrics/Training(Step): loss,1610378279298,0.06874586641788483
24248,-Metrics/Training(Step): loss,1610378281315,0.0868525505065918
24250,-Metrics/Training(Step): loss,1610378283285,0.08188773691654205
24252,-Metrics/Training(Step): loss,1610378285342,0.09501476585865021
24254,-Metrics/Training(Step): loss,1610378297048,0.06657280772924423
24256,-Metrics/Training(Step): loss,1610378301025,0.08214765042066574
24258,-Metrics/Training(Step): loss,1610378305219,0.06681256741285324
24260,-Metrics/Training(Step): loss,1610378308919,0.08294118195772171
24262,-Metrics/Training(Step): loss,1610378312858,0.10196170955896378
24264,-Metrics/Training(Step): loss,1610378316847,0.07918103039264679
24266,-Metrics/Training(Step): loss,1610378321130,0.08618621528148651
24268,-Metrics/Training(Step): loss,1610378325419,0.06678197532892227
24270,-Metrics/Training(Step): loss,1610378329424,0.07510952651500702
24272,-Metrics/Training(Step): loss,1610378332932,0.07941927015781403
24274,-Metrics/Training(Step): loss,1610378336419,0.0767008513212204
24276,-Metrics/Training(Step): loss,1610378339678,0.08394636958837509
24278,-Metrics/Training(Step): loss,1610378342829,0.07692126929759979
24280,-Metrics/Training(Step): loss,1610378346567,0.06924233585596085
24282,-Metrics/Training(Step): loss,1610378350420,0.08694104105234146
24284,-Metrics/Training(Step): loss,1610378353592,0.09306826442480087
24286,-Metrics/Training(Step): loss,1610378356741,0.0982629805803299
24288,-Metrics/Training(Step): loss,1610378359820,0.08308892697095871
24290,-Metrics/Training(Step): loss,1610378362817,0.07580289244651794
24292,-Metrics/Training(Step): loss,1610378366520,0.08236850053071976
24294,-Metrics/Training(Step): loss,1610378370548,0.08295826613903046
24296,-Metrics/Training(Step): loss,1610378374136,0.07280122488737106
24298,-Metrics/Training(Step): loss,1610378377719,0.09856478869915009
24300,-Metrics/Training(Step): loss,1610378381052,0.08524490892887115
24302,-Metrics/Training(Step): loss,1610378383635,0.08735805004835129
24304,-Metrics/Training(Step): loss,1610378385735,0.07262292504310608
24306,-Metrics/Training(Step): loss,1610378387939,0.08662911504507065
24308,-Metrics/Training(Step): loss,1610378389945,0.09832935035228729
24310,-Metrics/Training(Step): loss,1610378392048,0.09869281947612762
24312,-Metrics/Training(Step): loss,1610378394120,0.0897044986486435
24314,-Metrics/Training(Step): loss,1610378396310,0.0855700746178627
24316,-Metrics/Training(Step): loss,1610378398430,0.06565213948488235
24318,-Metrics/Training(Step): loss,1610378400485,0.0620507076382637
24320,-Metrics/Training(Step): loss,1610378402591,0.07866355776786804
24322,-Metrics/Training(Step): loss,1610378404664,0.08331093937158585
24324,-Metrics/Training(Step): loss,1610378406700,0.08572173118591309
24326,-Metrics/Training(Step): loss,1610378408744,0.07290641218423843
24328,-Metrics/Training(Step): loss,1610378410765,0.09904840588569641
24330,-Metrics/Training(Step): loss,1610378412779,0.09993934631347656
24332,-Metrics/Training(Step): loss,1610378414855,0.09300511330366135
24334,-Metrics/Training(Step): loss,1610378416842,0.08986149728298187
24336,-Metrics/Training(Step): loss,1610378418875,0.09787791967391968
24338,-Metrics/Training(Step): loss,1610378420884,0.07243865728378296
24340,-Metrics/Training(Step): loss,1610378432727,0.0839691013097763
24342,-Metrics/Training(Step): loss,1610378437320,0.07681934535503387
24344,-Metrics/Training(Step): loss,1610378441220,0.0888407826423645
24346,-Metrics/Training(Step): loss,1610378445422,0.08141592890024185
24348,-Metrics/Training(Step): loss,1610378449720,0.07003969699144363
24350,-Metrics/Training(Step): loss,1610378453121,0.09026073664426804
24352,-Metrics/Training(Step): loss,1610378457017,0.09875525534152985
24354,-Metrics/Training(Step): loss,1610378460816,0.08187109977006912
24356,-Metrics/Training(Step): loss,1610378464941,0.10675068944692612
24358,-Metrics/Training(Step): loss,1610378468532,0.07609803229570389
24360,-Metrics/Training(Step): loss,1610378471995,0.10016277432441711
24362,-Metrics/Training(Step): loss,1610378475283,0.0943920761346817
24364,-Metrics/Training(Step): loss,1610378478832,0.09022693336009979
24366,-Metrics/Training(Step): loss,1610378482358,0.0756985992193222
24368,-Metrics/Training(Step): loss,1610378485820,0.08731620013713837
24370,-Metrics/Training(Step): loss,1610378489519,0.08484035730361938
24372,-Metrics/Training(Step): loss,1610378492812,0.08210742473602295
24374,-Metrics/Training(Step): loss,1610378495945,0.10181532055139542
24376,-Metrics/Training(Step): loss,1610378499920,0.10215380787849426
24378,-Metrics/Training(Step): loss,1610378503120,0.09789324551820755
24380,-Metrics/Training(Step): loss,1610378506815,0.06175462156534195
24382,-Metrics/Training(Step): loss,1610378510525,0.09124002605676651
24384,-Metrics/Training(Step): loss,1610378514119,0.07871181517839432
24386,-Metrics/Training(Step): loss,1610378516677,0.09857245534658432
24388,-Metrics/Training(Step): loss,1610378518985,0.07964669913053513
24390,-Metrics/Training(Step): loss,1610378521080,0.09808607399463654
24392,-Metrics/Training(Step): loss,1610378523049,0.0850486010313034
24394,-Metrics/Training(Step): loss,1610378525106,0.07498675584793091
24396,-Metrics/Training(Step): loss,1610378527192,0.08624499291181564
24398,-Metrics/Training(Step): loss,1610378529281,0.07027476280927658
24400,-Metrics/Training(Step): loss,1610378531373,0.0895647257566452
24402,-Metrics/Training(Step): loss,1610378533422,0.09009567648172379
24404,-Metrics/Training(Step): loss,1610378535421,0.0679105743765831
24406,-Metrics/Training(Step): loss,1610378537440,0.07611589878797531
24408,-Metrics/Training(Step): loss,1610378539539,0.08873388171195984
24410,-Metrics/Training(Step): loss,1610378541543,0.08272748440504074
24412,-Metrics/Training(Step): loss,1610378543515,0.09286952018737793
24414,-Metrics/Training(Step): loss,1610378545553,0.07768725603818893
24416,-Metrics/Training(Step): loss,1610378547468,0.07909706979990005
24418,-Metrics/Training(Step): loss,1610378549526,0.08247818797826767
24420,-Metrics/Training(Step): loss,1610378551591,0.063593290746212
24422,-Metrics/Training(Step): loss,1610378553612,0.07993209362030029
24424,-Metrics/Training(Step): loss,1610378555635,0.10410988330841064
24426,-Metrics/Training(Step): loss,1610378567744,0.07117258757352829
24428,-Metrics/Training(Step): loss,1610378571926,0.10054334253072739
24430,-Metrics/Training(Step): loss,1610378575821,0.08928170800209045
24432,-Metrics/Training(Step): loss,1610378579919,0.09249025583267212
24434,-Metrics/Training(Step): loss,1610378583820,0.0763198658823967
24436,-Metrics/Training(Step): loss,1610378587716,0.09138596057891846
24438,-Metrics/Training(Step): loss,1610378591718,0.08672521263360977
24440,-Metrics/Training(Step): loss,1610378596032,0.09985755383968353
24442,-Metrics/Training(Step): loss,1610378599951,0.08128776401281357
24444,-Metrics/Training(Step): loss,1610378603233,0.06032046303153038
24446,-Metrics/Training(Step): loss,1610378606134,0.0769314095377922
24448,-Metrics/Training(Step): loss,1610378609517,0.0944870263338089
24450,-Metrics/Training(Step): loss,1610378612817,0.058981865644454956
24452,-Metrics/Training(Step): loss,1610378616421,0.08199017494916916
24454,-Metrics/Training(Step): loss,1610378619858,0.07804001867771149
24456,-Metrics/Training(Step): loss,1610378623419,0.0763360783457756
24458,-Metrics/Training(Step): loss,1610378627103,0.0917191132903099
24460,-Metrics/Training(Step): loss,1610378630522,0.08893310278654099
24462,-Metrics/Training(Step): loss,1610378633947,0.07962732017040253
24464,-Metrics/Training(Step): loss,1610378637621,0.09437135607004166
24466,-Metrics/Training(Step): loss,1610378641215,0.07550694048404694
24468,-Metrics/Training(Step): loss,1610378644319,0.09652066975831985
24470,-Metrics/Training(Step): loss,1610378647824,0.06772379577159882
24472,-Metrics/Training(Step): loss,1610378650522,0.08761213719844818
24474,-Metrics/Training(Step): loss,1610378652780,0.10498087853193283
24476,-Metrics/Training(Step): loss,1610378655031,0.07996731996536255
24478,-Metrics/Training(Step): loss,1610378657126,0.07000953704118729
24480,-Metrics/Training(Step): loss,1610378659214,0.08271940052509308
24482,-Metrics/Training(Step): loss,1610378661341,0.10166861116886139
24484,-Metrics/Training(Step): loss,1610378663384,0.09179937094449997
24486,-Metrics/Training(Step): loss,1610378665424,0.06754525750875473
24488,-Metrics/Training(Step): loss,1610378667495,0.08549068868160248
24490,-Metrics/Training(Step): loss,1610378669503,0.08015721291303635
24492,-Metrics/Training(Step): loss,1610378671602,0.06723552942276001
24494,-Metrics/Training(Step): loss,1610378673618,0.07438451051712036
24496,-Metrics/Training(Step): loss,1610378675671,0.09344413876533508
24498,-Metrics/Training(Step): loss,1610378677694,0.0691133365035057
24500,-Metrics/Training(Step): loss,1610378679894,0.08410385251045227
24502,-Metrics/Training(Step): loss,1610378681887,0.0698506236076355
24504,-Metrics/Training(Step): loss,1610378684180,0.09327201545238495
24506,-Metrics/Training(Step): loss,1610378686239,0.08027700334787369
24508,-Metrics/Training(Step): loss,1610378688283,0.0835576206445694
24510,-Metrics/Training(Step): loss,1610378690303,0.08751731365919113
24512,-Metrics/Training(Step): loss,1610378702421,0.08867030590772629
24514,-Metrics/Training(Step): loss,1610378706520,0.09234373271465302
24516,-Metrics/Training(Step): loss,1610378710241,0.08321063965559006
24518,-Metrics/Training(Step): loss,1610378714417,0.07123340666294098
24520,-Metrics/Training(Step): loss,1610378718527,0.08447265625
24522,-Metrics/Training(Step): loss,1610378722532,0.08396655321121216
24524,-Metrics/Training(Step): loss,1610378726146,0.07435346394777298
24526,-Metrics/Training(Step): loss,1610378729918,0.0898071825504303
24528,-Metrics/Training(Step): loss,1610378733555,0.08812649548053741
24530,-Metrics/Training(Step): loss,1610378736820,0.05844150856137276
24532,-Metrics/Training(Step): loss,1610378740243,0.10326212644577026
24534,-Metrics/Training(Step): loss,1610378744103,0.07252395898103714
24536,-Metrics/Training(Step): loss,1610378747820,0.08280251920223236
24538,-Metrics/Training(Step): loss,1610378751396,0.0957995355129242
24540,-Metrics/Training(Step): loss,1610378755119,0.07566387951374054
24542,-Metrics/Training(Step): loss,1610378758820,0.07679545134305954
24544,-Metrics/Training(Step): loss,1610378762241,0.08870881795883179
24546,-Metrics/Training(Step): loss,1610378765614,0.07964009791612625
24548,-Metrics/Training(Step): loss,1610378768924,0.08120635896921158
24550,-Metrics/Training(Step): loss,1610378772426,0.09404577314853668
24552,-Metrics/Training(Step): loss,1610378776189,0.078779436647892
24554,-Metrics/Training(Step): loss,1610378779620,0.06721921265125275
24556,-Metrics/Training(Step): loss,1610378783310,0.07242243736982346
24558,-Metrics/Training(Step): loss,1610378786288,0.08889128267765045
24560,-Metrics/Training(Step): loss,1610378788712,0.05245799943804741
24562,-Metrics/Training(Step): loss,1610378790706,0.09409032016992569
24564,-Metrics/Training(Step): loss,1610378792843,0.06687963753938675
24566,-Metrics/Training(Step): loss,1610378794837,0.08372084051370621
24568,-Metrics/Training(Step): loss,1610378796910,0.10684415698051453
24570,-Metrics/Training(Step): loss,1610378799023,0.08321546763181686
24572,-Metrics/Training(Step): loss,1610378801015,0.094951331615448
24574,-Metrics/Training(Step): loss,1610378803082,0.09994342178106308
24576,-Metrics/Training(Step): loss,1610378804805,0.07328557968139648
24578,-Metrics/Training(Step): loss,1610378806903,0.08499248325824738
24580,-Metrics/Training(Step): loss,1610378808961,0.09288788586854935
24582,-Metrics/Training(Step): loss,1610378810937,0.08994538336992264
24584,-Metrics/Training(Step): loss,1610378812694,0.08605946600437164
24586,-Metrics/Training(Step): loss,1610378814699,0.08188311755657196
24588,-Metrics/Training(Step): loss,1610378816764,0.09452836960554123
24590,-Metrics/Training(Step): loss,1610378818836,0.07118503004312515
24592,-Metrics/Training(Step): loss,1610378820862,0.06454163789749146
24594,-Metrics/Training(Step): loss,1610378822868,0.05944155529141426
24596,-Metrics/Training(Step): loss,1610378824915,0.0891956090927124
24598,-Metrics/Training(Step): loss,1610378837227,0.08930153399705887
24600,-Metrics/Training(Step): loss,1610378841531,0.08350109308958054
24602,-Metrics/Training(Step): loss,1610378845822,0.09006944298744202
24604,-Metrics/Training(Step): loss,1610378849436,0.09858367592096329
24606,-Metrics/Training(Step): loss,1610378853016,0.07855070382356644
24608,-Metrics/Training(Step): loss,1610378856644,0.05269269645214081
24610,-Metrics/Training(Step): loss,1610378860616,0.08179175853729248
24612,-Metrics/Training(Step): loss,1610378864695,0.06992567330598831
24614,-Metrics/Training(Step): loss,1610378868321,0.09995227307081223
24616,-Metrics/Training(Step): loss,1610378872121,0.08459195494651794
24618,-Metrics/Training(Step): loss,1610378875221,0.07547903060913086
24620,-Metrics/Training(Step): loss,1610378878792,0.09016967564821243
24622,-Metrics/Training(Step): loss,1610378882215,0.08787461370229721
24624,-Metrics/Training(Step): loss,1610378886019,0.08467109501361847
24626,-Metrics/Training(Step): loss,1610378889814,0.07337929308414459
24628,-Metrics/Training(Step): loss,1610378893519,0.07642209529876709
24630,-Metrics/Training(Step): loss,1610378896731,0.09312646836042404
24632,-Metrics/Training(Step): loss,1610378899848,0.08121801167726517
24634,-Metrics/Training(Step): loss,1610378903221,0.10441776365041733
24636,-Metrics/Training(Step): loss,1610378906723,0.07898630201816559
24638,-Metrics/Training(Step): loss,1610378910440,0.07737688720226288
24640,-Metrics/Training(Step): loss,1610378914013,0.09682417660951614
24642,-Metrics/Training(Step): loss,1610378917264,0.07716688513755798
24644,-Metrics/Training(Step): loss,1610378920432,0.09965752065181732
24646,-Metrics/Training(Step): loss,1610378922592,0.08652684092521667
24648,-Metrics/Training(Step): loss,1610378924820,0.07809726893901825
24650,-Metrics/Training(Step): loss,1610378927178,0.07040015608072281
24652,-Metrics/Training(Step): loss,1610378929171,0.08867786824703217
24654,-Metrics/Training(Step): loss,1610378931250,0.1106630191206932
24656,-Metrics/Training(Step): loss,1610378933240,0.06671970337629318
24658,-Metrics/Training(Step): loss,1610378935317,0.06999184936285019
24660,-Metrics/Training(Step): loss,1610378937351,0.0897778570652008
24662,-Metrics/Training(Step): loss,1610378939422,0.0759068951010704
24664,-Metrics/Training(Step): loss,1610378941512,0.08105592429637909
24666,-Metrics/Training(Step): loss,1610378943477,0.10745754092931747
24668,-Metrics/Training(Step): loss,1610378945564,0.08663232624530792
24670,-Metrics/Training(Step): loss,1610378947533,0.05531696975231171
24672,-Metrics/Training(Step): loss,1610378949482,0.09829831123352051
24674,-Metrics/Training(Step): loss,1610378951481,0.10308824479579926
24676,-Metrics/Training(Step): loss,1610378953421,0.08049310743808746
24678,-Metrics/Training(Step): loss,1610378955477,0.08868080377578735
24680,-Metrics/Training(Step): loss,1610378957504,0.08683888614177704
24682,-Metrics/Training(Step): loss,1610378959523,0.0899374708533287
24684,-Metrics/Training(Step): loss,1610378971444,0.07836515456438065
24686,-Metrics/Training(Step): loss,1610378975941,0.1107967421412468
24688,-Metrics/Training(Step): loss,1610378980025,0.08102839440107346
24690,-Metrics/Training(Step): loss,1610378984319,0.09254150092601776
24692,-Metrics/Training(Step): loss,1610378988420,0.0921158492565155
24694,-Metrics/Training(Step): loss,1610378992524,0.08077826350927353
24696,-Metrics/Training(Step): loss,1610378996755,0.07414917647838593
24698,-Metrics/Training(Step): loss,1610379000821,0.10061679035425186
24700,-Metrics/Training(Step): loss,1610379004408,0.07940054684877396
24702,-Metrics/Training(Step): loss,1610379007828,0.09798277169466019
24704,-Metrics/Training(Step): loss,1610379011816,0.08265896886587143
24706,-Metrics/Training(Step): loss,1610379015404,0.08613933622837067
24708,-Metrics/Training(Step): loss,1610379018837,0.105422243475914
24710,-Metrics/Training(Step): loss,1610379022458,0.09573901444673538
24712,-Metrics/Training(Step): loss,1610379025921,0.0960298404097557
24714,-Metrics/Training(Step): loss,1610379029046,0.09050672501325607
24716,-Metrics/Training(Step): loss,1610379032559,0.08216043561697006
24718,-Metrics/Training(Step): loss,1610379035912,0.06313124299049377
24720,-Metrics/Training(Step): loss,1610379039354,0.05670466274023056
24722,-Metrics/Training(Step): loss,1610379043078,0.08792563527822495
24724,-Metrics/Training(Step): loss,1610379046536,0.08223690092563629
24726,-Metrics/Training(Step): loss,1610379050192,0.0751192569732666
24728,-Metrics/Training(Step): loss,1610379053839,0.08994216471910477
24730,-Metrics/Training(Step): loss,1610379056529,0.076182521879673
24732,-Metrics/Training(Step): loss,1610379058634,0.09093613922595978
24734,-Metrics/Training(Step): loss,1610379060678,0.07451057434082031
24736,-Metrics/Training(Step): loss,1610379062704,0.0836329534649849
24738,-Metrics/Training(Step): loss,1610379064764,0.08627133816480637
24740,-Metrics/Training(Step): loss,1610379066765,0.09243689477443695
24742,-Metrics/Training(Step): loss,1610379068838,0.0841832086443901
24744,-Metrics/Training(Step): loss,1610379070915,0.10883259028196335
24746,-Metrics/Training(Step): loss,1610379072887,0.08163411915302277
24748,-Metrics/Training(Step): loss,1610379074933,0.08196783065795898
24750,-Metrics/Training(Step): loss,1610379077003,0.06887715309858322
24752,-Metrics/Training(Step): loss,1610379079013,0.07746296375989914
24754,-Metrics/Training(Step): loss,1610379080992,0.07986625283956528
24756,-Metrics/Training(Step): loss,1610379082891,0.08728768676519394
24758,-Metrics/Training(Step): loss,1610379084908,0.10012451559305191
24760,-Metrics/Training(Step): loss,1610379086948,0.07504267245531082
24762,-Metrics/Training(Step): loss,1610379088913,0.103395476937294
24764,-Metrics/Training(Step): loss,1610379090987,0.0833488181233406
24766,-Metrics/Training(Step): loss,1610379093005,0.08541664481163025
24768,-Metrics/Training(Step): loss,1610379094963,0.0679931491613388
24770,-Metrics/Training(Step): loss,1610379107127,0.09065810590982437
24772,-Metrics/Training(Step): loss,1610379111419,0.0897301658987999
24774,-Metrics/Training(Step): loss,1610379115039,0.11846520751714706
24776,-Metrics/Training(Step): loss,1610379119041,0.07986292243003845
24778,-Metrics/Training(Step): loss,1610379123330,0.11013080179691315
24780,-Metrics/Training(Step): loss,1610379127637,0.09120352566242218
24782,-Metrics/Training(Step): loss,1610379132020,0.09992377460002899
24784,-Metrics/Training(Step): loss,1610379135347,0.06576908379793167
24786,-Metrics/Training(Step): loss,1610379139022,0.0773853287100792
24788,-Metrics/Training(Step): loss,1610379142320,0.08454794436693192
24790,-Metrics/Training(Step): loss,1610379146020,0.07074015587568283
24792,-Metrics/Training(Step): loss,1610379149542,0.0629231184720993
24794,-Metrics/Training(Step): loss,1610379152520,0.09353096038103104
24796,-Metrics/Training(Step): loss,1610379156771,0.09776947647333145
24798,-Metrics/Training(Step): loss,1610379160158,0.08784741163253784
24800,-Metrics/Training(Step): loss,1610379164138,0.07400030642747879
24802,-Metrics/Training(Step): loss,1610379167388,0.06692527234554291
24804,-Metrics/Training(Step): loss,1610379170629,0.0928274542093277
24806,-Metrics/Training(Step): loss,1610379174017,0.07990692555904388
24808,-Metrics/Training(Step): loss,1610379177722,0.06721416860818863
24810,-Metrics/Training(Step): loss,1610379181239,0.09094022959470749
24812,-Metrics/Training(Step): loss,1610379184558,0.06906428188085556
24814,-Metrics/Training(Step): loss,1610379187937,0.09567909687757492
24816,-Metrics/Training(Step): loss,1610379190637,0.08439580351114273
24818,-Metrics/Training(Step): loss,1610379193020,0.0795476958155632
24820,-Metrics/Training(Step): loss,1610379195268,0.09405569732189178
24822,-Metrics/Training(Step): loss,1610379197329,0.07198154181241989
24824,-Metrics/Training(Step): loss,1610379199380,0.08315134793519974
24826,-Metrics/Training(Step): loss,1610379201500,0.08648918569087982
24828,-Metrics/Training(Step): loss,1610379203581,0.10253684967756271
24830,-Metrics/Training(Step): loss,1610379205653,0.07267224788665771
24832,-Metrics/Training(Step): loss,1610379207608,0.06610848754644394
24834,-Metrics/Training(Step): loss,1610379209607,0.07879725098609924
24836,-Metrics/Training(Step): loss,1610379211525,0.0796193927526474
24838,-Metrics/Training(Step): loss,1610379213430,0.09731955826282501
24840,-Metrics/Training(Step): loss,1610379215499,0.0992886945605278
24842,-Metrics/Training(Step): loss,1610379217571,0.07382582128047943
24844,-Metrics/Training(Step): loss,1610379219599,0.09471284598112106
24846,-Metrics/Training(Step): loss,1610379221637,0.09421442449092865
24848,-Metrics/Training(Step): loss,1610379223721,0.07825399935245514
24850,-Metrics/Training(Step): loss,1610379225771,0.07074488699436188
24852,-Metrics/Training(Step): loss,1610379227805,0.09183835983276367
24854,-Metrics/Training(Step): loss,1610379229702,0.08589280396699905
24856,-Metrics/Training(Step): loss,1610379242148,0.09994907677173615
24858,-Metrics/Training(Step): loss,1610379246458,0.09499791264533997
24860,-Metrics/Training(Step): loss,1610379250258,0.10335099697113037
24862,-Metrics/Training(Step): loss,1610379254147,0.08866959065198898
24864,-Metrics/Training(Step): loss,1610379258525,0.08607867360115051
24866,-Metrics/Training(Step): loss,1610379262610,0.0901922658085823
24868,-Metrics/Training(Step): loss,1610379266054,0.09970656782388687
24870,-Metrics/Training(Step): loss,1610379269743,0.07960209995508194
24872,-Metrics/Training(Step): loss,1610379273386,0.07533091306686401
24874,-Metrics/Training(Step): loss,1610379276890,0.08751573413610458
24876,-Metrics/Training(Step): loss,1610379280723,0.09688969701528549
24878,-Metrics/Training(Step): loss,1610379284419,0.07936905324459076
24880,-Metrics/Training(Step): loss,1610379287990,0.08341324329376221
24882,-Metrics/Training(Step): loss,1610379291469,0.08948289602994919
24884,-Metrics/Training(Step): loss,1610379295624,0.08787453174591064
24886,-Metrics/Training(Step): loss,1610379299280,0.08599007874727249
24888,-Metrics/Training(Step): loss,1610379303020,0.08839426189661026
24890,-Metrics/Training(Step): loss,1610379306335,0.08580412715673447
24892,-Metrics/Training(Step): loss,1610379309820,0.06763742864131927
24894,-Metrics/Training(Step): loss,1610379313220,0.07748259603977203
24896,-Metrics/Training(Step): loss,1610379316621,0.08510199189186096
24898,-Metrics/Training(Step): loss,1610379320020,0.06781817972660065
24900,-Metrics/Training(Step): loss,1610379323206,0.07223249971866608
24902,-Metrics/Training(Step): loss,1610379325547,0.07281120866537094
24904,-Metrics/Training(Step): loss,1610379327659,0.08268554508686066
24906,-Metrics/Training(Step): loss,1610379329801,0.07813753932714462
24908,-Metrics/Training(Step): loss,1610379331839,0.07770539820194244
24910,-Metrics/Training(Step): loss,1610379333928,0.07286319136619568
24912,-Metrics/Training(Step): loss,1610379336041,0.08901634812355042
24914,-Metrics/Training(Step): loss,1610379338109,0.08561889827251434
24916,-Metrics/Training(Step): loss,1610379340176,0.08036632835865021
24918,-Metrics/Training(Step): loss,1610379342120,0.09342406690120697
24920,-Metrics/Training(Step): loss,1610379344180,0.09133712947368622
24922,-Metrics/Training(Step): loss,1610379346173,0.09733059257268906
24924,-Metrics/Training(Step): loss,1610379348210,0.07647299021482468
24926,-Metrics/Training(Step): loss,1610379350142,0.07892517000436783
24928,-Metrics/Training(Step): loss,1610379352197,0.0958852767944336
24930,-Metrics/Training(Step): loss,1610379354241,0.08159907162189484
24932,-Metrics/Training(Step): loss,1610379356268,0.06521918624639511
24934,-Metrics/Training(Step): loss,1610379358319,0.06833367049694061
24936,-Metrics/Training(Step): loss,1610379360300,0.08825109153985977
24938,-Metrics/Training(Step): loss,1610379362320,0.0935463085770607
24940,-Metrics/Training(Step): loss,1610379364345,0.09276179224252701
24942,-Metrics/Training(Step): loss,1610379376837,0.07652195543050766
24944,-Metrics/Training(Step): loss,1610379381219,0.0793464183807373
24946,-Metrics/Training(Step): loss,1610379385022,0.08925407379865646
24948,-Metrics/Training(Step): loss,1610379388619,0.08850086480379105
24950,-Metrics/Training(Step): loss,1610379392318,0.0947481095790863
24952,-Metrics/Training(Step): loss,1610379396048,0.08283834159374237
24954,-Metrics/Training(Step): loss,1610379400222,0.07705429941415787
24956,-Metrics/Training(Step): loss,1610379404021,0.08561613410711288
24958,-Metrics/Training(Step): loss,1610379407951,0.08322516828775406
24960,-Metrics/Training(Step): loss,1610379411441,0.08014795184135437
24962,-Metrics/Training(Step): loss,1610379414820,0.1019495353102684
24964,-Metrics/Training(Step): loss,1610379418257,0.07613734900951385
24966,-Metrics/Training(Step): loss,1610379421911,0.06427837163209915
24968,-Metrics/Training(Step): loss,1610379425720,0.08755014091730118
24970,-Metrics/Training(Step): loss,1610379428937,0.07483267784118652
24972,-Metrics/Training(Step): loss,1610379432457,0.09918587654829025
24974,-Metrics/Training(Step): loss,1610379436116,0.09499343484640121
24976,-Metrics/Training(Step): loss,1610379439421,0.07885009795427322
24978,-Metrics/Training(Step): loss,1610379443155,0.09810664504766464
24980,-Metrics/Training(Step): loss,1610379446832,0.07994566857814789
24982,-Metrics/Training(Step): loss,1610379450322,0.10017445683479309
24984,-Metrics/Training(Step): loss,1610379453620,0.07011469453573227
24986,-Metrics/Training(Step): loss,1610379457120,0.07991623133420944
24988,-Metrics/Training(Step): loss,1610379460103,0.0690401941537857
24990,-Metrics/Training(Step): loss,1610379462278,0.09478909522294998
24992,-Metrics/Training(Step): loss,1610379464554,0.07927104830741882
24994,-Metrics/Training(Step): loss,1610379466547,0.09556785225868225
24996,-Metrics/Training(Step): loss,1610379468626,0.08472225815057755
24998,-Metrics/Training(Step): loss,1610379470724,0.10099373012781143
25000,-Metrics/Training(Step): loss,1610379472786,0.08504564315080643
25002,-Metrics/Training(Step): loss,1610379474876,0.07559061795473099
25004,-Metrics/Training(Step): loss,1610379476902,0.08811706304550171
25006,-Metrics/Training(Step): loss,1610379478927,0.05738373473286629
25008,-Metrics/Training(Step): loss,1610379480956,0.07052663713693619
25010,-Metrics/Training(Step): loss,1610379482890,0.09368736296892166
25012,-Metrics/Training(Step): loss,1610379484876,0.08504077047109604
25014,-Metrics/Training(Step): loss,1610379486886,0.0708855614066124
25016,-Metrics/Training(Step): loss,1610379488858,0.08430591225624084
25018,-Metrics/Training(Step): loss,1610379490836,0.06693802773952484
25020,-Metrics/Training(Step): loss,1610379492831,0.07028377056121826
25022,-Metrics/Training(Step): loss,1610379494871,0.09038697183132172
25024,-Metrics/Training(Step): loss,1610379496907,0.08774354308843613
25026,-Metrics/Training(Step): loss,1610379498951,0.09342364221811295
25028,-Metrics/Training(Step): loss,1610379511233,0.08395586162805557
25030,-Metrics/Training(Step): loss,1610379515419,0.08028211444616318
25032,-Metrics/Training(Step): loss,1610379519220,0.09623080492019653
25034,-Metrics/Training(Step): loss,1610379523131,0.07363074272871017
25036,-Metrics/Training(Step): loss,1610379526948,0.08998873084783554
25038,-Metrics/Training(Step): loss,1610379530945,0.0834469273686409
25040,-Metrics/Training(Step): loss,1610379535448,0.0933612659573555
25042,-Metrics/Training(Step): loss,1610379539562,0.07376648485660553
25044,-Metrics/Training(Step): loss,1610379542932,0.06774228811264038
25046,-Metrics/Training(Step): loss,1610379546478,0.09137872606515884
25048,-Metrics/Training(Step): loss,1610379550025,0.07735337316989899
25050,-Metrics/Training(Step): loss,1610379553639,0.08928646892309189
25052,-Metrics/Training(Step): loss,1610379557222,0.0893329456448555
25054,-Metrics/Training(Step): loss,1610379561023,0.09054777026176453
25056,-Metrics/Training(Step): loss,1610379564458,0.09289921075105667
25058,-Metrics/Training(Step): loss,1610379568548,0.05658004805445671
25060,-Metrics/Training(Step): loss,1610379571958,0.07816112786531448
25062,-Metrics/Training(Step): loss,1610379575358,0.07941616326570511
25064,-Metrics/Training(Step): loss,1610379579219,0.1122286394238472
25066,-Metrics/Training(Step): loss,1610379582920,0.07887234538793564
25068,-Metrics/Training(Step): loss,1610379586115,0.0790686309337616
25070,-Metrics/Training(Step): loss,1610379589888,0.05705087259411812
25072,-Metrics/Training(Step): loss,1610379592866,0.07108185440301895
25074,-Metrics/Training(Step): loss,1610379595333,0.10809052735567093
25076,-Metrics/Training(Step): loss,1610379597519,0.0830201804637909
25078,-Metrics/Training(Step): loss,1610379599610,0.08530105650424957
25080,-Metrics/Training(Step): loss,1610379601643,0.09783180058002472
25082,-Metrics/Training(Step): loss,1610379603733,0.10495259612798691
25084,-Metrics/Training(Step): loss,1610379605801,0.09805938601493835
25086,-Metrics/Training(Step): loss,1610379607877,0.06871682405471802
25088,-Metrics/Training(Step): loss,1610379609976,0.06652597337961197
25090,-Metrics/Training(Step): loss,1610379612076,0.08256987482309341
25092,-Metrics/Training(Step): loss,1610379614059,0.09346634894609451
25094,-Metrics/Training(Step): loss,1610379616097,0.10981155931949615
25096,-Metrics/Training(Step): loss,1610379618166,0.07557179778814316
25098,-Metrics/Training(Step): loss,1610379620179,0.07635290920734406
25100,-Metrics/Training(Step): loss,1610379622095,0.086407870054245
25102,-Metrics/Training(Step): loss,1610379624168,0.0794958770275116
25104,-Metrics/Training(Step): loss,1610379626058,0.07874340564012527
25106,-Metrics/Training(Step): loss,1610379628083,0.09933259338140488
25108,-Metrics/Training(Step): loss,1610379630251,0.07098611444234848
25110,-Metrics/Training(Step): loss,1610379632149,0.085340216755867
25112,-Metrics/Training(Step): loss,1610379634195,0.09149806946516037
25114,-Metrics/Training(Step): loss,1610379647632,0.0827222540974617
25116,-Metrics/Training(Step): loss,1610379651620,0.09055771678686142
25118,-Metrics/Training(Step): loss,1610379655826,0.08859215676784515
25120,-Metrics/Training(Step): loss,1610379659820,0.08255977928638458
25122,-Metrics/Training(Step): loss,1610379663619,0.08640934526920319
25124,-Metrics/Training(Step): loss,1610379667733,0.08998019248247147
25126,-Metrics/Training(Step): loss,1610379671586,0.09007419645786285
25128,-Metrics/Training(Step): loss,1610379675520,0.0646907314658165
25130,-Metrics/Training(Step): loss,1610379678823,0.07705610990524292
25132,-Metrics/Training(Step): loss,1610379682320,0.06935995072126389
25134,-Metrics/Training(Step): loss,1610379685517,0.06822141259908676
25136,-Metrics/Training(Step): loss,1610379689319,0.07995516806840897
25138,-Metrics/Training(Step): loss,1610379693049,0.07407932728528976
25140,-Metrics/Training(Step): loss,1610379696820,0.09140119701623917
25142,-Metrics/Training(Step): loss,1610379699780,0.09096605330705643
25144,-Metrics/Training(Step): loss,1610379703619,0.09083091467618942
25146,-Metrics/Training(Step): loss,1610379706741,0.09556848555803299
25148,-Metrics/Training(Step): loss,1610379710519,0.0760762020945549
25150,-Metrics/Training(Step): loss,1610379714320,0.09095091372728348
25152,-Metrics/Training(Step): loss,1610379718119,0.07121510803699493
25154,-Metrics/Training(Step): loss,1610379721059,0.07751589268445969
25156,-Metrics/Training(Step): loss,1610379724225,0.09262237697839737
25158,-Metrics/Training(Step): loss,1610379728051,0.08135762065649033
25160,-Metrics/Training(Step): loss,1610379731160,0.07124790549278259
25162,-Metrics/Training(Step): loss,1610379733356,0.08598535507917404
25164,-Metrics/Training(Step): loss,1610379735437,0.08519012480974197
25166,-Metrics/Training(Step): loss,1610379737487,0.09614541381597519
25168,-Metrics/Training(Step): loss,1610379739506,0.09549669176340103
25170,-Metrics/Training(Step): loss,1610379741601,0.05805937200784683
25172,-Metrics/Training(Step): loss,1610379743702,0.06919176876544952
25174,-Metrics/Training(Step): loss,1610379745805,0.07841569185256958
25176,-Metrics/Training(Step): loss,1610379747843,0.06457727402448654
25178,-Metrics/Training(Step): loss,1610379749903,0.10041742026805878
25180,-Metrics/Training(Step): loss,1610379751642,0.08338770270347595
25182,-Metrics/Training(Step): loss,1610379753718,0.09576751291751862
25184,-Metrics/Training(Step): loss,1610379755786,0.0941062718629837
25186,-Metrics/Training(Step): loss,1610379757819,0.07017211616039276
25188,-Metrics/Training(Step): loss,1610379759821,0.07587385177612305
25190,-Metrics/Training(Step): loss,1610379761848,0.08949190378189087
25192,-Metrics/Training(Step): loss,1610379763866,0.0858943909406662
25194,-Metrics/Training(Step): loss,1610379765926,0.07254080474376678
25196,-Metrics/Training(Step): loss,1610379767955,0.09197229892015457
25198,-Metrics/Training(Step): loss,1610379769983,0.10211370885372162
25200,-Metrics/Training(Step): loss,1610379782320,0.07666129618883133
25202,-Metrics/Training(Step): loss,1610379786623,0.09028630703687668
25204,-Metrics/Training(Step): loss,1610379790830,0.08096636086702347
25206,-Metrics/Training(Step): loss,1610379794721,0.07641267776489258
25208,-Metrics/Training(Step): loss,1610379798625,0.08739209175109863
25210,-Metrics/Training(Step): loss,1610379802918,0.08896952867507935
25212,-Metrics/Training(Step): loss,1610379806816,0.059842754155397415
25214,-Metrics/Training(Step): loss,1610379811058,0.07926064729690552
25216,-Metrics/Training(Step): loss,1610379814519,0.0713149830698967
25218,-Metrics/Training(Step): loss,1610379817543,0.08660263568162918
25220,-Metrics/Training(Step): loss,1610379821655,0.08267062157392502
25222,-Metrics/Training(Step): loss,1610379825324,0.06774736940860748
25224,-Metrics/Training(Step): loss,1610379829119,0.07887205481529236
25226,-Metrics/Training(Step): loss,1610379832419,0.08061666786670685
25228,-Metrics/Training(Step): loss,1610379836306,0.08420135825872421
25230,-Metrics/Training(Step): loss,1610379840270,0.0836423709988594
25232,-Metrics/Training(Step): loss,1610379843629,0.06707045435905457
25234,-Metrics/Training(Step): loss,1610379847127,0.07806301862001419
25236,-Metrics/Training(Step): loss,1610379850920,0.07982096821069717
25238,-Metrics/Training(Step): loss,1610379854520,0.08171918243169785
25240,-Metrics/Training(Step): loss,1610379858154,0.09812597930431366
25242,-Metrics/Training(Step): loss,1610379861336,0.09730873256921768
25244,-Metrics/Training(Step): loss,1610379864458,0.10202251374721527
25246,-Metrics/Training(Step): loss,1610379866842,0.09442264586687088
25248,-Metrics/Training(Step): loss,1610379869112,0.09384196251630783
25250,-Metrics/Training(Step): loss,1610379871158,0.08131721615791321
25252,-Metrics/Training(Step): loss,1610379873227,0.08293681591749191
25254,-Metrics/Training(Step): loss,1610379875209,0.08383751660585403
25256,-Metrics/Training(Step): loss,1610379877301,0.10003025829792023
25258,-Metrics/Training(Step): loss,1610379879410,0.07214754074811935
25260,-Metrics/Training(Step): loss,1610379881477,0.10298272222280502
25262,-Metrics/Training(Step): loss,1610379883547,0.06625242531299591
25264,-Metrics/Training(Step): loss,1610379885482,0.08813656866550446
25266,-Metrics/Training(Step): loss,1610379887524,0.09264547377824783
25268,-Metrics/Training(Step): loss,1610379889455,0.08090266585350037
25270,-Metrics/Training(Step): loss,1610379891501,0.08567863702774048
25272,-Metrics/Training(Step): loss,1610379893517,0.0709826648235321
25274,-Metrics/Training(Step): loss,1610379895506,0.09585439413785934
25276,-Metrics/Training(Step): loss,1610379897554,0.10562237352132797
25278,-Metrics/Training(Step): loss,1610379899513,0.08969604969024658
25280,-Metrics/Training(Step): loss,1610379901435,0.07877419143915176
25282,-Metrics/Training(Step): loss,1610379903444,0.0826248973608017
25284,-Metrics/Training(Step): loss,1610379905466,0.10968594253063202
25286,-Metrics/Training(Step): loss,1610379917826,0.08870436996221542
25288,-Metrics/Training(Step): loss,1610379921940,0.0844331756234169
25290,-Metrics/Training(Step): loss,1610379926022,0.07830104231834412
25292,-Metrics/Training(Step): loss,1610379930120,0.06837340444326401
25294,-Metrics/Training(Step): loss,1610379933934,0.08793316781520844
25296,-Metrics/Training(Step): loss,1610379937519,0.09067948907613754
25298,-Metrics/Training(Step): loss,1610379941344,0.08803799003362656
25300,-Metrics/Training(Step): loss,1610379945215,0.07732058316469193
25302,-Metrics/Training(Step): loss,1610379948915,0.09003333747386932
25304,-Metrics/Training(Step): loss,1610379952141,0.08297573775053024
25306,-Metrics/Training(Step): loss,1610379955919,0.0840502679347992
25308,-Metrics/Training(Step): loss,1610379959740,0.09032946079969406
25310,-Metrics/Training(Step): loss,1610379963063,0.09083669632673264
25312,-Metrics/Training(Step): loss,1610379966543,0.09733918309211731
25314,-Metrics/Training(Step): loss,1610379970131,0.058587465435266495
25316,-Metrics/Training(Step): loss,1610379973722,0.09382196515798569
25318,-Metrics/Training(Step): loss,1610379977119,0.08174887299537659
25320,-Metrics/Training(Step): loss,1610379980458,0.06299754977226257
25322,-Metrics/Training(Step): loss,1610379984140,0.08871583640575409
25324,-Metrics/Training(Step): loss,1610379988122,0.08249828219413757
25326,-Metrics/Training(Step): loss,1610379991721,0.06808997690677643
25328,-Metrics/Training(Step): loss,1610379995247,0.06825339794158936
25330,-Metrics/Training(Step): loss,1610379998286,0.0737074613571167
25332,-Metrics/Training(Step): loss,1610380001044,0.08604732155799866
25334,-Metrics/Training(Step): loss,1610380003280,0.07426203787326813
25336,-Metrics/Training(Step): loss,1610380005706,0.08280660957098007
25338,-Metrics/Training(Step): loss,1610380007697,0.09760428220033646
25340,-Metrics/Training(Step): loss,1610380009749,0.09174326062202454
25342,-Metrics/Training(Step): loss,1610380011788,0.09950338304042816
25344,-Metrics/Training(Step): loss,1610380013892,0.09590250998735428
25346,-Metrics/Training(Step): loss,1610380015974,0.08096976578235626
25348,-Metrics/Training(Step): loss,1610380017846,0.07735127210617065
25350,-Metrics/Training(Step): loss,1610380019818,0.07844226807355881
25352,-Metrics/Training(Step): loss,1610380021859,0.06984370946884155
25354,-Metrics/Training(Step): loss,1610380023825,0.09038731455802917
25356,-Metrics/Training(Step): loss,1610380025800,0.08345349878072739
25358,-Metrics/Training(Step): loss,1610380027657,0.0819956511259079
25360,-Metrics/Training(Step): loss,1610380029535,0.10424046963453293
25362,-Metrics/Training(Step): loss,1610380031583,0.0913887619972229
25364,-Metrics/Training(Step): loss,1610380033599,0.08471705764532089
25366,-Metrics/Training(Step): loss,1610380035601,0.09220198541879654
25368,-Metrics/Training(Step): loss,1610380037626,0.08375654369592667
25370,-Metrics/Training(Step): loss,1610380039646,0.08644361793994904
25372,-Metrics/Training(Step): loss,1610380052434,0.09550553560256958
25374,-Metrics/Training(Step): loss,1610380056424,0.07860815525054932
25376,-Metrics/Training(Step): loss,1610380060923,0.0863385871052742
25378,-Metrics/Training(Step): loss,1610380065016,0.09715602546930313
25380,-Metrics/Training(Step): loss,1610380069250,0.0827554315328598
25382,-Metrics/Training(Step): loss,1610380073318,0.09188174456357956
25384,-Metrics/Training(Step): loss,1610380077219,0.08719515800476074
25386,-Metrics/Training(Step): loss,1610380081091,0.0888810083270073
25388,-Metrics/Training(Step): loss,1610380084620,0.07464654743671417
25390,-Metrics/Training(Step): loss,1610380088019,0.06465686112642288
25392,-Metrics/Training(Step): loss,1610380091234,0.1101575642824173
25394,-Metrics/Training(Step): loss,1610380094832,0.06874379515647888
25396,-Metrics/Training(Step): loss,1610380098532,0.08820486068725586
25398,-Metrics/Training(Step): loss,1610380102223,0.08285687118768692
25400,-Metrics/Training(Step): loss,1610380105720,0.06668001413345337
25402,-Metrics/Training(Step): loss,1610380109451,0.081385038793087
25404,-Metrics/Training(Step): loss,1610380112520,0.08275222778320312
25406,-Metrics/Training(Step): loss,1610380116038,0.07680752128362656
25408,-Metrics/Training(Step): loss,1610380120022,0.09179305285215378
25410,-Metrics/Training(Step): loss,1610380123448,0.08217087388038635
25412,-Metrics/Training(Step): loss,1610380127135,0.09694557636976242
25414,-Metrics/Training(Step): loss,1610380130520,0.0851740688085556
25416,-Metrics/Training(Step): loss,1610380133376,0.08586151897907257
25418,-Metrics/Training(Step): loss,1610380135725,0.06865379214286804
25420,-Metrics/Training(Step): loss,1610380137868,0.06292606145143509
25422,-Metrics/Training(Step): loss,1610380139967,0.08711318671703339
25424,-Metrics/Training(Step): loss,1610380141990,0.07682021707296371
25426,-Metrics/Training(Step): loss,1610380144038,0.08777673542499542
25428,-Metrics/Training(Step): loss,1610380146080,0.09688156098127365
25430,-Metrics/Training(Step): loss,1610380148158,0.07746153324842453
25432,-Metrics/Training(Step): loss,1610380150206,0.07415363937616348
25434,-Metrics/Training(Step): loss,1610380152010,0.06756241619586945
25436,-Metrics/Training(Step): loss,1610380154056,0.07766280323266983
25438,-Metrics/Training(Step): loss,1610380155997,0.09020933508872986
25440,-Metrics/Training(Step): loss,1610380158054,0.07423023134469986
25442,-Metrics/Training(Step): loss,1610380160117,0.08752967417240143
25444,-Metrics/Training(Step): loss,1610380162185,0.0873510017991066
25446,-Metrics/Training(Step): loss,1610380164090,0.08493512868881226
25448,-Metrics/Training(Step): loss,1610380165879,0.10009454935789108
25450,-Metrics/Training(Step): loss,1610380167938,0.06921619176864624
25452,-Metrics/Training(Step): loss,1610380169980,0.10116890072822571
25454,-Metrics/Training(Step): loss,1610380172034,0.09905675053596497
25456,-Metrics/Training(Step): loss,1610380174096,0.08362046629190445
25458,-Metrics/Training(Step): loss,1610380186025,0.07739699631929398
25460,-Metrics/Training(Step): loss,1610380190224,0.08524572849273682
25462,-Metrics/Training(Step): loss,1610380194258,0.08398919552564621
25464,-Metrics/Training(Step): loss,1610380198321,0.09937931597232819
25466,-Metrics/Training(Step): loss,1610380202325,0.08857806771993637
25468,-Metrics/Training(Step): loss,1610380206320,0.09375301003456116
25470,-Metrics/Training(Step): loss,1610380210637,0.07851240038871765
25472,-Metrics/Training(Step): loss,1610380214017,0.06952305883169174
25474,-Metrics/Training(Step): loss,1610380217215,0.06823495775461197
25476,-Metrics/Training(Step): loss,1610380220219,0.11190176755189896
25478,-Metrics/Training(Step): loss,1610380223961,0.083796426653862
25480,-Metrics/Training(Step): loss,1610380227418,0.09105434268712997
25482,-Metrics/Training(Step): loss,1610380230755,0.07241692394018173
25484,-Metrics/Training(Step): loss,1610380234121,0.07687663286924362
25486,-Metrics/Training(Step): loss,1610380237726,0.08498851954936981
25488,-Metrics/Training(Step): loss,1610380241619,0.08086834102869034
25490,-Metrics/Training(Step): loss,1610380244634,0.09071667492389679
25492,-Metrics/Training(Step): loss,1610380248136,0.08562201261520386
25494,-Metrics/Training(Step): loss,1610380251526,0.07855236530303955
25496,-Metrics/Training(Step): loss,1610380255621,0.0583496131002903
25498,-Metrics/Training(Step): loss,1610380259143,0.09907366335391998
25500,-Metrics/Training(Step): loss,1610380262536,0.07524692267179489
25502,-Metrics/Training(Step): loss,1610380266408,0.07756708562374115
25504,-Metrics/Training(Step): loss,1610380268835,0.08783828467130661
25506,-Metrics/Training(Step): loss,1610380271080,0.07959213107824326
25508,-Metrics/Training(Step): loss,1610380273280,0.07649282366037369
25510,-Metrics/Training(Step): loss,1610380275351,0.0837382823228836
25512,-Metrics/Training(Step): loss,1610380277422,0.08955033123493195
25514,-Metrics/Training(Step): loss,1610380279487,0.0978018045425415
25516,-Metrics/Training(Step): loss,1610380281543,0.07952158898115158
25518,-Metrics/Training(Step): loss,1610380283531,0.07243553549051285
25520,-Metrics/Training(Step): loss,1610380285617,0.07973004877567291
25522,-Metrics/Training(Step): loss,1610380287635,0.097166046500206
25524,-Metrics/Training(Step): loss,1610380289643,0.08398411422967911
25526,-Metrics/Training(Step): loss,1610380291552,0.06844177842140198
25528,-Metrics/Training(Step): loss,1610380293579,0.06622639298439026
25530,-Metrics/Training(Step): loss,1610380295556,0.07193771004676819
25532,-Metrics/Training(Step): loss,1610380297557,0.09229537844657898
25534,-Metrics/Training(Step): loss,1610380299513,0.07091642171144485
25536,-Metrics/Training(Step): loss,1610380301375,0.09152209758758545
25538,-Metrics/Training(Step): loss,1610380303413,0.0846017375588417
25540,-Metrics/Training(Step): loss,1610380305433,0.09015516191720963
25542,-Metrics/Training(Step): loss,1610380307475,0.07186377048492432
25544,-Metrics/Training(Step): loss,1610380320028,0.08892395347356796
25546,-Metrics/Training(Step): loss,1610380324220,0.0804181918501854
25548,-Metrics/Training(Step): loss,1610380328117,0.08473975211381912
25550,-Metrics/Training(Step): loss,1610380332121,0.0823788270354271
25552,-Metrics/Training(Step): loss,1610380336037,0.09631658345460892
25554,-Metrics/Training(Step): loss,1610380340220,0.08892327547073364
25556,-Metrics/Training(Step): loss,1610380344116,0.08621607720851898
25558,-Metrics/Training(Step): loss,1610380347520,0.07352997362613678
25560,-Metrics/Training(Step): loss,1610380351120,0.09308268129825592
25562,-Metrics/Training(Step): loss,1610380355040,0.07293841987848282
25564,-Metrics/Training(Step): loss,1610380358764,0.09073933959007263
25566,-Metrics/Training(Step): loss,1610380362432,0.07508553564548492
25568,-Metrics/Training(Step): loss,1610380366169,0.08211398124694824
25570,-Metrics/Training(Step): loss,1610380369599,0.09142085909843445
25572,-Metrics/Training(Step): loss,1610380373157,0.06119066849350929
25574,-Metrics/Training(Step): loss,1610380376421,0.07714486122131348
25576,-Metrics/Training(Step): loss,1610380380550,0.08418077975511551
25578,-Metrics/Training(Step): loss,1610380384040,0.08316653221845627
25580,-Metrics/Training(Step): loss,1610380387620,0.07525042444467545
25582,-Metrics/Training(Step): loss,1610380391720,0.10323718935251236
25584,-Metrics/Training(Step): loss,1610380395015,0.07983078807592392
25586,-Metrics/Training(Step): loss,1610380398419,0.09260900318622589
25588,-Metrics/Training(Step): loss,1610380401965,0.09648778289556503
25590,-Metrics/Training(Step): loss,1610380404555,0.08323325961828232
25592,-Metrics/Training(Step): loss,1610380406594,0.08406907320022583
25594,-Metrics/Training(Step): loss,1610380408644,0.08263706415891647
25596,-Metrics/Training(Step): loss,1610380410619,0.07683582603931427
25598,-Metrics/Training(Step): loss,1610380412703,0.06973002851009369
25600,-Metrics/Training(Step): loss,1610380414813,0.06987189501523972
25602,-Metrics/Training(Step): loss,1610380416886,0.08863366395235062
25604,-Metrics/Training(Step): loss,1610380418956,0.09237829595804214
25606,-Metrics/Training(Step): loss,1610380420667,0.09780211746692657
25608,-Metrics/Training(Step): loss,1610380422755,0.0966978594660759
25610,-Metrics/Training(Step): loss,1610380424783,0.08106736093759537
25612,-Metrics/Training(Step): loss,1610380426624,0.0798313096165657
25614,-Metrics/Training(Step): loss,1610380428655,0.08034234493970871
25616,-Metrics/Training(Step): loss,1610380430716,0.06078936532139778
25618,-Metrics/Training(Step): loss,1610380432621,0.07670389115810394
25620,-Metrics/Training(Step): loss,1610380434700,0.09879159927368164
25622,-Metrics/Training(Step): loss,1610380436529,0.09260182082653046
25624,-Metrics/Training(Step): loss,1610380438531,0.07992494106292725
25626,-Metrics/Training(Step): loss,1610380440554,0.09306351840496063
25628,-Metrics/Training(Step): loss,1610380442564,0.08821234852075577
25630,-Metrics/Training(Step): loss,1610380454435,0.09355419874191284
25632,-Metrics/Training(Step): loss,1610380458620,0.08375518769025803
25634,-Metrics/Training(Step): loss,1610380462519,0.08201329410076141
25636,-Metrics/Training(Step): loss,1610380466617,0.09827211499214172
25638,-Metrics/Training(Step): loss,1610380470422,0.08429038524627686
25640,-Metrics/Training(Step): loss,1610380474255,0.09292623400688171
25642,-Metrics/Training(Step): loss,1610380478122,0.082103431224823
25644,-Metrics/Training(Step): loss,1610380482019,0.07882548868656158
25646,-Metrics/Training(Step): loss,1610380485628,0.07846163213253021
25648,-Metrics/Training(Step): loss,1610380489327,0.07122598588466644
25650,-Metrics/Training(Step): loss,1610380492823,0.08594822883605957
25652,-Metrics/Training(Step): loss,1610380496101,0.08365892618894577
25654,-Metrics/Training(Step): loss,1610380500194,0.06989411264657974
25656,-Metrics/Training(Step): loss,1610380503569,0.0782991424202919
25658,-Metrics/Training(Step): loss,1610380507126,0.08071184903383255
25660,-Metrics/Training(Step): loss,1610380510640,0.0880054160952568
25662,-Metrics/Training(Step): loss,1610380514319,0.0749920904636383
25664,-Metrics/Training(Step): loss,1610380517932,0.09956653416156769
25666,-Metrics/Training(Step): loss,1610380521726,0.06853441148996353
25668,-Metrics/Training(Step): loss,1610380525620,0.08612561970949173
25670,-Metrics/Training(Step): loss,1610380529320,0.09532135725021362
25672,-Metrics/Training(Step): loss,1610380532819,0.08263686299324036
25674,-Metrics/Training(Step): loss,1610380536149,0.10548856109380722
25676,-Metrics/Training(Step): loss,1610380538586,0.09255951642990112
25678,-Metrics/Training(Step): loss,1610380540668,0.0903681069612503
25680,-Metrics/Training(Step): loss,1610380542770,0.08978895097970963
25682,-Metrics/Training(Step): loss,1610380544747,0.07845734059810638
25684,-Metrics/Training(Step): loss,1610380546800,0.08058346807956696
25686,-Metrics/Training(Step): loss,1610380548898,0.09588352590799332
25688,-Metrics/Training(Step): loss,1610380550959,0.08114108443260193
25690,-Metrics/Training(Step): loss,1610380552949,0.08528894186019897
25692,-Metrics/Training(Step): loss,1610380555035,0.0880548506975174
25694,-Metrics/Training(Step): loss,1610380556958,0.09470637887716293
25696,-Metrics/Training(Step): loss,1610380558854,0.0827859416604042
25698,-Metrics/Training(Step): loss,1610380560923,0.07726572453975677
25700,-Metrics/Training(Step): loss,1610380562827,0.08612217754125595
25702,-Metrics/Training(Step): loss,1610380564849,0.07398319989442825
25704,-Metrics/Training(Step): loss,1610380566899,0.07250082492828369
25706,-Metrics/Training(Step): loss,1610380568878,0.07999710738658905
25708,-Metrics/Training(Step): loss,1610380570967,0.08508166670799255
25710,-Metrics/Training(Step): loss,1610380572918,0.08477882295846939
25712,-Metrics/Training(Step): loss,1610380574947,0.07973761111497879
25714,-Metrics/Training(Step): loss,1610380576985,0.06968332827091217
25716,-Metrics/Training(Step): loss,1610380589222,0.08424320071935654
25718,-Metrics/Training(Step): loss,1610380593320,0.10321398824453354
25720,-Metrics/Training(Step): loss,1610380597330,0.09751492738723755
25722,-Metrics/Training(Step): loss,1610380601658,0.07809337228536606
25724,-Metrics/Training(Step): loss,1610380605623,0.09131575375795364
25726,-Metrics/Training(Step): loss,1610380609719,0.09287505596876144
25728,-Metrics/Training(Step): loss,1610380613245,0.07089796662330627
25730,-Metrics/Training(Step): loss,1610380617351,0.06218264624476433
25732,-Metrics/Training(Step): loss,1610380621422,0.0792534276843071
25734,-Metrics/Training(Step): loss,1610380625121,0.08191883563995361
25736,-Metrics/Training(Step): loss,1610380628407,0.07624111324548721
25738,-Metrics/Training(Step): loss,1610380631937,0.07266934216022491
25740,-Metrics/Training(Step): loss,1610380635819,0.08250224590301514
25742,-Metrics/Training(Step): loss,1610380639320,0.08390459418296814
25744,-Metrics/Training(Step): loss,1610380642723,0.08779264986515045
25746,-Metrics/Training(Step): loss,1610380645997,0.07234279811382294
25748,-Metrics/Training(Step): loss,1610380649914,0.0937834307551384
25750,-Metrics/Training(Step): loss,1610380653142,0.07491755485534668
25752,-Metrics/Training(Step): loss,1610380656621,0.09715820848941803
25754,-Metrics/Training(Step): loss,1610380659920,0.09732566773891449
25756,-Metrics/Training(Step): loss,1610380663620,0.08366462588310242
25758,-Metrics/Training(Step): loss,1610380667019,0.09207655489444733
25760,-Metrics/Training(Step): loss,1610380670425,0.08448652178049088
25762,-Metrics/Training(Step): loss,1610380673091,0.06450315564870834
25764,-Metrics/Training(Step): loss,1610380675358,0.08786552399396896
25766,-Metrics/Training(Step): loss,1610380677394,0.07004812359809875
25768,-Metrics/Training(Step): loss,1610380679457,0.07243601232767105
25770,-Metrics/Training(Step): loss,1610380681494,0.08187007158994675
25772,-Metrics/Training(Step): loss,1610380683593,0.06610153615474701
25774,-Metrics/Training(Step): loss,1610380685662,0.09442074596881866
25776,-Metrics/Training(Step): loss,1610380687673,0.09864228963851929
25778,-Metrics/Training(Step): loss,1610380689751,0.08860624581575394
25780,-Metrics/Training(Step): loss,1610380691789,0.07557478547096252
25782,-Metrics/Training(Step): loss,1610380693719,0.08349476754665375
25784,-Metrics/Training(Step): loss,1610380695556,0.08083292096853256
25786,-Metrics/Training(Step): loss,1610380697589,0.08019053190946579
25788,-Metrics/Training(Step): loss,1610380699549,0.09485211968421936
25790,-Metrics/Training(Step): loss,1610380701507,0.08426699787378311
25792,-Metrics/Training(Step): loss,1610380703534,0.09979645907878876
25794,-Metrics/Training(Step): loss,1610380705571,0.08101847022771835
25796,-Metrics/Training(Step): loss,1610380707638,0.06522884964942932
25798,-Metrics/Training(Step): loss,1610380709684,0.06627937406301498
25800,-Metrics/Training(Step): loss,1610380711737,0.08382811397314072
